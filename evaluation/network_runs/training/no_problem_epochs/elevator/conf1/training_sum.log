Training log data for domain elevator:
printing the data chronological
Epoch 1:
Training data for problem d-01.pddl in epoch 1:
model creation time: 149.71213388442993s
problem epoch data for epoch 1, problem epoch 1
	sampling search time: 1130.5218830108643s
	during this search the following actions were chosen:
		move-up-fast fast1 n6 n8 was chosen with probability 0.077145
		move-up-slow slow1-0 n4 n7 was chosen with probability 0.074869
		board p2 slow1-0 n7 n0 n1 was chosen with probability 0.082581
		leave p2 slow1-0 n7 n1 n0 was chosen with probability 0.070506
	training time: 1085.626540184021s
	during the training the following losses were computed:
		loss: 8.006300
		loss: 7.796800
		loss: 7.625300
		loss: 7.429400
		loss: 7.237800
		loss: 7.054100
		loss: 6.876800
		loss: 6.702000
		loss: 6.528700
		loss: 6.356200
		loss: 6.186200
		loss: 6.020100
		loss: 5.860700
		loss: 5.711100
		loss: 5.574400
		loss: 5.452300
		loss: 5.346100
		loss: 5.257700
		loss: 5.187900
		loss: 5.136100
		loss: 5.100600
		loss: 5.077800
		loss: 5.063800
		loss: 5.055400
		loss: 5.048200
		loss: 5.040300
		loss: 5.030300
		loss: 5.016300
		loss: 4.998400
		loss: 4.977400
		loss: 4.954000
		loss: 4.931000
		loss: 4.908300
		loss: 4.886800
		loss: 4.867800
		loss: 4.850800
		loss: 4.835800
		loss: 4.823300
		loss: 4.812600
		loss: 4.803500
		loss: 4.796000
		loss: 4.789600
		loss: 4.783700
		loss: 4.777300
		loss: 4.770000
		loss: 4.761600
		loss: 4.752500
		loss: 4.743000
		loss: 4.732900
		loss: 4.724000
		loss: 4.714900
		loss: 4.706400
		loss: 4.698300
		loss: 4.690500
		loss: 4.683200
		loss: 4.675900
		loss: 4.668700
		loss: 4.661400
		loss: 4.654000
		loss: 4.646500
		loss: 4.638700
		loss: 4.630800
		loss: 4.622900
		loss: 4.614900
		loss: 4.606800
		loss: 4.598600
		loss: 4.590500
		loss: 4.582300
		loss: 4.574100
		loss: 4.565900
		loss: 4.557500
		loss: 4.548900
		loss: 4.540200
		loss: 4.531300
		loss: 4.522100
		loss: 4.512700
		loss: 4.503200
		loss: 4.493700
		loss: 4.484400
		loss: 4.474900
		loss: 4.465300
		loss: 4.455800
		loss: 4.446200
		loss: 4.436400
		loss: 4.426400
		loss: 4.416500
		loss: 4.406300
		loss: 4.395900
		loss: 4.385600
		loss: 4.374900
		loss: 4.364300
		loss: 4.353400
		loss: 4.342500
		loss: 4.331300
		loss: 4.319800
		loss: 4.308300
		loss: 4.296500
		loss: 4.284300
		loss: 4.271900
		loss: 4.260000
		loss: 4.247900
		loss: 4.235600
		loss: 4.223100
		loss: 4.210900
		loss: 4.198300
		loss: 4.186000
		loss: 4.173500
		loss: 4.160800
		loss: 4.148100
		loss: 4.135600
		loss: 4.123200
		loss: 4.110900
		loss: 4.098700
		loss: 4.086400
		loss: 4.074200
		loss: 4.062400
		loss: 4.050900
		loss: 4.039500
		loss: 4.028200
		loss: 4.017200
		loss: 4.006200
		loss: 3.995500
		loss: 3.985100
		loss: 3.974900
		loss: 3.964900
		loss: 3.955100
		loss: 3.945200
		loss: 3.935700
		loss: 3.926400
		loss: 3.917100
		loss: 3.908000
		loss: 3.899000
		loss: 3.890200
		loss: 3.881700
		loss: 3.873200
		loss: 3.864900
		loss: 3.856800
		loss: 3.848700
		loss: 3.840700
		loss: 3.833100
		loss: 3.825400
		loss: 3.817600
		loss: 3.810000
		loss: 3.802400
		loss: 3.794800
		loss: 3.787200
		loss: 3.779800
		loss: 3.772300
		loss: 3.764800
		loss: 3.757400
		loss: 3.750000
		loss: 3.742700
		loss: 3.735400
		loss: 3.728300
		loss: 3.721100
		loss: 3.714000
		loss: 3.706900
		loss: 3.699800
		loss: 3.693000
		loss: 3.686200
		loss: 3.679300
		loss: 3.672500
		loss: 3.666000
		loss: 3.659100
		loss: 3.652500
		loss: 3.645900
		loss: 3.639300
		loss: 3.632800
		loss: 3.626100
		loss: 3.619700
		loss: 3.613400
		loss: 3.607200
		loss: 3.601100
		loss: 3.595100
		loss: 3.589200
		loss: 3.583300
		loss: 3.577500
		loss: 3.571800
		loss: 3.566300
		loss: 3.560800
		loss: 3.555400
		loss: 3.550200
		loss: 3.545100
		loss: 3.539900
		loss: 3.535000
		loss: 3.530100
		loss: 3.525300
		loss: 3.520500
		loss: 3.515800
		loss: 3.511200
		loss: 3.506500
		loss: 3.502000
		loss: 3.497600
		loss: 3.493300
		loss: 3.489100
		loss: 3.484900
		loss: 3.481000
		loss: 3.476900
		loss: 3.473100
		loss: 3.469200
		loss: 3.465400
		loss: 3.461800
		loss: 3.458200
		loss: 3.454600
		loss: 3.451200
		loss: 3.447600
		loss: 3.444300
		loss: 3.441000
		loss: 3.437800
		loss: 3.434600
		loss: 3.431600
		loss: 3.428400
		loss: 3.425400
		loss: 3.422500
		loss: 3.419600
		loss: 3.416800
		loss: 3.414000
		loss: 3.411100
		loss: 3.408500
		loss: 3.405500
		loss: 3.403000
		loss: 3.400300
		loss: 3.397700
		loss: 3.395100
		loss: 3.392600
		loss: 3.390000
		loss: 3.387600
		loss: 3.385200
		loss: 3.382800
		loss: 3.380400
		loss: 3.378000
		loss: 3.375900
		loss: 3.373600
		loss: 3.371400
		loss: 3.369200
		loss: 3.367100
		loss: 3.365100
		loss: 3.362900
		loss: 3.360900
		loss: 3.358800
		loss: 3.356900
		loss: 3.354900
		loss: 3.352900
		loss: 3.351000
		loss: 3.349100
		loss: 3.347300
		loss: 3.345600
		loss: 3.344000
		loss: 3.342100
		loss: 3.340600
		loss: 3.338800
		loss: 3.337200
		loss: 3.335800
		loss: 3.334100
		loss: 3.332500
		loss: 3.331000
		loss: 3.329600
		loss: 3.328100
		loss: 3.326700
		loss: 3.325300
		loss: 3.323900
		loss: 3.322600
		loss: 3.321200
		loss: 3.319900
		loss: 3.318600
		loss: 3.317300
		loss: 3.316000
		loss: 3.314700
		loss: 3.313600
		loss: 3.312300
		loss: 3.311100
		loss: 3.310000
		loss: 3.308900
		loss: 3.307700
		loss: 3.306700
		loss: 3.305600
		loss: 3.304500
		loss: 3.303500
		loss: 3.302500
		loss: 3.301400
		loss: 3.300500
		loss: 3.299600
		loss: 3.298700
		loss: 3.297700
		loss: 3.296800
		loss: 3.295900
		loss: 3.295000
		loss: 3.294100
		loss: 3.293300
		loss: 3.292400
		loss: 3.291700
		loss: 3.290900
		loss: 3.290200
		loss: 3.289400
		loss: 3.288700
		loss: 3.287800
		loss: 3.287200
		loss: 3.286300
		loss: 3.285600
		loss: 3.284900
	Overall the loss development was 8.006300 -> 3.284900

Epoch 2:
Training data for problem d-01.pddl in epoch 2:
model creation time: 279.6948766708374s
problem epoch data for epoch 2, problem epoch 1
	sampling search time: 469.023903131485s
	during this search the following actions were chosen:
		move-down-slow slow0-0 n4 n1 was chosen with probability 0.476597
		board p1 slow0-0 n1 n0 n1 was chosen with probability 0.960527
		leave p1 slow0-0 n1 n1 n0 was chosen with probability 0.532276
	training time: 968.8037195205688s
	during the training the following losses were computed:
		loss: 2.318700
		loss: 2.471800
		loss: 2.324600
		loss: 2.367300
		loss: 2.397100
		loss: 2.356700
		loss: 2.319000
		loss: 2.319900
		loss: 2.338900
		loss: 2.347200
		loss: 2.337000
		loss: 2.321400
		loss: 2.314200
		loss: 2.317900
		loss: 2.323800
		loss: 2.324200
		loss: 2.318100
		loss: 2.311000
		loss: 2.308500
		loss: 2.310600
		loss: 2.313500
		loss: 2.313100
		loss: 2.309600
		loss: 2.305700
		loss: 2.304000
		loss: 2.304400
		loss: 2.305000
		loss: 2.304200
		loss: 2.302100
		loss: 2.300100
		loss: 2.299300
		loss: 2.299500
		loss: 2.299400
		loss: 2.298300
		loss: 2.296900
		loss: 2.295900
		loss: 2.295400
		loss: 2.295000
		loss: 2.294500
		loss: 2.293800
		loss: 2.293100
		loss: 2.292500
		loss: 2.291800
		loss: 2.291200
		loss: 2.290700
		loss: 2.290200
		loss: 2.289500
		loss: 2.288900
		loss: 2.288400
		loss: 2.288100
		loss: 2.287700
		loss: 2.287100
		loss: 2.286400
		loss: 2.285900
		loss: 2.285600
		loss: 2.285200
		loss: 2.284700
		loss: 2.284100
		loss: 2.283800
		loss: 2.283400
		loss: 2.283000
		loss: 2.282500
		loss: 2.282000
		loss: 2.281700
		loss: 2.281300
		loss: 2.280900
		loss: 2.280500
		loss: 2.280100
		loss: 2.279700
		loss: 2.279300
		loss: 2.279000
		loss: 2.278600
		loss: 2.278200
		loss: 2.277800
		loss: 2.277500
		loss: 2.277100
		loss: 2.276700
		loss: 2.276400
		loss: 2.276100
		loss: 2.275700
		loss: 2.275400
		loss: 2.275000
		loss: 2.274700
		loss: 2.274400
		loss: 2.274000
		loss: 2.273700
		loss: 2.273400
		loss: 2.273100
		loss: 2.272700
		loss: 2.272400
		loss: 2.272100
		loss: 2.271800
		loss: 2.271500
		loss: 2.271200
		loss: 2.270900
		loss: 2.270600
		loss: 2.270300
		loss: 2.270000
		loss: 2.269700
		loss: 2.269400
		loss: 2.269100
		loss: 2.268800
		loss: 2.268500
		loss: 2.268200
		loss: 2.267900
		loss: 2.267600
		loss: 2.267300
		loss: 2.267100
		loss: 2.266800
		loss: 2.266500
		loss: 2.266200
		loss: 2.265900
		loss: 2.265700
		loss: 2.265400
		loss: 2.265100
		loss: 2.264800
		loss: 2.264600
		loss: 2.264300
		loss: 2.264000
		loss: 2.263800
		loss: 2.263500
		loss: 2.263300
		loss: 2.263000
		loss: 2.262800
		loss: 2.262500
		loss: 2.262300
		loss: 2.262000
		loss: 2.261800
		loss: 2.261500
		loss: 2.261300
		loss: 2.261000
		loss: 2.260800
		loss: 2.260600
		loss: 2.260300
		loss: 2.260100
		loss: 2.259900
		loss: 2.259700
		loss: 2.259400
		loss: 2.259200
		loss: 2.259000
		loss: 2.258800
		loss: 2.258600
		loss: 2.258300
		loss: 2.258100
		loss: 2.257900
		loss: 2.257700
		loss: 2.257500
		loss: 2.257300
		loss: 2.257100
		loss: 2.256900
		loss: 2.256700
		loss: 2.256500
		loss: 2.256300
		loss: 2.256100
		loss: 2.255800
		loss: 2.255700
		loss: 2.255500
		loss: 2.255300
		loss: 2.255100
		loss: 2.254900
		loss: 2.254700
		loss: 2.254500
		loss: 2.254300
		loss: 2.254100
		loss: 2.253900
		loss: 2.253700
		loss: 2.253600
		loss: 2.253400
		loss: 2.253200
		loss: 2.253000
		loss: 2.252800
		loss: 2.252600
		loss: 2.252500
		loss: 2.252300
		loss: 2.252100
		loss: 2.251900
		loss: 2.251700
		loss: 2.251600
		loss: 2.251400
		loss: 2.251200
		loss: 2.251100
		loss: 2.250900
		loss: 2.250700
		loss: 2.250500
		loss: 2.250400
		loss: 2.250200
		loss: 2.250000
		loss: 2.249900
		loss: 2.249700
		loss: 2.249500
		loss: 2.249400
		loss: 2.249200
		loss: 2.249000
		loss: 2.248900
		loss: 2.248700
		loss: 2.248500
		loss: 2.248400
		loss: 2.248200
		loss: 2.248100
		loss: 2.247900
		loss: 2.247700
		loss: 2.247600
		loss: 2.247400
		loss: 2.247300
		loss: 2.247100
		loss: 2.247000
		loss: 2.246800
		loss: 2.246700
		loss: 2.246500
		loss: 2.246400
		loss: 2.246200
		loss: 2.246000
		loss: 2.245900
		loss: 2.245700
		loss: 2.245600
		loss: 2.245400
		loss: 2.245300
		loss: 2.245100
		loss: 2.245000
		loss: 2.244800
		loss: 2.244700
		loss: 2.244600
		loss: 2.244400
		loss: 2.244300
		loss: 2.244100
		loss: 2.244000
		loss: 2.243800
		loss: 2.243700
		loss: 2.243500
		loss: 2.243400
		loss: 2.243200
		loss: 2.243100
		loss: 2.243000
		loss: 2.242800
		loss: 2.242700
		loss: 2.242500
		loss: 2.242400
		loss: 2.242300
		loss: 2.242100
		loss: 2.242000
		loss: 2.241800
		loss: 2.241700
		loss: 2.241600
		loss: 2.241500
		loss: 2.241300
		loss: 2.241200
		loss: 2.241000
		loss: 2.240900
		loss: 2.240800
		loss: 2.240600
		loss: 2.240500
		loss: 2.240400
		loss: 2.240200
		loss: 2.240100
		loss: 2.239900
		loss: 2.239800
		loss: 2.239700
		loss: 2.239600
		loss: 2.239400
		loss: 2.239300
		loss: 2.239100
		loss: 2.239000
		loss: 2.238900
		loss: 2.238800
		loss: 2.238600
		loss: 2.238500
		loss: 2.238400
		loss: 2.238200
		loss: 2.238100
		loss: 2.238000
		loss: 2.237900
		loss: 2.237700
		loss: 2.237600
		loss: 2.237500
		loss: 2.237300
		loss: 2.237200
		loss: 2.237100
		loss: 2.237000
		loss: 2.236800
		loss: 2.236700
		loss: 2.236600
		loss: 2.236500
		loss: 2.236300
		loss: 2.236200
		loss: 2.236100
		loss: 2.236000
		loss: 2.235800
		loss: 2.235700
		loss: 2.235600
		loss: 2.235500
		loss: 2.235300
		loss: 2.235200
		loss: 2.235100
		loss: 2.235000
		loss: 2.234800
		loss: 2.234700
		loss: 2.234600
		loss: 2.234500
		loss: 2.234400
		loss: 2.234300
	Overall the loss development was 2.318700 -> 2.234300

Epoch 3:
Training data for problem d-01.pddl in epoch 3:
model creation time: 1297.506285905838s
problem epoch data for epoch 3, problem epoch 1
	sampling search time: 587.6502759456635s
	during this search the following actions were chosen:
		move-down-slow slow0-0 n4 n1 was chosen with probability 0.505536
		board p1 slow0-0 n1 n0 n1 was chosen with probability 0.992023
		move-down-slow slow0-0 n1 n0 was chosen with probability 0.499228
		board p3 slow0-0 n0 n1 n2 was chosen with probability 0.502387
		leave p3 slow0-0 n0 n2 n1 was chosen with probability 0.337979
	training time: 1784.5554912090302s
	during the training the following losses were computed:
		loss: 2.234100
		loss: 2.477200
		loss: 2.246300
		loss: 2.318900
		loss: 2.368600
		loss: 2.306600
		loss: 2.242100
		loss: 2.243000
		loss: 2.278100
		loss: 2.291700
		loss: 2.274100
		loss: 2.248000
		loss: 2.237100
		loss: 2.245700
		loss: 2.259200
		loss: 2.263600
		loss: 2.256300
		loss: 2.244200
		loss: 2.237100
		loss: 2.239100
		loss: 2.245600
		loss: 2.249300
		loss: 2.246900
		loss: 2.241100
		loss: 2.236700
		loss: 2.236400
		loss: 2.239100
		loss: 2.241500
		loss: 2.241200
		loss: 2.238600
		loss: 2.236000
		loss: 2.235300
		loss: 2.236400
		loss: 2.237600
		loss: 2.237600
		loss: 2.236400
		loss: 2.235100
		loss: 2.234600
		loss: 2.235000
		loss: 2.235500
		loss: 2.235400
		loss: 2.234800
		loss: 2.234100
		loss: 2.233800
		loss: 2.233900
		loss: 2.234100
		loss: 2.234000
		loss: 2.233700
		loss: 2.233300
		loss: 2.233200
		loss: 2.233200
		loss: 2.233200
		loss: 2.233100
		loss: 2.232900
		loss: 2.232700
		loss: 2.232600
		loss: 2.232600
		loss: 2.232600
		loss: 2.232400
		loss: 2.232300
		loss: 2.232200
		loss: 2.232100
		loss: 2.232100
		loss: 2.232000
		loss: 2.231900
		loss: 2.231800
		loss: 2.231700
		loss: 2.231700
		loss: 2.231600
		loss: 2.231500
		loss: 2.231400
		loss: 2.231300
		loss: 2.231300
		loss: 2.231200
		loss: 2.231100
		loss: 2.231000
		loss: 2.231000
		loss: 2.230900
		loss: 2.230800
		loss: 2.230700
		loss: 2.230700
		loss: 2.230600
		loss: 2.230600
		loss: 2.230500
		loss: 2.230400
		loss: 2.230300
		loss: 2.230300
		loss: 2.230200
		loss: 2.230100
		loss: 2.230100
		loss: 2.230000
		loss: 2.229900
		loss: 2.229900
		loss: 2.229800
		loss: 2.229800
		loss: 2.229700
		loss: 2.229600
		loss: 2.229600
		loss: 2.229500
		loss: 2.229400
		loss: 2.229400
		loss: 2.229300
		loss: 2.229300
		loss: 2.229200
		loss: 2.229100
		loss: 2.229100
		loss: 2.229000
		loss: 2.228900
		loss: 2.228900
		loss: 2.228800
		loss: 2.228700
		loss: 2.228700
		loss: 2.228600
		loss: 2.228600
		loss: 2.228500
		loss: 2.228400
		loss: 2.228400
		loss: 2.228300
		loss: 2.228200
		loss: 2.228200
		loss: 2.228100
		loss: 2.228000
		loss: 2.228000
		loss: 2.227900
		loss: 2.227900
		loss: 2.227800
		loss: 2.227700
		loss: 2.227700
		loss: 2.227600
		loss: 2.227500
		loss: 2.227500
		loss: 2.227400
		loss: 2.227300
		loss: 2.227300
		loss: 2.227200
		loss: 2.227100
		loss: 2.227100
		loss: 2.227000
		loss: 2.227000
		loss: 2.226900
		loss: 2.226900
		loss: 2.226800
		loss: 2.226700
		loss: 2.226700
		loss: 2.226600
		loss: 2.226600
		loss: 2.226500
		loss: 2.226400
		loss: 2.226400
		loss: 2.226300
		loss: 2.226300
		loss: 2.226200
		loss: 2.226200
		loss: 2.226100
		loss: 2.226000
		loss: 2.226000
		loss: 2.225900
		loss: 2.225900
		loss: 2.225800
		loss: 2.225800
		loss: 2.225700
		loss: 2.225600
		loss: 2.225600
		loss: 2.225500
		loss: 2.225500
		loss: 2.225400
		loss: 2.225400
		loss: 2.225300
		loss: 2.225200
		loss: 2.225200
		loss: 2.225100
		loss: 2.225100
		loss: 2.225000
		loss: 2.225000
		loss: 2.224900
		loss: 2.224800
		loss: 2.224800
		loss: 2.224700
		loss: 2.224700
		loss: 2.224600
		loss: 2.224600
		loss: 2.224500
		loss: 2.224400
		loss: 2.224400
		loss: 2.224300
		loss: 2.224300
		loss: 2.224200
		loss: 2.224200
		loss: 2.224100
		loss: 2.224000
		loss: 2.224000
		loss: 2.223900
		loss: 2.223900
		loss: 2.223800
		loss: 2.223800
		loss: 2.223700
		loss: 2.223600
		loss: 2.223600
		loss: 2.223500
		loss: 2.223500
		loss: 2.223400
		loss: 2.223400
		loss: 2.223300
		loss: 2.223300
		loss: 2.223200
		loss: 2.223100
		loss: 2.223100
		loss: 2.223000
		loss: 2.223000
		loss: 2.222900
		loss: 2.222900
		loss: 2.222800
		loss: 2.222700
		loss: 2.222700
		loss: 2.222600
		loss: 2.222600
		loss: 2.222500
		loss: 2.222500
		loss: 2.222400
		loss: 2.222400
		loss: 2.222300
		loss: 2.222200
		loss: 2.222200
		loss: 2.222100
		loss: 2.222100
		loss: 2.222000
		loss: 2.222000
		loss: 2.221900
		loss: 2.221800
		loss: 2.221800
		loss: 2.221700
		loss: 2.221700
		loss: 2.221600
		loss: 2.221600
		loss: 2.221500
		loss: 2.221400
		loss: 2.221400
		loss: 2.221300
		loss: 2.221300
		loss: 2.221200
		loss: 2.221200
		loss: 2.221100
		loss: 2.221100
		loss: 2.221000
		loss: 2.220900
		loss: 2.220900
		loss: 2.220800
		loss: 2.220800
		loss: 2.220700
		loss: 2.220600
		loss: 2.220600
		loss: 2.220500
		loss: 2.220500
		loss: 2.220400
		loss: 2.220400
		loss: 2.220300
		loss: 2.220200
		loss: 2.220200
		loss: 2.220100
		loss: 2.220100
		loss: 2.220000
		loss: 2.219900
		loss: 2.219900
		loss: 2.219800
		loss: 2.219800
		loss: 2.219700
		loss: 2.219700
		loss: 2.219600
		loss: 2.219500
		loss: 2.219500
		loss: 2.219400
		loss: 2.219400
		loss: 2.219300
		loss: 2.219300
		loss: 2.219200
		loss: 2.219200
		loss: 2.219100
		loss: 2.219000
		loss: 2.219000
		loss: 2.218900
		loss: 2.218900
		loss: 2.218800
		loss: 2.218700
		loss: 2.218700
		loss: 2.218600
		loss: 2.218600
		loss: 2.218500
		loss: 2.218500
		loss: 2.218400
		loss: 2.218300
		loss: 2.218300
		loss: 2.218200
		loss: 2.218200
		loss: 2.218100
		loss: 2.218100
		loss: 2.218000
		loss: 2.218000
		loss: 2.217900
		loss: 2.217800
		loss: 2.217800
	Overall the loss development was 2.234100 -> 2.217800

