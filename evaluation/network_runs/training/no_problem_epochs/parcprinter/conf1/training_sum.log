Training log data for domain parcprinter:
printing the data chronological
Epoch 1:
Training data for problem d-01.pddl in epoch 1:
model creation time: 16.8443922996521s
problem epoch data for epoch 1, problem epoch 1
	sampling search time: 3.7428791522979736s
	during this search the following actions were chosen:
	training time: 73.23563528060913s
	during the training the following losses were computed:
		loss: 1.090400
		loss: 1.072200
		loss: 1.055000
		loss: 1.039900
		loss: 1.023900
		loss: 1.008400
		loss: 0.993700
		loss: 0.979200
		loss: 0.964400
		loss: 0.949800
		loss: 0.935300
		loss: 0.921000
		loss: 0.906700
		loss: 0.892500
		loss: 0.878600
		loss: 0.864700
		loss: 0.851200
		loss: 0.838100
		loss: 0.825100
		loss: 0.812300
		loss: 0.800000
		loss: 0.788000
		loss: 0.776200
		loss: 0.764800
		loss: 0.753900
		loss: 0.743300
		loss: 0.733200
		loss: 0.723400
		loss: 0.714100
		loss: 0.705000
		loss: 0.696300
		loss: 0.687900
		loss: 0.679700
		loss: 0.671800
		loss: 0.664100
		loss: 0.656700
		loss: 0.649400
		loss: 0.642300
		loss: 0.635300
		loss: 0.628400
		loss: 0.621600
		loss: 0.615000
		loss: 0.608400
		loss: 0.602000
		loss: 0.595600
		loss: 0.589300
		loss: 0.583000
		loss: 0.576900
		loss: 0.570800
		loss: 0.564800
		loss: 0.558800
		loss: 0.552900
		loss: 0.547100
		loss: 0.541300
		loss: 0.535600
		loss: 0.530000
		loss: 0.524400
		loss: 0.518900
		loss: 0.513500
		loss: 0.508100
		loss: 0.502800
		loss: 0.497600
		loss: 0.492300
		loss: 0.487200
		loss: 0.482100
		loss: 0.477100
		loss: 0.472200
		loss: 0.467300
		loss: 0.462400
		loss: 0.457600
		loss: 0.452900
		loss: 0.448300
		loss: 0.443700
		loss: 0.439100
		loss: 0.434600
		loss: 0.430200
		loss: 0.425800
		loss: 0.421500
		loss: 0.417200
		loss: 0.412900
		loss: 0.408800
		loss: 0.404600
		loss: 0.400500
		loss: 0.396500
		loss: 0.392500
		loss: 0.388500
		loss: 0.384600
		loss: 0.380700
		loss: 0.376900
		loss: 0.373200
		loss: 0.369400
		loss: 0.365700
		loss: 0.362100
		loss: 0.358500
		loss: 0.354900
		loss: 0.351400
		loss: 0.347900
		loss: 0.344400
		loss: 0.341000
		loss: 0.337600
		loss: 0.334200
		loss: 0.330900
		loss: 0.327700
		loss: 0.324400
		loss: 0.321200
		loss: 0.318100
		loss: 0.315000
		loss: 0.311900
		loss: 0.308800
		loss: 0.305700
		loss: 0.302700
		loss: 0.299800
		loss: 0.296800
		loss: 0.293900
		loss: 0.291100
		loss: 0.288200
		loss: 0.285400
		loss: 0.282600
		loss: 0.279900
		loss: 0.277100
		loss: 0.274400
		loss: 0.271800
		loss: 0.269200
		loss: 0.266500
		loss: 0.264000
		loss: 0.261400
		loss: 0.258900
		loss: 0.256400
		loss: 0.253900
		loss: 0.251500
		loss: 0.249100
		loss: 0.246700
		loss: 0.244300
		loss: 0.241900
		loss: 0.239600
		loss: 0.237300
		loss: 0.235000
		loss: 0.232800
		loss: 0.230600
		loss: 0.228400
		loss: 0.226200
		loss: 0.224000
		loss: 0.221900
		loss: 0.219800
		loss: 0.217700
		loss: 0.215600
		loss: 0.213600
		loss: 0.211500
		loss: 0.209500
		loss: 0.207500
		loss: 0.205600
		loss: 0.203600
		loss: 0.201700
		loss: 0.199800
		loss: 0.197900
		loss: 0.196000
		loss: 0.194200
		loss: 0.192400
		loss: 0.190600
		loss: 0.188800
		loss: 0.187000
		loss: 0.185200
		loss: 0.183500
		loss: 0.181800
		loss: 0.180100
		loss: 0.178400
		loss: 0.176700
		loss: 0.175100
		loss: 0.173400
		loss: 0.171800
		loss: 0.170200
		loss: 0.168600
		loss: 0.167100
		loss: 0.165500
		loss: 0.164000
		loss: 0.162400
		loss: 0.160900
		loss: 0.159400
		loss: 0.158000
		loss: 0.156500
		loss: 0.155000
		loss: 0.153600
		loss: 0.152200
		loss: 0.150800
		loss: 0.149400
		loss: 0.148000
		loss: 0.146700
		loss: 0.145300
		loss: 0.144000
		loss: 0.142600
		loss: 0.141300
		loss: 0.140000
		loss: 0.138800
		loss: 0.137500
		loss: 0.136200
		loss: 0.135000
		loss: 0.133800
		loss: 0.132500
		loss: 0.131300
		loss: 0.130100
		loss: 0.128900
		loss: 0.127800
		loss: 0.126600
		loss: 0.125500
		loss: 0.124300
		loss: 0.123200
		loss: 0.122100
		loss: 0.121000
		loss: 0.119900
		loss: 0.118800
		loss: 0.117700
		loss: 0.116600
		loss: 0.115600
		loss: 0.114600
		loss: 0.113500
		loss: 0.112500
		loss: 0.111500
		loss: 0.110500
		loss: 0.109500
		loss: 0.108500
		loss: 0.107500
		loss: 0.106600
		loss: 0.105600
		loss: 0.104700
		loss: 0.103700
		loss: 0.102800
		loss: 0.101900
		loss: 0.101000
		loss: 0.100100
		loss: 0.099200
		loss: 0.098300
		loss: 0.097500
		loss: 0.096600
		loss: 0.095700
		loss: 0.094900
		loss: 0.094000
		loss: 0.093200
		loss: 0.092400
		loss: 0.091600
		loss: 0.090800
		loss: 0.090000
		loss: 0.089200
		loss: 0.088400
		loss: 0.087600
		loss: 0.086800
		loss: 0.086100
		loss: 0.085300
		loss: 0.084600
		loss: 0.083800
		loss: 0.083100
		loss: 0.082400
		loss: 0.081700
		loss: 0.080900
		loss: 0.080200
		loss: 0.079500
		loss: 0.078900
		loss: 0.078200
		loss: 0.077500
		loss: 0.076800
		loss: 0.076200
		loss: 0.075500
		loss: 0.074800
		loss: 0.074200
		loss: 0.073600
		loss: 0.072900
		loss: 0.072300
		loss: 0.071700
		loss: 0.071100
		loss: 0.070400
		loss: 0.069800
		loss: 0.069200
		loss: 0.068700
		loss: 0.068100
		loss: 0.067500
		loss: 0.066900
		loss: 0.066300
		loss: 0.065800
		loss: 0.065200
		loss: 0.064700
		loss: 0.064100
		loss: 0.063600
		loss: 0.063000
		loss: 0.062500
		loss: 0.062000
		loss: 0.061400
		loss: 0.060900
		loss: 0.060400
		loss: 0.059900
		loss: 0.059400
		loss: 0.058900
		loss: 0.058400
		loss: 0.057900
		loss: 0.057400
		loss: 0.056900
		loss: 0.056500
		loss: 0.056000
		loss: 0.055500
		loss: 0.055100
		loss: 0.054600
		loss: 0.054200
	Overall the loss development was 1.090400 -> 0.054200

Training data for problem d-02.pddl in epoch 1:
model creation time: 31.48408579826355s
problem epoch data for epoch 1, problem epoch 1
	sampling search time: 7.135568618774414s
	during this search the following actions were chosen:
	training time: 101.77666759490967s
	during the training the following losses were computed:
		loss: 1.461900
		loss: 1.381400
		loss: 1.317400
		loss: 1.254700
		loss: 1.191800
		loss: 1.137300
		loss: 1.087700
		loss: 1.042300
		loss: 1.003000
		loss: 0.968500
		loss: 0.939000
		loss: 0.914000
		loss: 0.893900
		loss: 0.877100
		loss: 0.863100
		loss: 0.851400
		loss: 0.841500
		loss: 0.832900
		loss: 0.825200
		loss: 0.818000
		loss: 0.811000
		loss: 0.803900
		loss: 0.796700
		loss: 0.789300
		loss: 0.781700
		loss: 0.774100
		loss: 0.766400
		loss: 0.759000
		loss: 0.751900
		loss: 0.745300
		loss: 0.739300
		loss: 0.733900
		loss: 0.729100
		loss: 0.724800
		loss: 0.721100
		loss: 0.717700
		loss: 0.714600
		loss: 0.711900
		loss: 0.709400
		loss: 0.707100
		loss: 0.705100
		loss: 0.703200
		loss: 0.701600
		loss: 0.700100
		loss: 0.698600
		loss: 0.697200
		loss: 0.695800
		loss: 0.694300
		loss: 0.692900
		loss: 0.691500
		loss: 0.690000
		loss: 0.688500
		loss: 0.687000
		loss: 0.685400
		loss: 0.683600
		loss: 0.681600
		loss: 0.679600
		loss: 0.677300
		loss: 0.674900
		loss: 0.672300
		loss: 0.669600
		loss: 0.666700
		loss: 0.663500
		loss: 0.660200
		loss: 0.656500
		loss: 0.652600
		loss: 0.648500
		loss: 0.644200
		loss: 0.639600
		loss: 0.634800
		loss: 0.629800
		loss: 0.624700
		loss: 0.619500
		loss: 0.614500
		loss: 0.609600
		loss: 0.605300
		loss: 0.601300
		loss: 0.597600
		loss: 0.594100
		loss: 0.590800
		loss: 0.587700
		loss: 0.584900
		loss: 0.582300
		loss: 0.579800
		loss: 0.577500
		loss: 0.575400
		loss: 0.573600
		loss: 0.572000
		loss: 0.570600
		loss: 0.569500
		loss: 0.568700
		loss: 0.568000
		loss: 0.567200
		loss: 0.566500
		loss: 0.565700
		loss: 0.565000
		loss: 0.564300
		loss: 0.563800
		loss: 0.563300
		loss: 0.562800
		loss: 0.562200
		loss: 0.561600
		loss: 0.561000
		loss: 0.560400
		loss: 0.559900
		loss: 0.559400
		loss: 0.558900
		loss: 0.558300
		loss: 0.557800
		loss: 0.557200
		loss: 0.556700
		loss: 0.556200
		loss: 0.555800
		loss: 0.555300
		loss: 0.554800
		loss: 0.554300
		loss: 0.553900
		loss: 0.553500
		loss: 0.553000
		loss: 0.552600
		loss: 0.552200
		loss: 0.551800
		loss: 0.551400
		loss: 0.551100
		loss: 0.550700
		loss: 0.550400
		loss: 0.550000
		loss: 0.549700
		loss: 0.549400
		loss: 0.549000
		loss: 0.548700
		loss: 0.548400
		loss: 0.548100
		loss: 0.547800
		loss: 0.547500
		loss: 0.547200
		loss: 0.547000
		loss: 0.546700
		loss: 0.546400
		loss: 0.546100
		loss: 0.545900
		loss: 0.545600
		loss: 0.545300
		loss: 0.545100
		loss: 0.544800
		loss: 0.544600
		loss: 0.544300
		loss: 0.544100
		loss: 0.543900
		loss: 0.543600
		loss: 0.543400
		loss: 0.543200
		loss: 0.542900
		loss: 0.542700
		loss: 0.542500
		loss: 0.542300
		loss: 0.542000
		loss: 0.541800
		loss: 0.541600
		loss: 0.541400
		loss: 0.541200
		loss: 0.541000
		loss: 0.540800
		loss: 0.540600
		loss: 0.540400
		loss: 0.540200
		loss: 0.540000
		loss: 0.539900
		loss: 0.539700
		loss: 0.539500
		loss: 0.539300
		loss: 0.539100
		loss: 0.539000
		loss: 0.538800
		loss: 0.538600
		loss: 0.538500
		loss: 0.538300
		loss: 0.538200
		loss: 0.538000
		loss: 0.537800
		loss: 0.537700
		loss: 0.537500
		loss: 0.537400
		loss: 0.537200
		loss: 0.537100
		loss: 0.536900
		loss: 0.536800
		loss: 0.536600
		loss: 0.536500
		loss: 0.536400
		loss: 0.536200
		loss: 0.536100
		loss: 0.535900
		loss: 0.535800
		loss: 0.535700
		loss: 0.535600
		loss: 0.535400
		loss: 0.535300
		loss: 0.535200
		loss: 0.535100
		loss: 0.534900
		loss: 0.534800
		loss: 0.534700
		loss: 0.534600
		loss: 0.534500
		loss: 0.534400
		loss: 0.534300
		loss: 0.534200
		loss: 0.534100
		loss: 0.533900
		loss: 0.533800
		loss: 0.533700
		loss: 0.533600
		loss: 0.533500
		loss: 0.533400
		loss: 0.533300
		loss: 0.533200
		loss: 0.533100
		loss: 0.533000
		loss: 0.532900
		loss: 0.532800
		loss: 0.532700
		loss: 0.532600
		loss: 0.532500
		loss: 0.532400
		loss: 0.532400
		loss: 0.532300
		loss: 0.532200
		loss: 0.532100
		loss: 0.532000
		loss: 0.531900
		loss: 0.531800
		loss: 0.531800
		loss: 0.531700
		loss: 0.531600
		loss: 0.531500
		loss: 0.531400
		loss: 0.531400
		loss: 0.531300
		loss: 0.531300
		loss: 0.531200
		loss: 0.531100
		loss: 0.531000
		loss: 0.530900
		loss: 0.530900
		loss: 0.530800
		loss: 0.530700
		loss: 0.530600
		loss: 0.530600
		loss: 0.530500
		loss: 0.530400
		loss: 0.530300
		loss: 0.530300
		loss: 0.530200
		loss: 0.530100
		loss: 0.530100
		loss: 0.530000
		loss: 0.530000
		loss: 0.529900
		loss: 0.529800
		loss: 0.529800
		loss: 0.529700
		loss: 0.529700
		loss: 0.529600
		loss: 0.529500
		loss: 0.529500
		loss: 0.529400
		loss: 0.529400
		loss: 0.529300
		loss: 0.529200
		loss: 0.529200
		loss: 0.529100
		loss: 0.529100
		loss: 0.529000
		loss: 0.529000
		loss: 0.528900
		loss: 0.528900
		loss: 0.528800
		loss: 0.528800
		loss: 0.528700
		loss: 0.528700
		loss: 0.528600
		loss: 0.528600
		loss: 0.528500
		loss: 0.528500
		loss: 0.528400
		loss: 0.528400
		loss: 0.528300
		loss: 0.528300
		loss: 0.528200
		loss: 0.528200
		loss: 0.528100
		loss: 0.528100
		loss: 0.528100
		loss: 0.528100
		loss: 0.528000
		loss: 0.527900
		loss: 0.527900
		loss: 0.527900
		loss: 0.527800
	Overall the loss development was 1.461900 -> 0.527800

Training data for problem d-03.pddl in epoch 1:
model creation time: 25.78506588935852s
problem epoch data for epoch 1, problem epoch 1
	sampling search time: 6.119212627410889s
	during this search the following actions were chosen:
	training time: 87.2435245513916s
	during the training the following losses were computed:
		loss: 2.106500
		loss: 2.012900
		loss: 1.944200
		loss: 1.898000
		loss: 1.869700
		loss: 1.854900
		loss: 1.848500
		loss: 1.847400
		loss: 1.849100
		loss: 1.851500
		loss: 1.853400
		loss: 1.853800
		loss: 1.852600
		loss: 1.849800
		loss: 1.845900
		loss: 1.841400
		loss: 1.836700
		loss: 1.832300
		loss: 1.828400
		loss: 1.825200
		loss: 1.822800
		loss: 1.821100
		loss: 1.820000
		loss: 1.819400
		loss: 1.819100
		loss: 1.819000
		loss: 1.818900
		loss: 1.818700
		loss: 1.818100
		loss: 1.817200
		loss: 1.816100
		loss: 1.814900
		loss: 1.813500
		loss: 1.812200
		loss: 1.811000
		loss: 1.809900
		loss: 1.808900
		loss: 1.807900
		loss: 1.807000
		loss: 1.806100
		loss: 1.805100
		loss: 1.804100
		loss: 1.802900
		loss: 1.801600
		loss: 1.800200
		loss: 1.798600
		loss: 1.797000
		loss: 1.795100
		loss: 1.793100
		loss: 1.791000
		loss: 1.788700
		loss: 1.786200
		loss: 1.783600
		loss: 1.780800
		loss: 1.777700
		loss: 1.774500
		loss: 1.770900
		loss: 1.767100
		loss: 1.763000
		loss: 1.758600
		loss: 1.753900
		loss: 1.748900
		loss: 1.743700
		loss: 1.738400
		loss: 1.732900
		loss: 1.727400
		loss: 1.721900
		loss: 1.716500
		loss: 1.711300
		loss: 1.706400
		loss: 1.701900
		loss: 1.697800
		loss: 1.694200
		loss: 1.691000
		loss: 1.688200
		loss: 1.685900
		loss: 1.683900
		loss: 1.682400
		loss: 1.681200
		loss: 1.680400
		loss: 1.679800
		loss: 1.679200
		loss: 1.678800
		loss: 1.678600
		loss: 1.678400
		loss: 1.678300
		loss: 1.678100
		loss: 1.678000
		loss: 1.677900
		loss: 1.677800
		loss: 1.677800
		loss: 1.677700
		loss: 1.677600
		loss: 1.677500
		loss: 1.677500
		loss: 1.677400
		loss: 1.677400
		loss: 1.677300
		loss: 1.677200
		loss: 1.677100
		loss: 1.677000
		loss: 1.677000
		loss: 1.676900
		loss: 1.676800
		loss: 1.676700
		loss: 1.676600
		loss: 1.676500
		loss: 1.676500
		loss: 1.676400
		loss: 1.676300
		loss: 1.676200
		loss: 1.676100
		loss: 1.676100
		loss: 1.676000
		loss: 1.675900
		loss: 1.675800
		loss: 1.675800
		loss: 1.675700
		loss: 1.675600
		loss: 1.675600
		loss: 1.675500
		loss: 1.675500
		loss: 1.675400
		loss: 1.675300
		loss: 1.675300
		loss: 1.675200
		loss: 1.675200
		loss: 1.675100
		loss: 1.675100
		loss: 1.675000
		loss: 1.675000
		loss: 1.674900
		loss: 1.674900
		loss: 1.674800
		loss: 1.674800
		loss: 1.674700
		loss: 1.674700
		loss: 1.674700
		loss: 1.674600
		loss: 1.674600
		loss: 1.674500
		loss: 1.674500
		loss: 1.674500
		loss: 1.674400
		loss: 1.674400
		loss: 1.674400
		loss: 1.674300
		loss: 1.674300
		loss: 1.674300
		loss: 1.674200
		loss: 1.674200
		loss: 1.674200
		loss: 1.674100
		loss: 1.674100
		loss: 1.674100
		loss: 1.674000
		loss: 1.674000
		loss: 1.674000
		loss: 1.673900
		loss: 1.673900
		loss: 1.673900
		loss: 1.673800
		loss: 1.673800
		loss: 1.673800
		loss: 1.673700
		loss: 1.673700
		loss: 1.673700
		loss: 1.673600
		loss: 1.673600
		loss: 1.673600
		loss: 1.673500
		loss: 1.673500
		loss: 1.673500
		loss: 1.673400
		loss: 1.673400
		loss: 1.673400
		loss: 1.673400
		loss: 1.673300
		loss: 1.673300
		loss: 1.673300
		loss: 1.673200
		loss: 1.673200
		loss: 1.673200
		loss: 1.673200
		loss: 1.673100
		loss: 1.673100
		loss: 1.673100
		loss: 1.673000
		loss: 1.673000
		loss: 1.673000
		loss: 1.673000
		loss: 1.672900
		loss: 1.672900
		loss: 1.672900
		loss: 1.672900
		loss: 1.672800
		loss: 1.672800
		loss: 1.672800
		loss: 1.672700
		loss: 1.672700
		loss: 1.672700
		loss: 1.672700
		loss: 1.672600
		loss: 1.672600
		loss: 1.672600
		loss: 1.672600
		loss: 1.672500
		loss: 1.672500
		loss: 1.672500
		loss: 1.672500
		loss: 1.672400
		loss: 1.672400
		loss: 1.672400
		loss: 1.672400
		loss: 1.672400
		loss: 1.672300
		loss: 1.672300
		loss: 1.672300
		loss: 1.672300
		loss: 1.672200
		loss: 1.672200
		loss: 1.672200
		loss: 1.672200
		loss: 1.672100
		loss: 1.672100
		loss: 1.672100
		loss: 1.672100
		loss: 1.672100
		loss: 1.672000
		loss: 1.672000
		loss: 1.672000
		loss: 1.672000
		loss: 1.672000
		loss: 1.671900
		loss: 1.671900
		loss: 1.671900
		loss: 1.671900
		loss: 1.671800
		loss: 1.671800
		loss: 1.671800
		loss: 1.671800
		loss: 1.671800
		loss: 1.671700
		loss: 1.671700
		loss: 1.671700
		loss: 1.671700
		loss: 1.671700
		loss: 1.671600
		loss: 1.671600
		loss: 1.671600
		loss: 1.671600
		loss: 1.671600
		loss: 1.671600
		loss: 1.671500
		loss: 1.671500
		loss: 1.671500
		loss: 1.671500
		loss: 1.671500
		loss: 1.671400
		loss: 1.671400
		loss: 1.671400
		loss: 1.671400
		loss: 1.671400
		loss: 1.671400
		loss: 1.671300
		loss: 1.671300
		loss: 1.671300
		loss: 1.671300
		loss: 1.671300
		loss: 1.671200
		loss: 1.671200
		loss: 1.671200
		loss: 1.671200
		loss: 1.671200
		loss: 1.671200
		loss: 1.671100
		loss: 1.671100
		loss: 1.671100
		loss: 1.671100
		loss: 1.671100
		loss: 1.671100
		loss: 1.671000
		loss: 1.671000
		loss: 1.671000
		loss: 1.671000
		loss: 1.671000
		loss: 1.671000
		loss: 1.670900
		loss: 1.670900
		loss: 1.670900
		loss: 1.670900
		loss: 1.670900
		loss: 1.670900
		loss: 1.670800
		loss: 1.670800
		loss: 1.670800
		loss: 1.670800
		loss: 1.670800
		loss: 1.670800
		loss: 1.670700
	Overall the loss development was 2.106500 -> 1.670700

Training data for problem d-04.pddl in epoch 1:
model creation time: 64.55152988433838s
problem epoch data for epoch 1, problem epoch 1
	sampling search time: 34.687849283218384s
	during this search the following actions were chosen:
	training time: 296.14674949645996s
	during the training the following losses were computed:
		loss: 7.657700
		loss: 6.948500
		loss: 6.337300
		loss: 5.800100
		loss: 5.320500
		loss: 4.891800
		loss: 4.513400
		loss: 4.186500
		loss: 3.911900
		loss: 3.690000
		loss: 3.519200
		loss: 3.393200
		loss: 3.305500
		loss: 3.248700
		loss: 3.215800
		loss: 3.199800
		loss: 3.193700
		loss: 3.192400
		loss: 3.192300
		loss: 3.191400
		loss: 3.189000
		loss: 3.185600
		loss: 3.181800
		loss: 3.177800
		loss: 3.173300
		loss: 3.168100
		loss: 3.161900
		loss: 3.154500
		loss: 3.145500
		loss: 3.135000
		loss: 3.123000
		loss: 3.109600
		loss: 3.095400
		loss: 3.080500
		loss: 3.065700
		loss: 3.051000
		loss: 3.036600
		loss: 3.023100
		loss: 3.010100
		loss: 2.997300
		loss: 2.985000
		loss: 2.972900
		loss: 2.961100
		loss: 2.949400
		loss: 2.937700
		loss: 2.925800
		loss: 2.913900
		loss: 2.901900
		loss: 2.890000
		loss: 2.878500
		loss: 2.867600
		loss: 2.857300
		loss: 2.847600
		loss: 2.838300
		loss: 2.828900
		loss: 2.819100
		loss: 2.808800
		loss: 2.798100
		loss: 2.787600
		loss: 2.777700
		loss: 2.768300
		loss: 2.759300
		loss: 2.750400
		loss: 2.741700
		loss: 2.733600
		loss: 2.725800
		loss: 2.718300
		loss: 2.711400
		loss: 2.705300
		loss: 2.699400
		loss: 2.693500
		loss: 2.688000
		loss: 2.682900
		loss: 2.678200
		loss: 2.673700
		loss: 2.669400
		loss: 2.665300
		loss: 2.661400
		loss: 2.657700
		loss: 2.654300
		loss: 2.651000
		loss: 2.647800
		loss: 2.644900
		loss: 2.642200
		loss: 2.639500
		loss: 2.637000
		loss: 2.634800
		loss: 2.632700
		loss: 2.630700
		loss: 2.628700
		loss: 2.626800
		loss: 2.625000
		loss: 2.623200
		loss: 2.621400
		loss: 2.619700
		loss: 2.618100
		loss: 2.616600
		loss: 2.615100
		loss: 2.613700
		loss: 2.612200
		loss: 2.610800
		loss: 2.609400
		loss: 2.608000
		loss: 2.606700
		loss: 2.605500
		loss: 2.604200
		loss: 2.602900
		loss: 2.601700
		loss: 2.600400
		loss: 2.599300
		loss: 2.598100
		loss: 2.596900
		loss: 2.595800
		loss: 2.594600
		loss: 2.593500
		loss: 2.592400
		loss: 2.591400
		loss: 2.590300
		loss: 2.589200
		loss: 2.588200
		loss: 2.587200
		loss: 2.586100
		loss: 2.585100
		loss: 2.584100
		loss: 2.583200
		loss: 2.582200
		loss: 2.581200
		loss: 2.580300
		loss: 2.579300
		loss: 2.578400
		loss: 2.577500
		loss: 2.576600
		loss: 2.575700
		loss: 2.574800
		loss: 2.573900
		loss: 2.573000
		loss: 2.572200
		loss: 2.571300
		loss: 2.570400
		loss: 2.569600
		loss: 2.568800
		loss: 2.568000
		loss: 2.567100
		loss: 2.566300
		loss: 2.565500
		loss: 2.564700
		loss: 2.564000
		loss: 2.563200
		loss: 2.562400
		loss: 2.561700
		loss: 2.560900
		loss: 2.560200
		loss: 2.559400
		loss: 2.558700
		loss: 2.558000
		loss: 2.557200
		loss: 2.556500
		loss: 2.555800
		loss: 2.555100
		loss: 2.554400
		loss: 2.553700
		loss: 2.553100
		loss: 2.552400
		loss: 2.551700
		loss: 2.551100
		loss: 2.550400
		loss: 2.549800
		loss: 2.549100
		loss: 2.548500
		loss: 2.547900
		loss: 2.547200
		loss: 2.546600
		loss: 2.546000
		loss: 2.545400
		loss: 2.544800
		loss: 2.544200
		loss: 2.543600
		loss: 2.543000
		loss: 2.542400
		loss: 2.541800
		loss: 2.541300
		loss: 2.540700
		loss: 2.540100
		loss: 2.539600
		loss: 2.539000
		loss: 2.538500
		loss: 2.538000
		loss: 2.537400
		loss: 2.536900
		loss: 2.536400
		loss: 2.535800
		loss: 2.535300
		loss: 2.534800
		loss: 2.534300
		loss: 2.533800
		loss: 2.533300
		loss: 2.532800
		loss: 2.532300
		loss: 2.531800
		loss: 2.531400
		loss: 2.530900
		loss: 2.530400
		loss: 2.529900
		loss: 2.529500
		loss: 2.529000
		loss: 2.528500
		loss: 2.528100
		loss: 2.527600
		loss: 2.527200
		loss: 2.526800
		loss: 2.526300
		loss: 2.525900
		loss: 2.525500
		loss: 2.525000
		loss: 2.524600
		loss: 2.524200
		loss: 2.523800
		loss: 2.523400
		loss: 2.523000
		loss: 2.522600
		loss: 2.522200
		loss: 2.521800
		loss: 2.521400
		loss: 2.521000
		loss: 2.520600
		loss: 2.520200
		loss: 2.519800
		loss: 2.519400
		loss: 2.519100
		loss: 2.518700
		loss: 2.518300
		loss: 2.518000
		loss: 2.517600
		loss: 2.517300
		loss: 2.516900
		loss: 2.516500
		loss: 2.516200
		loss: 2.515800
		loss: 2.515500
		loss: 2.515200
		loss: 2.514800
		loss: 2.514500
		loss: 2.514200
		loss: 2.513800
		loss: 2.513500
		loss: 2.513200
		loss: 2.512900
		loss: 2.512500
		loss: 2.512200
		loss: 2.511900
		loss: 2.511600
		loss: 2.511300
		loss: 2.511000
		loss: 2.510700
		loss: 2.510400
		loss: 2.510100
		loss: 2.509800
		loss: 2.509500
		loss: 2.509200
		loss: 2.508900
		loss: 2.508700
		loss: 2.508400
		loss: 2.508100
		loss: 2.507800
		loss: 2.507500
		loss: 2.507200
		loss: 2.507000
		loss: 2.506700
		loss: 2.506400
		loss: 2.506200
		loss: 2.505900
		loss: 2.505600
		loss: 2.505400
		loss: 2.505100
		loss: 2.504900
		loss: 2.504600
		loss: 2.504400
		loss: 2.504100
		loss: 2.503900
		loss: 2.503600
		loss: 2.503400
		loss: 2.503100
		loss: 2.502900
		loss: 2.502700
		loss: 2.502400
		loss: 2.502200
		loss: 2.502000
		loss: 2.501700
		loss: 2.501500
		loss: 2.501300
		loss: 2.501100
		loss: 2.500800
		loss: 2.500600
		loss: 2.500400
		loss: 2.500200
		loss: 2.500000
		loss: 2.499700
		loss: 2.499500
		loss: 2.499300
		loss: 2.499100
	Overall the loss development was 7.657700 -> 2.499100

Epoch 2:
Training data for problem d-01.pddl in epoch 2:
model creation time: 19.38369917869568s
problem epoch data for epoch 2, problem epoch 1
	sampling search time: 6.6947901248931885s
	during this search the following actions were chosen:
	training time: 76.5420036315918s
	during the training the following losses were computed:
		loss: 0.058800
		loss: 0.057900
		loss: 0.056600
		loss: 0.055500
		loss: 0.054400
		loss: 0.053200
		loss: 0.052200
		loss: 0.051100
		loss: 0.050100
		loss: 0.049100
		loss: 0.048200
		loss: 0.047200
		loss: 0.046400
		loss: 0.045500
		loss: 0.044600
		loss: 0.043800
		loss: 0.043000
		loss: 0.042300
		loss: 0.041500
		loss: 0.040800
		loss: 0.040100
		loss: 0.039400
		loss: 0.038700
		loss: 0.038000
		loss: 0.037400
		loss: 0.036800
		loss: 0.036100
		loss: 0.035500
		loss: 0.035000
		loss: 0.034400
		loss: 0.033800
		loss: 0.033300
		loss: 0.032800
		loss: 0.032200
		loss: 0.031700
		loss: 0.031200
		loss: 0.030700
		loss: 0.030300
		loss: 0.029800
		loss: 0.029400
		loss: 0.028900
		loss: 0.028500
		loss: 0.028100
		loss: 0.027600
		loss: 0.027200
		loss: 0.026800
		loss: 0.026500
		loss: 0.026100
		loss: 0.025700
		loss: 0.025300
		loss: 0.025000
		loss: 0.024600
		loss: 0.024300
		loss: 0.024000
		loss: 0.023600
		loss: 0.023300
		loss: 0.023000
		loss: 0.022700
		loss: 0.022400
		loss: 0.022100
		loss: 0.021800
		loss: 0.021500
		loss: 0.021300
		loss: 0.021000
		loss: 0.020700
		loss: 0.020500
		loss: 0.020200
		loss: 0.020000
		loss: 0.019700
		loss: 0.019500
		loss: 0.019200
		loss: 0.019000
		loss: 0.018800
		loss: 0.018600
		loss: 0.018400
		loss: 0.018100
		loss: 0.017900
		loss: 0.017700
		loss: 0.017500
		loss: 0.017300
		loss: 0.017100
		loss: 0.017000
		loss: 0.016800
		loss: 0.016600
		loss: 0.016400
		loss: 0.016200
		loss: 0.016100
		loss: 0.015900
		loss: 0.015700
		loss: 0.015600
		loss: 0.015400
		loss: 0.015300
		loss: 0.015100
		loss: 0.015000
		loss: 0.014800
		loss: 0.014700
		loss: 0.014500
		loss: 0.014400
		loss: 0.014300
		loss: 0.014100
		loss: 0.014000
		loss: 0.013900
		loss: 0.013700
		loss: 0.013600
		loss: 0.013500
		loss: 0.013400
		loss: 0.013200
		loss: 0.013100
		loss: 0.013000
		loss: 0.012900
		loss: 0.012800
		loss: 0.012700
		loss: 0.012600
		loss: 0.012500
		loss: 0.012400
		loss: 0.012300
		loss: 0.012200
		loss: 0.012100
		loss: 0.012000
		loss: 0.011900
		loss: 0.011800
		loss: 0.011700
		loss: 0.011600
		loss: 0.011500
		loss: 0.011400
		loss: 0.011300
		loss: 0.011200
		loss: 0.011200
		loss: 0.011100
		loss: 0.011000
		loss: 0.010900
		loss: 0.010800
		loss: 0.010800
		loss: 0.010700
		loss: 0.010600
		loss: 0.010500
		loss: 0.010500
		loss: 0.010400
		loss: 0.010300
		loss: 0.010300
		loss: 0.010200
		loss: 0.010100
		loss: 0.010000
		loss: 0.010000
		loss: 0.009900
		loss: 0.009900
		loss: 0.009800
		loss: 0.009700
		loss: 0.009700
		loss: 0.009600
		loss: 0.009500
		loss: 0.009500
		loss: 0.009400
		loss: 0.009400
		loss: 0.009300
		loss: 0.009300
		loss: 0.009200
		loss: 0.009100
		loss: 0.009100
		loss: 0.009000
		loss: 0.009000
		loss: 0.008900
		loss: 0.008900
		loss: 0.008800
		loss: 0.008800
		loss: 0.008700
		loss: 0.008700
		loss: 0.008600
		loss: 0.008600
		loss: 0.008600
		loss: 0.008500
		loss: 0.008500
		loss: 0.008400
		loss: 0.008400
		loss: 0.008300
		loss: 0.008300
		loss: 0.008200
		loss: 0.008200
		loss: 0.008200
		loss: 0.008100
		loss: 0.008100
		loss: 0.008000
		loss: 0.008000
		loss: 0.008000
		loss: 0.007900
		loss: 0.007900
		loss: 0.007900
		loss: 0.007800
		loss: 0.007800
		loss: 0.007700
		loss: 0.007700
		loss: 0.007700
		loss: 0.007600
		loss: 0.007600
		loss: 0.007600
		loss: 0.007500
		loss: 0.007500
		loss: 0.007500
		loss: 0.007400
		loss: 0.007400
		loss: 0.007400
		loss: 0.007300
		loss: 0.007300
		loss: 0.007300
		loss: 0.007300
		loss: 0.007200
		loss: 0.007200
		loss: 0.007200
		loss: 0.007100
		loss: 0.007100
		loss: 0.007100
		loss: 0.007100
		loss: 0.007000
		loss: 0.007000
		loss: 0.007000
		loss: 0.007000
		loss: 0.006900
		loss: 0.006900
		loss: 0.006900
		loss: 0.006900
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005600
		loss: 0.005600
		loss: 0.005600
		loss: 0.005600
		loss: 0.005600
		loss: 0.005600
		loss: 0.005600
		loss: 0.005600
	Overall the loss development was 0.058800 -> 0.005600

Training data for problem d-02.pddl in epoch 2:
model creation time: 34.873331785202026s
problem epoch data for epoch 2, problem epoch 1
	sampling search time: 9.917089939117432s
	during this search the following actions were chosen:
	training time: 108.87103796005249s
	during the training the following losses were computed:
		loss: 1.045900
		loss: 0.977100
		loss: 0.923700
		loss: 0.867900
		loss: 0.818900
		loss: 0.778200
		loss: 0.741700
		loss: 0.711700
		loss: 0.686600
		loss: 0.666900
		loss: 0.651600
		loss: 0.640300
		loss: 0.632400
		loss: 0.627200
		loss: 0.624100
		loss: 0.622400
		loss: 0.621400
		loss: 0.620500
		loss: 0.619400
		loss: 0.617500
		loss: 0.614900
		loss: 0.611400
		loss: 0.607100
		loss: 0.601900
		loss: 0.596000
		loss: 0.589500
		loss: 0.582400
		loss: 0.575000
		loss: 0.567300
		loss: 0.559500
		loss: 0.551800
		loss: 0.544200
		loss: 0.536700
		loss: 0.529600
		loss: 0.523000
		loss: 0.516800
		loss: 0.511300
		loss: 0.506400
		loss: 0.502300
		loss: 0.498600
		loss: 0.495400
		loss: 0.492500
		loss: 0.490000
		loss: 0.487800
		loss: 0.486000
		loss: 0.484400
		loss: 0.483000
		loss: 0.481600
		loss: 0.480100
		loss: 0.478400
		loss: 0.476600
		loss: 0.474800
		loss: 0.472900
		loss: 0.470900
		loss: 0.468900
		loss: 0.466700
		loss: 0.464200
		loss: 0.461700
		loss: 0.459000
		loss: 0.456200
		loss: 0.453300
		loss: 0.450200
		loss: 0.446900
		loss: 0.443400
		loss: 0.439900
		loss: 0.436400
		loss: 0.432800
		loss: 0.429200
		loss: 0.425800
		loss: 0.422600
		loss: 0.419900
		loss: 0.417600
		loss: 0.415900
		loss: 0.414600
		loss: 0.413500
		loss: 0.412400
		loss: 0.411400
		loss: 0.410500
		loss: 0.409600
		loss: 0.408600
		loss: 0.407700
		loss: 0.406800
		loss: 0.405900
		loss: 0.405200
		loss: 0.404500
		loss: 0.403800
		loss: 0.403300
		loss: 0.402800
		loss: 0.402400
		loss: 0.402000
		loss: 0.401600
		loss: 0.401100
		loss: 0.400700
		loss: 0.400300
		loss: 0.399900
		loss: 0.399500
		loss: 0.399100
		loss: 0.398800
		loss: 0.398500
		loss: 0.398200
		loss: 0.397900
		loss: 0.397600
		loss: 0.397300
		loss: 0.397100
		loss: 0.396800
		loss: 0.396500
		loss: 0.396200
		loss: 0.395900
		loss: 0.395600
		loss: 0.395300
		loss: 0.395100
		loss: 0.394800
		loss: 0.394600
		loss: 0.394400
		loss: 0.394100
		loss: 0.393900
		loss: 0.393700
		loss: 0.393500
		loss: 0.393200
		loss: 0.393000
		loss: 0.392800
		loss: 0.392600
		loss: 0.392400
		loss: 0.392200
		loss: 0.392100
		loss: 0.391900
		loss: 0.391700
		loss: 0.391500
		loss: 0.391300
		loss: 0.391100
		loss: 0.391000
		loss: 0.390800
		loss: 0.390600
		loss: 0.390500
		loss: 0.390300
		loss: 0.390200
		loss: 0.390000
		loss: 0.389800
		loss: 0.389700
		loss: 0.389500
		loss: 0.389400
		loss: 0.389200
		loss: 0.389100
		loss: 0.389000
		loss: 0.388800
		loss: 0.388700
		loss: 0.388500
		loss: 0.388400
		loss: 0.388300
		loss: 0.388100
		loss: 0.388000
		loss: 0.387900
		loss: 0.387700
		loss: 0.387600
		loss: 0.387500
		loss: 0.387400
		loss: 0.387200
		loss: 0.387100
		loss: 0.387000
		loss: 0.386900
		loss: 0.386800
		loss: 0.386700
		loss: 0.386500
		loss: 0.386400
		loss: 0.386300
		loss: 0.386200
		loss: 0.386100
		loss: 0.386000
		loss: 0.385900
		loss: 0.385800
		loss: 0.385700
		loss: 0.385500
		loss: 0.385400
		loss: 0.385300
		loss: 0.385200
		loss: 0.385100
		loss: 0.385000
		loss: 0.384900
		loss: 0.384800
		loss: 0.384700
		loss: 0.384600
		loss: 0.384500
		loss: 0.384400
		loss: 0.384400
		loss: 0.384300
		loss: 0.384200
		loss: 0.384100
		loss: 0.384000
		loss: 0.383900
		loss: 0.383800
		loss: 0.383700
		loss: 0.383600
		loss: 0.383500
		loss: 0.383500
		loss: 0.383400
		loss: 0.383300
		loss: 0.383200
		loss: 0.383100
		loss: 0.383000
		loss: 0.383000
		loss: 0.382900
		loss: 0.382800
		loss: 0.382700
		loss: 0.382600
		loss: 0.382600
		loss: 0.382500
		loss: 0.382400
		loss: 0.382300
		loss: 0.382200
		loss: 0.382200
		loss: 0.382100
		loss: 0.382000
		loss: 0.381900
		loss: 0.381900
		loss: 0.381800
		loss: 0.381700
		loss: 0.381700
		loss: 0.381600
		loss: 0.381500
		loss: 0.381400
		loss: 0.381400
		loss: 0.381300
		loss: 0.381200
		loss: 0.381200
		loss: 0.381100
		loss: 0.381000
		loss: 0.381000
		loss: 0.380900
		loss: 0.380900
		loss: 0.380800
		loss: 0.380700
		loss: 0.380700
		loss: 0.380600
		loss: 0.380500
		loss: 0.380500
		loss: 0.380400
		loss: 0.380400
		loss: 0.380300
		loss: 0.380200
		loss: 0.380200
		loss: 0.380100
		loss: 0.380100
		loss: 0.380000
		loss: 0.380000
		loss: 0.379900
		loss: 0.379800
		loss: 0.379800
		loss: 0.379700
		loss: 0.379700
		loss: 0.379600
		loss: 0.379600
		loss: 0.379500
		loss: 0.379500
		loss: 0.379400
		loss: 0.379400
		loss: 0.379300
		loss: 0.379300
		loss: 0.379200
		loss: 0.379200
		loss: 0.379100
		loss: 0.379100
		loss: 0.379000
		loss: 0.379000
		loss: 0.379000
		loss: 0.378900
		loss: 0.378900
		loss: 0.378800
		loss: 0.378800
		loss: 0.378700
		loss: 0.378700
		loss: 0.378600
		loss: 0.378600
		loss: 0.378600
		loss: 0.378500
		loss: 0.378500
		loss: 0.378400
		loss: 0.378400
		loss: 0.378400
		loss: 0.378300
		loss: 0.378300
		loss: 0.378200
		loss: 0.378200
		loss: 0.378200
		loss: 0.378100
		loss: 0.378100
		loss: 0.378000
		loss: 0.378000
		loss: 0.378000
		loss: 0.377900
		loss: 0.377900
		loss: 0.377900
		loss: 0.377800
		loss: 0.377800
		loss: 0.377800
		loss: 0.377700
		loss: 0.377700
		loss: 0.377700
		loss: 0.377600
		loss: 0.377600
		loss: 0.377600
	Overall the loss development was 1.045900 -> 0.377600

Training data for problem d-03.pddl in epoch 2:
model creation time: 27.77871322631836s
problem epoch data for epoch 2, problem epoch 1
	sampling search time: 8.933602571487427s
	during this search the following actions were chosen:
	training time: 88.44689440727234s
	during the training the following losses were computed:
		loss: 2.119900
		loss: 2.035400
		loss: 1.971200
		loss: 1.925100
		loss: 1.893900
		loss: 1.874900
		loss: 1.863500
		loss: 1.858000
		loss: 1.856200
		loss: 1.856400
		loss: 1.857300
		loss: 1.857800
		loss: 1.857400
		loss: 1.855900
		loss: 1.853300
		loss: 1.849700
		loss: 1.845600
		loss: 1.841200
		loss: 1.836800
		loss: 1.832800
		loss: 1.829300
		loss: 1.826700
		loss: 1.824800
		loss: 1.823700
		loss: 1.823200
		loss: 1.823000
		loss: 1.822800
		loss: 1.822400
		loss: 1.821800
		loss: 1.820800
		loss: 1.819600
		loss: 1.818400
		loss: 1.817000
		loss: 1.815800
		loss: 1.814700
		loss: 1.813600
		loss: 1.812700
		loss: 1.811800
		loss: 1.811000
		loss: 1.810100
		loss: 1.809200
		loss: 1.808300
		loss: 1.807200
		loss: 1.806000
		loss: 1.804600
		loss: 1.803200
		loss: 1.801700
		loss: 1.800200
		loss: 1.798500
		loss: 1.796800
		loss: 1.795000
		loss: 1.793000
		loss: 1.790900
		loss: 1.788600
		loss: 1.786000
		loss: 1.783100
		loss: 1.779900
		loss: 1.776500
		loss: 1.772800
		loss: 1.768900
		loss: 1.764600
		loss: 1.760000
		loss: 1.755100
		loss: 1.749800
		loss: 1.744200
		loss: 1.738300
		loss: 1.732100
		loss: 1.726000
		loss: 1.719800
		loss: 1.713800
		loss: 1.708000
		loss: 1.702500
		loss: 1.697600
		loss: 1.693200
		loss: 1.689500
		loss: 1.686300
		loss: 1.683700
		loss: 1.681700
		loss: 1.680100
		loss: 1.679000
		loss: 1.678100
		loss: 1.677500
		loss: 1.677200
		loss: 1.676800
		loss: 1.676600
		loss: 1.676500
		loss: 1.676500
		loss: 1.676400
		loss: 1.676400
		loss: 1.676300
		loss: 1.676300
		loss: 1.676300
		loss: 1.676300
		loss: 1.676200
		loss: 1.676200
		loss: 1.676100
		loss: 1.676100
		loss: 1.676000
		loss: 1.676000
		loss: 1.675900
		loss: 1.675800
		loss: 1.675800
		loss: 1.675700
		loss: 1.675600
		loss: 1.675500
		loss: 1.675500
		loss: 1.675400
		loss: 1.675300
		loss: 1.675200
		loss: 1.675100
		loss: 1.675100
		loss: 1.675000
		loss: 1.674900
		loss: 1.674800
		loss: 1.674800
		loss: 1.674700
		loss: 1.674600
		loss: 1.674500
		loss: 1.674500
		loss: 1.674400
		loss: 1.674300
		loss: 1.674300
		loss: 1.674200
		loss: 1.674200
		loss: 1.674100
		loss: 1.674000
		loss: 1.674000
		loss: 1.673900
		loss: 1.673900
		loss: 1.673800
		loss: 1.673800
		loss: 1.673700
		loss: 1.673700
		loss: 1.673600
		loss: 1.673600
		loss: 1.673600
		loss: 1.673500
		loss: 1.673500
		loss: 1.673400
		loss: 1.673400
		loss: 1.673400
		loss: 1.673300
		loss: 1.673300
		loss: 1.673200
		loss: 1.673200
		loss: 1.673200
		loss: 1.673100
		loss: 1.673100
		loss: 1.673100
		loss: 1.673000
		loss: 1.673000
		loss: 1.673000
		loss: 1.672900
		loss: 1.672900
		loss: 1.672900
		loss: 1.672800
		loss: 1.672800
		loss: 1.672800
		loss: 1.672700
		loss: 1.672700
		loss: 1.672700
		loss: 1.672600
		loss: 1.672600
		loss: 1.672600
		loss: 1.672500
		loss: 1.672500
		loss: 1.672500
		loss: 1.672400
		loss: 1.672400
		loss: 1.672400
		loss: 1.672300
		loss: 1.672300
		loss: 1.672300
		loss: 1.672200
		loss: 1.672200
		loss: 1.672200
		loss: 1.672100
		loss: 1.672100
		loss: 1.672100
		loss: 1.672000
		loss: 1.672000
		loss: 1.672000
		loss: 1.672000
		loss: 1.671900
		loss: 1.671900
		loss: 1.671900
		loss: 1.671800
		loss: 1.671800
		loss: 1.671800
		loss: 1.671700
		loss: 1.671700
		loss: 1.671700
		loss: 1.671700
		loss: 1.671600
		loss: 1.671600
		loss: 1.671600
		loss: 1.671600
		loss: 1.671500
		loss: 1.671500
		loss: 1.671500
		loss: 1.671400
		loss: 1.671400
		loss: 1.671400
		loss: 1.671400
		loss: 1.671300
		loss: 1.671300
		loss: 1.671300
		loss: 1.671300
		loss: 1.671200
		loss: 1.671200
		loss: 1.671200
		loss: 1.671200
		loss: 1.671100
		loss: 1.671100
		loss: 1.671100
		loss: 1.671100
		loss: 1.671000
		loss: 1.671000
		loss: 1.671000
		loss: 1.671000
		loss: 1.670900
		loss: 1.670900
		loss: 1.670900
		loss: 1.670900
		loss: 1.670900
		loss: 1.670800
		loss: 1.670800
		loss: 1.670800
		loss: 1.670800
		loss: 1.670700
		loss: 1.670700
		loss: 1.670700
		loss: 1.670700
		loss: 1.670700
		loss: 1.670600
		loss: 1.670600
		loss: 1.670600
		loss: 1.670600
		loss: 1.670500
		loss: 1.670500
		loss: 1.670500
		loss: 1.670500
		loss: 1.670500
		loss: 1.670400
		loss: 1.670400
		loss: 1.670400
		loss: 1.670400
		loss: 1.670400
		loss: 1.670300
		loss: 1.670300
		loss: 1.670300
		loss: 1.670300
		loss: 1.670300
		loss: 1.670200
		loss: 1.670200
		loss: 1.670200
		loss: 1.670200
		loss: 1.670200
		loss: 1.670100
		loss: 1.670100
		loss: 1.670100
		loss: 1.670100
		loss: 1.670100
		loss: 1.670100
		loss: 1.670000
		loss: 1.670000
		loss: 1.670000
		loss: 1.670000
		loss: 1.670000
		loss: 1.669900
		loss: 1.669900
		loss: 1.669900
		loss: 1.669900
		loss: 1.669900
		loss: 1.669900
		loss: 1.669800
		loss: 1.669800
		loss: 1.669800
		loss: 1.669800
		loss: 1.669800
		loss: 1.669700
		loss: 1.669700
		loss: 1.669700
		loss: 1.669700
		loss: 1.669700
		loss: 1.669700
		loss: 1.669600
		loss: 1.669600
		loss: 1.669600
		loss: 1.669600
		loss: 1.669600
		loss: 1.669600
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669400
		loss: 1.669400
	Overall the loss development was 2.119900 -> 1.669400

Training data for problem d-04.pddl in epoch 2:
model creation time: 64.1333692073822s
problem epoch data for epoch 2, problem epoch 1
	sampling search time: 41.98466467857361s
	during this search the following actions were chosen:
	training time: 322.68229603767395s
	during the training the following losses were computed:
		loss: 7.126300
		loss: 6.457200
		loss: 5.892300
		loss: 5.405100
		loss: 4.978500
		loss: 4.604100
		loss: 4.287000
		loss: 4.005100
		loss: 3.769800
		loss: 3.572100
		loss: 3.413100
		loss: 3.291200
		loss: 3.200600
		loss: 3.135200
		loss: 3.090500
		loss: 3.065400
		loss: 3.050600
		loss: 3.040600
		loss: 3.034200
		loss: 3.028900
		loss: 3.024100
		loss: 3.018800
		loss: 3.013000
		loss: 3.006500
		loss: 2.999100
		loss: 2.990400
		loss: 2.980600
		loss: 2.969700
		loss: 2.957900
		loss: 2.945400
		loss: 2.932400
		loss: 2.919100
		loss: 2.905600
		loss: 2.891800
		loss: 2.878000
		loss: 2.863800
		loss: 2.849700
		loss: 2.835300
		loss: 2.820900
		loss: 2.807000
		loss: 2.793700
		loss: 2.781000
		loss: 2.769100
		loss: 2.758200
		loss: 2.748000
		loss: 2.738300
		loss: 2.729200
		loss: 2.720600
		loss: 2.712400
		loss: 2.704800
		loss: 2.697500
		loss: 2.690200
		loss: 2.683000
		loss: 2.676100
		loss: 2.669600
		loss: 2.663400
		loss: 2.657300
		loss: 2.651200
		loss: 2.645400
		loss: 2.639900
		loss: 2.634600
		loss: 2.629100
		loss: 2.623600
		loss: 2.618100
		loss: 2.612400
		loss: 2.606700
		loss: 2.601300
		loss: 2.596200
		loss: 2.591300
		loss: 2.586700
		loss: 2.582000
		loss: 2.577500
		loss: 2.573000
		loss: 2.568500
		loss: 2.564300
		loss: 2.560100
		loss: 2.556100
		loss: 2.552100
		loss: 2.548300
		loss: 2.544700
		loss: 2.541200
		loss: 2.537900
		loss: 2.534700
		loss: 2.531700
		loss: 2.528800
		loss: 2.526000
		loss: 2.523300
		loss: 2.520700
		loss: 2.518200
		loss: 2.515800
		loss: 2.513500
		loss: 2.511200
		loss: 2.509100
		loss: 2.507000
		loss: 2.505000
		loss: 2.503100
		loss: 2.501200
		loss: 2.499400
		loss: 2.497700
		loss: 2.496000
		loss: 2.494400
		loss: 2.492800
		loss: 2.491300
		loss: 2.489700
		loss: 2.488200
		loss: 2.486700
		loss: 2.485300
		loss: 2.483800
		loss: 2.482400
		loss: 2.481100
		loss: 2.479700
		loss: 2.478400
		loss: 2.477100
		loss: 2.475800
		loss: 2.474500
		loss: 2.473200
		loss: 2.472000
		loss: 2.470700
		loss: 2.469500
		loss: 2.468300
		loss: 2.467200
		loss: 2.466000
		loss: 2.464900
		loss: 2.463800
		loss: 2.462600
		loss: 2.461500
		loss: 2.460500
		loss: 2.459400
		loss: 2.458300
		loss: 2.457300
		loss: 2.456300
		loss: 2.455200
		loss: 2.454200
		loss: 2.453300
		loss: 2.452300
		loss: 2.451300
		loss: 2.450400
		loss: 2.449400
		loss: 2.448500
		loss: 2.447500
		loss: 2.446600
		loss: 2.445700
		loss: 2.444800
		loss: 2.444000
		loss: 2.443100
		loss: 2.442200
		loss: 2.441400
		loss: 2.440500
		loss: 2.439700
		loss: 2.438900
		loss: 2.438000
		loss: 2.437200
		loss: 2.436400
		loss: 2.435600
		loss: 2.434800
		loss: 2.434100
		loss: 2.433300
		loss: 2.432500
		loss: 2.431700
		loss: 2.431000
		loss: 2.430200
		loss: 2.429500
		loss: 2.428800
		loss: 2.428000
		loss: 2.427300
		loss: 2.426600
		loss: 2.425900
		loss: 2.425200
		loss: 2.424500
		loss: 2.423800
		loss: 2.423100
		loss: 2.422500
		loss: 2.421800
		loss: 2.421200
		loss: 2.420500
		loss: 2.419900
		loss: 2.419200
		loss: 2.418600
		loss: 2.418000
		loss: 2.417300
		loss: 2.416700
		loss: 2.416100
		loss: 2.415500
		loss: 2.414900
		loss: 2.414300
		loss: 2.413800
		loss: 2.413200
		loss: 2.412600
		loss: 2.412000
		loss: 2.411500
		loss: 2.410900
		loss: 2.410400
		loss: 2.409800
		loss: 2.409300
		loss: 2.408700
		loss: 2.408200
		loss: 2.407700
		loss: 2.407200
		loss: 2.406700
		loss: 2.406100
		loss: 2.405600
		loss: 2.405200
		loss: 2.404700
		loss: 2.404200
		loss: 2.403700
		loss: 2.403200
		loss: 2.402700
		loss: 2.402200
		loss: 2.401800
		loss: 2.401300
		loss: 2.400800
		loss: 2.400400
		loss: 2.399900
		loss: 2.399500
		loss: 2.399000
		loss: 2.398600
		loss: 2.398100
		loss: 2.397700
		loss: 2.397300
		loss: 2.396800
		loss: 2.396400
		loss: 2.396000
		loss: 2.395600
		loss: 2.395200
		loss: 2.394800
		loss: 2.394400
		loss: 2.394000
		loss: 2.393600
		loss: 2.393200
		loss: 2.392800
		loss: 2.392400
		loss: 2.392000
		loss: 2.391600
		loss: 2.391200
		loss: 2.390800
		loss: 2.390500
		loss: 2.390100
		loss: 2.389700
		loss: 2.389400
		loss: 2.389000
		loss: 2.388700
		loss: 2.388300
		loss: 2.387900
		loss: 2.387600
		loss: 2.387200
		loss: 2.386900
		loss: 2.386600
		loss: 2.386200
		loss: 2.385900
		loss: 2.385600
		loss: 2.385200
		loss: 2.384900
		loss: 2.384600
		loss: 2.384300
		loss: 2.383900
		loss: 2.383600
		loss: 2.383300
		loss: 2.383000
		loss: 2.382700
		loss: 2.382400
		loss: 2.382100
		loss: 2.381800
		loss: 2.381500
		loss: 2.381200
		loss: 2.380900
		loss: 2.380600
		loss: 2.380300
		loss: 2.380000
		loss: 2.379700
		loss: 2.379400
		loss: 2.379200
		loss: 2.378900
		loss: 2.378600
		loss: 2.378300
		loss: 2.378000
		loss: 2.377800
		loss: 2.377500
		loss: 2.377200
		loss: 2.377000
		loss: 2.376700
		loss: 2.376500
		loss: 2.376200
		loss: 2.375900
		loss: 2.375700
		loss: 2.375400
		loss: 2.375200
		loss: 2.374900
		loss: 2.374700
		loss: 2.374400
		loss: 2.374200
		loss: 2.374000
		loss: 2.373700
		loss: 2.373500
		loss: 2.373200
		loss: 2.373000
		loss: 2.372800
		loss: 2.372600
		loss: 2.372300
		loss: 2.372100
		loss: 2.371900
	Overall the loss development was 7.126300 -> 2.371900

Epoch 3:
Training data for problem d-01.pddl in epoch 3:
model creation time: 18.367470264434814s
problem epoch data for epoch 3, problem epoch 1
	sampling search time: 5.152872085571289s
	during this search the following actions were chosen:
	training time: 74.51051878929138s
	during the training the following losses were computed:
		loss: 0.058900
		loss: 0.058000
		loss: 0.056700
		loss: 0.055400
		loss: 0.054300
		loss: 0.053100
		loss: 0.052000
		loss: 0.051000
		loss: 0.049900
		loss: 0.048900
		loss: 0.048000
		loss: 0.047000
		loss: 0.046100
		loss: 0.045200
		loss: 0.044400
		loss: 0.043600
		loss: 0.042800
		loss: 0.042000
		loss: 0.041200
		loss: 0.040500
		loss: 0.039700
		loss: 0.039000
		loss: 0.038400
		loss: 0.037700
		loss: 0.037100
		loss: 0.036400
		loss: 0.035800
		loss: 0.035200
		loss: 0.034600
		loss: 0.034100
		loss: 0.033500
		loss: 0.033000
		loss: 0.032400
		loss: 0.031900
		loss: 0.031400
		loss: 0.030900
		loss: 0.030500
		loss: 0.030000
		loss: 0.029500
		loss: 0.029100
		loss: 0.028700
		loss: 0.028300
		loss: 0.027800
		loss: 0.027400
		loss: 0.027100
		loss: 0.026700
		loss: 0.026300
		loss: 0.025900
		loss: 0.025600
		loss: 0.025200
		loss: 0.024900
		loss: 0.024500
		loss: 0.024200
		loss: 0.023900
		loss: 0.023600
		loss: 0.023300
		loss: 0.023000
		loss: 0.022700
		loss: 0.022400
		loss: 0.022100
		loss: 0.021900
		loss: 0.021600
		loss: 0.021300
		loss: 0.021100
		loss: 0.020800
		loss: 0.020600
		loss: 0.020300
		loss: 0.020100
		loss: 0.019900
		loss: 0.019600
		loss: 0.019400
		loss: 0.019200
		loss: 0.019000
		loss: 0.018800
		loss: 0.018600
		loss: 0.018400
		loss: 0.018200
		loss: 0.018000
		loss: 0.017800
		loss: 0.017600
		loss: 0.017400
		loss: 0.017200
		loss: 0.017100
		loss: 0.016900
		loss: 0.016700
		loss: 0.016500
		loss: 0.016400
		loss: 0.016200
		loss: 0.016100
		loss: 0.015900
		loss: 0.015800
		loss: 0.015600
		loss: 0.015500
		loss: 0.015300
		loss: 0.015200
		loss: 0.015000
		loss: 0.014900
		loss: 0.014800
		loss: 0.014600
		loss: 0.014500
		loss: 0.014400
		loss: 0.014200
		loss: 0.014100
		loss: 0.014000
		loss: 0.013900
		loss: 0.013700
		loss: 0.013600
		loss: 0.013500
		loss: 0.013400
		loss: 0.013300
		loss: 0.013200
		loss: 0.013100
		loss: 0.013000
		loss: 0.012900
		loss: 0.012800
		loss: 0.012700
		loss: 0.012600
		loss: 0.012500
		loss: 0.012400
		loss: 0.012300
		loss: 0.012200
		loss: 0.012100
		loss: 0.012000
		loss: 0.011900
		loss: 0.011800
		loss: 0.011700
		loss: 0.011600
		loss: 0.011500
		loss: 0.011500
		loss: 0.011400
		loss: 0.011300
		loss: 0.011200
		loss: 0.011100
		loss: 0.011100
		loss: 0.011000
		loss: 0.010900
		loss: 0.010800
		loss: 0.010800
		loss: 0.010700
		loss: 0.010600
		loss: 0.010500
		loss: 0.010500
		loss: 0.010400
		loss: 0.010300
		loss: 0.010300
		loss: 0.010200
		loss: 0.010100
		loss: 0.010100
		loss: 0.010000
		loss: 0.010000
		loss: 0.009900
		loss: 0.009800
		loss: 0.009800
		loss: 0.009700
		loss: 0.009600
		loss: 0.009600
		loss: 0.009500
		loss: 0.009500
		loss: 0.009400
		loss: 0.009400
		loss: 0.009300
		loss: 0.009300
		loss: 0.009200
		loss: 0.009200
		loss: 0.009100
		loss: 0.009100
		loss: 0.009000
		loss: 0.009000
		loss: 0.008900
		loss: 0.008900
		loss: 0.008800
		loss: 0.008800
		loss: 0.008700
		loss: 0.008700
		loss: 0.008600
		loss: 0.008600
		loss: 0.008500
		loss: 0.008500
		loss: 0.008400
		loss: 0.008400
		loss: 0.008400
		loss: 0.008300
		loss: 0.008300
		loss: 0.008200
		loss: 0.008200
		loss: 0.008200
		loss: 0.008100
		loss: 0.008100
		loss: 0.008000
		loss: 0.008000
		loss: 0.008000
		loss: 0.007900
		loss: 0.007900
		loss: 0.007900
		loss: 0.007800
		loss: 0.007800
		loss: 0.007800
		loss: 0.007700
		loss: 0.007700
		loss: 0.007700
		loss: 0.007600
		loss: 0.007600
		loss: 0.007600
		loss: 0.007500
		loss: 0.007500
		loss: 0.007500
		loss: 0.007400
		loss: 0.007400
		loss: 0.007400
		loss: 0.007300
		loss: 0.007300
		loss: 0.007300
		loss: 0.007300
		loss: 0.007200
		loss: 0.007200
		loss: 0.007200
		loss: 0.007100
		loss: 0.007100
		loss: 0.007100
		loss: 0.007100
		loss: 0.007000
		loss: 0.007000
		loss: 0.007000
		loss: 0.007000
		loss: 0.006900
		loss: 0.006900
		loss: 0.006900
		loss: 0.006900
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
	Overall the loss development was 0.058900 -> 0.005700

Training data for problem d-02.pddl in epoch 3:
model creation time: 30.805100440979004s
problem epoch data for epoch 3, problem epoch 1
	sampling search time: 7.315032720565796s
	during this search the following actions were chosen:
	training time: 102.11588430404663s
	during the training the following losses were computed:
		loss: 0.871300
		loss: 0.828800
		loss: 0.791200
		loss: 0.758100
		loss: 0.729700
		loss: 0.706900
		loss: 0.687100
		loss: 0.671400
		loss: 0.659200
		loss: 0.650200
		loss: 0.643900
		loss: 0.639800
		loss: 0.637400
		loss: 0.636000
		loss: 0.634800
		loss: 0.633400
		loss: 0.631300
		loss: 0.628300
		loss: 0.624200
		loss: 0.619100
		loss: 0.613400
		loss: 0.607100
		loss: 0.600500
		loss: 0.593800
		loss: 0.587200
		loss: 0.580800
		loss: 0.574500
		loss: 0.568400
		loss: 0.562600
		loss: 0.557000
		loss: 0.551600
		loss: 0.546400
		loss: 0.541400
		loss: 0.536800
		loss: 0.532700
		loss: 0.529400
		loss: 0.527100
		loss: 0.525700
		loss: 0.525100
		loss: 0.524500
		loss: 0.523500
		loss: 0.522000
		loss: 0.520200
		loss: 0.518600
		loss: 0.517300
		loss: 0.516300
		loss: 0.515600
		loss: 0.514900
		loss: 0.514100
		loss: 0.513200
		loss: 0.512000
		loss: 0.510700
		loss: 0.509300
		loss: 0.508000
		loss: 0.506600
		loss: 0.505400
		loss: 0.504000
		loss: 0.502500
		loss: 0.500700
		loss: 0.498800
		loss: 0.496700
		loss: 0.494600
		loss: 0.492400
		loss: 0.490100
		loss: 0.487600
		loss: 0.484900
		loss: 0.482000
		loss: 0.479000
		loss: 0.476000
		loss: 0.473000
		loss: 0.470000
		loss: 0.467100
		loss: 0.464300
		loss: 0.461900
		loss: 0.459700
		loss: 0.457700
		loss: 0.456000
		loss: 0.454500
		loss: 0.453300
		loss: 0.452300
		loss: 0.451600
		loss: 0.450900
		loss: 0.450300
		loss: 0.449700
		loss: 0.449100
		loss: 0.448500
		loss: 0.447900
		loss: 0.447200
		loss: 0.446600
		loss: 0.446000
		loss: 0.445500
		loss: 0.444900
		loss: 0.444400
		loss: 0.443900
		loss: 0.443500
		loss: 0.443000
		loss: 0.442600
		loss: 0.442200
		loss: 0.441900
		loss: 0.441500
		loss: 0.441200
		loss: 0.440900
		loss: 0.440600
		loss: 0.440300
		loss: 0.440000
		loss: 0.439800
		loss: 0.439500
		loss: 0.439200
		loss: 0.438900
		loss: 0.438600
		loss: 0.438400
		loss: 0.438100
		loss: 0.437800
		loss: 0.437600
		loss: 0.437400
		loss: 0.437100
		loss: 0.436900
		loss: 0.436700
		loss: 0.436500
		loss: 0.436300
		loss: 0.436100
		loss: 0.435900
		loss: 0.435700
		loss: 0.435500
		loss: 0.435300
		loss: 0.435100
		loss: 0.434900
		loss: 0.434700
		loss: 0.434500
		loss: 0.434400
		loss: 0.434200
		loss: 0.434000
		loss: 0.433800
		loss: 0.433700
		loss: 0.433500
		loss: 0.433400
		loss: 0.433200
		loss: 0.433000
		loss: 0.432900
		loss: 0.432700
		loss: 0.432600
		loss: 0.432500
		loss: 0.432300
		loss: 0.432200
		loss: 0.432000
		loss: 0.431900
		loss: 0.431800
		loss: 0.431600
		loss: 0.431500
		loss: 0.431400
		loss: 0.431200
		loss: 0.431100
		loss: 0.431000
		loss: 0.430900
		loss: 0.430800
		loss: 0.430600
		loss: 0.430500
		loss: 0.430400
		loss: 0.430300
		loss: 0.430200
		loss: 0.430100
		loss: 0.430000
		loss: 0.429900
		loss: 0.429800
		loss: 0.429600
		loss: 0.429500
		loss: 0.429400
		loss: 0.429300
		loss: 0.429200
		loss: 0.429100
		loss: 0.429000
		loss: 0.429000
		loss: 0.428900
		loss: 0.428800
		loss: 0.428700
		loss: 0.428600
		loss: 0.428500
		loss: 0.428400
		loss: 0.428300
		loss: 0.428200
		loss: 0.428100
		loss: 0.428100
		loss: 0.428000
		loss: 0.427900
		loss: 0.427800
		loss: 0.427700
		loss: 0.427700
		loss: 0.427600
		loss: 0.427500
		loss: 0.427400
		loss: 0.427300
		loss: 0.427300
		loss: 0.427200
		loss: 0.427100
		loss: 0.427100
		loss: 0.427000
		loss: 0.426900
		loss: 0.426800
		loss: 0.426800
		loss: 0.426700
		loss: 0.426600
		loss: 0.426600
		loss: 0.426500
		loss: 0.426400
		loss: 0.426400
		loss: 0.426300
		loss: 0.426300
		loss: 0.426200
		loss: 0.426100
		loss: 0.426100
		loss: 0.426000
		loss: 0.426000
		loss: 0.425900
		loss: 0.425800
		loss: 0.425800
		loss: 0.425700
		loss: 0.425700
		loss: 0.425600
		loss: 0.425600
		loss: 0.425500
		loss: 0.425400
		loss: 0.425400
		loss: 0.425300
		loss: 0.425300
		loss: 0.425200
		loss: 0.425200
		loss: 0.425100
		loss: 0.425100
		loss: 0.425000
		loss: 0.425000
		loss: 0.424900
		loss: 0.424900
		loss: 0.424800
		loss: 0.424800
		loss: 0.424700
		loss: 0.424700
		loss: 0.424700
		loss: 0.424600
		loss: 0.424600
		loss: 0.424500
		loss: 0.424500
		loss: 0.424400
		loss: 0.424400
		loss: 0.424300
		loss: 0.424300
		loss: 0.424300
		loss: 0.424200
		loss: 0.424200
		loss: 0.424100
		loss: 0.424100
		loss: 0.424100
		loss: 0.424000
		loss: 0.424000
		loss: 0.423900
		loss: 0.423900
		loss: 0.423900
		loss: 0.423800
		loss: 0.423800
		loss: 0.423800
		loss: 0.423700
		loss: 0.423700
		loss: 0.423600
		loss: 0.423600
		loss: 0.423600
		loss: 0.423500
		loss: 0.423500
		loss: 0.423500
		loss: 0.423400
		loss: 0.423400
		loss: 0.423400
		loss: 0.423300
		loss: 0.423300
		loss: 0.423300
		loss: 0.423300
		loss: 0.423200
		loss: 0.423200
		loss: 0.423200
		loss: 0.423100
		loss: 0.423100
		loss: 0.423100
		loss: 0.423000
		loss: 0.423000
		loss: 0.423000
		loss: 0.423000
		loss: 0.422900
		loss: 0.422900
		loss: 0.422900
		loss: 0.422800
		loss: 0.422800
		loss: 0.422800
		loss: 0.422800
		loss: 0.422700
		loss: 0.422700
		loss: 0.422700
		loss: 0.422700
		loss: 0.422600
		loss: 0.422600
		loss: 0.422600
		loss: 0.422600
		loss: 0.422500
	Overall the loss development was 0.871300 -> 0.422500

Training data for problem d-03.pddl in epoch 3:
model creation time: 26.56689977645874s
problem epoch data for epoch 3, problem epoch 1
	sampling search time: 6.942477464675903s
	during this search the following actions were chosen:
	training time: 86.75339031219482s
	during the training the following losses were computed:
		loss: 2.901600
		loss: 2.880100
		loss: 2.863500
		loss: 2.850300
		loss: 2.837400
		loss: 2.826100
		loss: 2.816400
		loss: 2.807900
		loss: 2.800000
		loss: 2.792800
		loss: 2.786400
		loss: 2.781100
		loss: 2.777100
		loss: 2.774300
		loss: 2.772700
		loss: 2.771900
		loss: 2.771600
		loss: 2.771300
		loss: 2.770900
		loss: 2.770100
		loss: 2.768900
		loss: 2.767500
		loss: 2.765900
		loss: 2.764400
		loss: 2.763100
		loss: 2.761800
		loss: 2.760700
		loss: 2.759500
		loss: 2.758200
		loss: 2.756600
		loss: 2.754700
		loss: 2.752500
		loss: 2.749800
		loss: 2.746600
		loss: 2.742800
		loss: 2.738600
		loss: 2.733800
		loss: 2.728300
		loss: 2.722300
		loss: 2.715500
		loss: 2.707900
		loss: 2.699300
		loss: 2.689600
		loss: 2.678700
		loss: 2.666500
		loss: 2.652800
		loss: 2.637600
		loss: 2.620700
		loss: 2.602300
		loss: 2.582400
		loss: 2.560700
		loss: 2.537700
		loss: 2.513500
		loss: 2.488800
		loss: 2.463900
		loss: 2.439300
		loss: 2.415700
		loss: 2.393900
		loss: 2.374200
		loss: 2.357400
		loss: 2.343600
		loss: 2.332600
		loss: 2.324300
		loss: 2.318300
		loss: 2.314000
		loss: 2.310900
		loss: 2.308800
		loss: 2.307500
		loss: 2.306700
		loss: 2.306200
		loss: 2.305900
		loss: 2.305700
		loss: 2.305700
		loss: 2.305700
		loss: 2.305700
		loss: 2.305700
		loss: 2.305700
		loss: 2.305800
		loss: 2.305900
		loss: 2.305900
		loss: 2.306000
		loss: 2.306200
		loss: 2.306200
		loss: 2.306000
		loss: 2.305900
		loss: 2.306100
		loss: 2.306000
		loss: 2.305800
		loss: 2.305900
		loss: 2.305900
		loss: 2.305700
		loss: 2.305700
		loss: 2.305700
		loss: 2.305500
		loss: 2.305400
		loss: 2.305400
		loss: 2.305300
		loss: 2.305200
		loss: 2.305200
		loss: 2.305100
		loss: 2.305000
		loss: 2.305000
		loss: 2.304900
		loss: 2.304800
		loss: 2.304700
		loss: 2.304600
		loss: 2.304500
		loss: 2.304400
		loss: 2.304400
		loss: 2.304300
		loss: 2.304200
		loss: 2.304100
		loss: 2.304000
		loss: 2.303900
		loss: 2.303800
		loss: 2.303800
		loss: 2.303700
		loss: 2.303600
		loss: 2.303500
		loss: 2.303400
		loss: 2.303300
		loss: 2.303300
		loss: 2.303200
		loss: 2.303100
		loss: 2.303000
		loss: 2.303000
		loss: 2.302900
		loss: 2.302800
		loss: 2.302700
		loss: 2.302600
		loss: 2.302600
		loss: 2.302500
		loss: 2.302400
		loss: 2.302400
		loss: 2.302300
		loss: 2.302200
		loss: 2.302100
		loss: 2.302100
		loss: 2.302000
		loss: 2.301900
		loss: 2.301900
		loss: 2.301800
		loss: 2.301700
		loss: 2.301700
		loss: 2.301600
		loss: 2.301600
		loss: 2.301500
		loss: 2.301400
		loss: 2.301400
		loss: 2.301300
		loss: 2.301300
		loss: 2.301200
		loss: 2.301200
		loss: 2.301100
		loss: 2.301100
		loss: 2.301000
		loss: 2.301000
		loss: 2.300900
		loss: 2.300900
		loss: 2.300800
		loss: 2.300800
		loss: 2.300700
		loss: 2.300700
		loss: 2.300600
		loss: 2.300600
		loss: 2.300500
		loss: 2.300500
		loss: 2.300400
		loss: 2.300400
		loss: 2.300400
		loss: 2.300300
		loss: 2.300300
		loss: 2.300200
		loss: 2.300200
		loss: 2.300200
		loss: 2.300100
		loss: 2.300100
		loss: 2.300100
		loss: 2.300000
		loss: 2.300000
		loss: 2.300000
		loss: 2.299900
		loss: 2.299900
		loss: 2.299900
		loss: 2.299800
		loss: 2.299800
		loss: 2.299800
		loss: 2.299700
		loss: 2.299700
		loss: 2.299700
		loss: 2.299600
		loss: 2.299600
		loss: 2.299600
		loss: 2.299600
		loss: 2.299500
		loss: 2.299500
		loss: 2.299500
		loss: 2.299400
		loss: 2.299400
		loss: 2.299400
		loss: 2.299400
		loss: 2.299300
		loss: 2.299300
		loss: 2.299300
		loss: 2.299300
		loss: 2.299300
		loss: 2.299200
		loss: 2.299200
		loss: 2.299200
		loss: 2.299100
		loss: 2.299100
		loss: 2.299100
		loss: 2.299100
		loss: 2.299000
		loss: 2.299000
		loss: 2.299000
		loss: 2.299000
		loss: 2.298900
		loss: 2.298900
		loss: 2.298900
		loss: 2.298900
		loss: 2.298800
		loss: 2.298800
		loss: 2.298800
		loss: 2.298800
		loss: 2.298700
		loss: 2.298700
		loss: 2.298700
		loss: 2.298700
		loss: 2.298700
		loss: 2.298600
		loss: 2.298600
		loss: 2.298600
		loss: 2.298600
		loss: 2.298500
		loss: 2.298500
		loss: 2.298500
		loss: 2.298500
		loss: 2.298500
		loss: 2.298400
		loss: 2.298400
		loss: 2.298400
		loss: 2.298400
		loss: 2.298300
		loss: 2.298300
		loss: 2.298300
		loss: 2.298300
		loss: 2.298300
		loss: 2.298200
		loss: 2.298200
		loss: 2.298200
		loss: 2.298200
		loss: 2.298200
		loss: 2.298100
		loss: 2.298100
		loss: 2.298100
		loss: 2.298100
		loss: 2.298100
		loss: 2.298000
		loss: 2.298000
		loss: 2.298000
		loss: 2.298000
		loss: 2.298000
		loss: 2.297900
		loss: 2.297900
		loss: 2.297900
		loss: 2.297900
		loss: 2.297900
		loss: 2.297900
		loss: 2.297900
		loss: 2.297800
		loss: 2.297800
		loss: 2.297800
		loss: 2.297800
		loss: 2.297800
		loss: 2.297700
		loss: 2.297700
		loss: 2.297700
		loss: 2.297700
		loss: 2.297700
		loss: 2.297600
		loss: 2.297600
		loss: 2.297600
		loss: 2.297600
		loss: 2.297600
		loss: 2.297500
		loss: 2.297500
		loss: 2.297500
		loss: 2.297500
		loss: 2.297500
		loss: 2.297500
		loss: 2.297400
		loss: 2.297400
		loss: 2.297400
		loss: 2.297400
		loss: 2.297400
		loss: 2.297400
		loss: 2.297300
		loss: 2.297300
		loss: 2.297300
	Overall the loss development was 2.901600 -> 2.297300

Training data for problem d-04.pddl in epoch 3:
model creation time: 64.00030136108398s
problem epoch data for epoch 3, problem epoch 1
	sampling search time: 29.634597539901733s
	during this search the following actions were chosen:
	training time: 223.37195944786072s
	during the training the following losses were computed:
		loss: 9.947900
		loss: 9.136800
		loss: 8.447300
		loss: 7.838000
		loss: 7.324200
		loss: 6.832600
		loss: 6.399500
		loss: 6.018100
		loss: 5.688600
		loss: 5.399400
		loss: 5.147400
		loss: 4.930600
		loss: 4.743900
		loss: 4.582700
		loss: 4.443700
		loss: 4.324500
		loss: 4.228800
		loss: 4.149000
		loss: 4.083500
		loss: 4.034800
		loss: 3.998500
		loss: 3.967100
		loss: 3.938600
		loss: 3.915500
		loss: 3.896300
		loss: 3.880700
		loss: 3.869700
		loss: 3.862500
		loss: 3.855600
		loss: 3.849000
		loss: 3.842500
		loss: 3.836000
		loss: 3.829200
		loss: 3.822000
		loss: 3.814400
		loss: 3.806300
		loss: 3.797600
		loss: 3.788500
		loss: 3.778800
		loss: 3.768500
		loss: 3.758100
		loss: 3.748100
		loss: 3.738800
		loss: 3.730000
		loss: 3.721400
		loss: 3.712900
		loss: 3.704300
		loss: 3.696300
		loss: 3.689000
		loss: 3.681800
		loss: 3.674400
		loss: 3.667000
		loss: 3.659600
		loss: 3.652200
		loss: 3.644400
		loss: 3.636200
		loss: 3.627500
		loss: 3.618500
		loss: 3.609300
		loss: 3.600200
		loss: 3.591200
		loss: 3.581500
		loss: 3.571300
		loss: 3.560700
		loss: 3.549800
		loss: 3.538300
		loss: 3.526300
		loss: 3.512700
		loss: 3.498500
		loss: 3.483900
		loss: 3.469100
		loss: 3.454000
		loss: 3.438900
		loss: 3.423900
		loss: 3.409100
		loss: 3.395500
		loss: 3.382100
		loss: 3.368700
		loss: 3.355300
		loss: 3.341800
		loss: 3.328700
		loss: 3.315700
		loss: 3.303300
		loss: 3.291600
		loss: 3.280500
		loss: 3.270900
		loss: 3.261600
		loss: 3.252900
		loss: 3.244800
		loss: 3.237400
		loss: 3.230700
		loss: 3.224500
		loss: 3.219000
		loss: 3.213900
		loss: 3.209200
		loss: 3.204900
		loss: 3.201000
		loss: 3.197500
		loss: 3.194200
		loss: 3.191300
		loss: 3.188600
		loss: 3.186200
		loss: 3.183900
		loss: 3.181900
		loss: 3.180000
		loss: 3.178200
		loss: 3.176500
		loss: 3.174900
		loss: 3.173500
		loss: 3.172000
		loss: 3.170700
		loss: 3.169400
		loss: 3.168200
		loss: 3.166900
		loss: 3.165700
		loss: 3.164600
		loss: 3.163400
		loss: 3.162300
		loss: 3.161200
		loss: 3.160200
		loss: 3.159100
		loss: 3.158100
		loss: 3.157100
		loss: 3.156100
		loss: 3.155100
		loss: 3.154100
		loss: 3.153100
		loss: 3.152100
		loss: 3.151200
		loss: 3.150200
		loss: 3.149300
		loss: 3.148400
		loss: 3.147500
		loss: 3.146600
		loss: 3.145700
		loss: 3.144800
		loss: 3.143900
		loss: 3.143000
		loss: 3.142200
		loss: 3.141300
		loss: 3.140500
		loss: 3.139600
		loss: 3.138800
		loss: 3.138000
		loss: 3.137100
		loss: 3.136300
		loss: 3.135500
		loss: 3.134800
		loss: 3.134000
		loss: 3.133200
		loss: 3.132400
		loss: 3.131600
		loss: 3.130900
		loss: 3.130100
		loss: 3.129400
		loss: 3.128700
		loss: 3.127900
		loss: 3.127200
		loss: 3.126500
		loss: 3.125800
		loss: 3.125100
		loss: 3.124400
		loss: 3.123700
		loss: 3.123000
		loss: 3.122300
		loss: 3.121600
		loss: 3.120900
		loss: 3.120300
		loss: 3.119600
		loss: 3.119000
		loss: 3.118300
		loss: 3.117700
		loss: 3.117000
		loss: 3.116400
		loss: 3.115800
		loss: 3.115200
		loss: 3.114500
		loss: 3.113900
		loss: 3.113300
		loss: 3.112700
		loss: 3.112100
		loss: 3.111500
		loss: 3.111000
		loss: 3.110400
		loss: 3.109800
		loss: 3.109200
		loss: 3.108700
		loss: 3.108100
		loss: 3.107500
		loss: 3.107000
		loss: 3.106400
		loss: 3.105900
		loss: 3.105400
		loss: 3.104800
		loss: 3.104300
		loss: 3.103800
		loss: 3.103200
		loss: 3.102700
		loss: 3.102200
		loss: 3.101700
		loss: 3.101200
		loss: 3.100700
		loss: 3.100200
		loss: 3.099700
		loss: 3.099200
		loss: 3.098700
		loss: 3.098200
		loss: 3.097800
		loss: 3.097300
		loss: 3.096800
		loss: 3.096400
		loss: 3.095900
		loss: 3.095400
		loss: 3.095000
		loss: 3.094500
		loss: 3.094100
		loss: 3.093600
		loss: 3.093200
		loss: 3.092800
		loss: 3.092300
		loss: 3.091900
		loss: 3.091500
		loss: 3.091000
		loss: 3.090600
		loss: 3.090200
		loss: 3.089800
		loss: 3.089400
		loss: 3.089000
		loss: 3.088600
		loss: 3.088200
		loss: 3.087800
		loss: 3.087400
		loss: 3.087000
		loss: 3.086600
		loss: 3.086200
		loss: 3.085800
		loss: 3.085400
		loss: 3.085100
		loss: 3.084700
		loss: 3.084300
		loss: 3.083900
		loss: 3.083600
		loss: 3.083200
		loss: 3.082800
		loss: 3.082500
		loss: 3.082100
		loss: 3.081800
		loss: 3.081400
		loss: 3.081100
		loss: 3.080700
		loss: 3.080400
		loss: 3.080100
		loss: 3.079700
		loss: 3.079400
		loss: 3.079100
		loss: 3.078700
		loss: 3.078400
		loss: 3.078100
		loss: 3.077700
		loss: 3.077400
		loss: 3.077100
		loss: 3.076800
		loss: 3.076500
		loss: 3.076200
		loss: 3.075900
		loss: 3.075600
		loss: 3.075300
		loss: 3.075000
		loss: 3.074700
		loss: 3.074400
		loss: 3.074100
		loss: 3.073800
		loss: 3.073500
		loss: 3.073200
		loss: 3.072900
		loss: 3.072600
		loss: 3.072300
		loss: 3.072100
		loss: 3.071800
		loss: 3.071500
		loss: 3.071200
		loss: 3.071000
		loss: 3.070700
		loss: 3.070400
		loss: 3.070200
		loss: 3.069900
		loss: 3.069600
		loss: 3.069400
		loss: 3.069100
		loss: 3.068900
		loss: 3.068600
		loss: 3.068300
		loss: 3.068100
		loss: 3.067800
		loss: 3.067600
		loss: 3.067300
		loss: 3.067100
		loss: 3.066900
		loss: 3.066600
		loss: 3.066400
	Overall the loss development was 9.947900 -> 3.066400

Epoch 4:
Training data for problem d-01.pddl in epoch 4:
model creation time: 18.50487232208252s
problem epoch data for epoch 4, problem epoch 1
	sampling search time: 7.876451253890991s
	during this search the following actions were chosen:
	training time: 77.29761290550232s
	during the training the following losses were computed:
		loss: 0.067300
		loss: 0.065900
		loss: 0.064600
		loss: 0.063200
		loss: 0.062000
		loss: 0.060700
		loss: 0.059500
		loss: 0.058400
		loss: 0.057200
		loss: 0.056100
		loss: 0.055100
		loss: 0.054100
		loss: 0.053100
		loss: 0.052100
		loss: 0.051200
		loss: 0.050200
		loss: 0.049400
		loss: 0.048500
		loss: 0.047600
		loss: 0.046800
		loss: 0.046000
		loss: 0.045200
		loss: 0.044500
		loss: 0.043700
		loss: 0.043000
		loss: 0.042300
		loss: 0.041600
		loss: 0.040900
		loss: 0.040300
		loss: 0.039600
		loss: 0.039000
		loss: 0.038400
		loss: 0.037800
		loss: 0.037200
		loss: 0.036600
		loss: 0.036100
		loss: 0.035500
		loss: 0.035000
		loss: 0.034500
		loss: 0.034000
		loss: 0.033500
		loss: 0.033000
		loss: 0.032500
		loss: 0.032100
		loss: 0.031600
		loss: 0.031200
		loss: 0.030700
		loss: 0.030300
		loss: 0.029900
		loss: 0.029400
		loss: 0.029000
		loss: 0.028600
		loss: 0.028300
		loss: 0.027900
		loss: 0.027500
		loss: 0.027100
		loss: 0.026800
		loss: 0.026400
		loss: 0.026100
		loss: 0.025800
		loss: 0.025400
		loss: 0.025100
		loss: 0.024800
		loss: 0.024500
		loss: 0.024200
		loss: 0.023900
		loss: 0.023600
		loss: 0.023300
		loss: 0.023000
		loss: 0.022700
		loss: 0.022500
		loss: 0.022200
		loss: 0.021900
		loss: 0.021700
		loss: 0.021400
		loss: 0.021200
		loss: 0.020900
		loss: 0.020700
		loss: 0.020400
		loss: 0.020200
		loss: 0.020000
		loss: 0.019800
		loss: 0.019600
		loss: 0.019300
		loss: 0.019100
		loss: 0.018900
		loss: 0.018700
		loss: 0.018500
		loss: 0.018300
		loss: 0.018100
		loss: 0.017900
		loss: 0.017800
		loss: 0.017600
		loss: 0.017400
		loss: 0.017200
		loss: 0.017000
		loss: 0.016900
		loss: 0.016700
		loss: 0.016500
		loss: 0.016400
		loss: 0.016200
		loss: 0.016100
		loss: 0.015900
		loss: 0.015800
		loss: 0.015600
		loss: 0.015500
		loss: 0.015300
		loss: 0.015200
		loss: 0.015000
		loss: 0.014900
		loss: 0.014800
		loss: 0.014600
		loss: 0.014500
		loss: 0.014400
		loss: 0.014200
		loss: 0.014100
		loss: 0.014000
		loss: 0.013900
		loss: 0.013700
		loss: 0.013600
		loss: 0.013500
		loss: 0.013400
		loss: 0.013300
		loss: 0.013200
		loss: 0.013100
		loss: 0.013000
		loss: 0.012800
		loss: 0.012700
		loss: 0.012600
		loss: 0.012500
		loss: 0.012400
		loss: 0.012300
		loss: 0.012200
		loss: 0.012100
		loss: 0.012100
		loss: 0.012000
		loss: 0.011900
		loss: 0.011800
		loss: 0.011700
		loss: 0.011600
		loss: 0.011500
		loss: 0.011400
		loss: 0.011300
		loss: 0.011300
		loss: 0.011200
		loss: 0.011100
		loss: 0.011000
		loss: 0.010900
		loss: 0.010900
		loss: 0.010800
		loss: 0.010700
		loss: 0.010600
		loss: 0.010600
		loss: 0.010500
		loss: 0.010400
		loss: 0.010400
		loss: 0.010300
		loss: 0.010200
		loss: 0.010200
		loss: 0.010100
		loss: 0.010000
		loss: 0.010000
		loss: 0.009900
		loss: 0.009800
		loss: 0.009800
		loss: 0.009700
		loss: 0.009600
		loss: 0.009600
		loss: 0.009500
		loss: 0.009500
		loss: 0.009400
		loss: 0.009400
		loss: 0.009300
		loss: 0.009200
		loss: 0.009200
		loss: 0.009100
		loss: 0.009100
		loss: 0.009000
		loss: 0.009000
		loss: 0.008900
		loss: 0.008900
		loss: 0.008800
		loss: 0.008800
		loss: 0.008700
		loss: 0.008700
		loss: 0.008600
		loss: 0.008600
		loss: 0.008500
		loss: 0.008500
		loss: 0.008500
		loss: 0.008400
		loss: 0.008400
		loss: 0.008300
		loss: 0.008300
		loss: 0.008200
		loss: 0.008200
		loss: 0.008200
		loss: 0.008100
		loss: 0.008100
		loss: 0.008000
		loss: 0.008000
		loss: 0.008000
		loss: 0.007900
		loss: 0.007900
		loss: 0.007800
		loss: 0.007800
		loss: 0.007800
		loss: 0.007700
		loss: 0.007700
		loss: 0.007700
		loss: 0.007600
		loss: 0.007600
		loss: 0.007600
		loss: 0.007500
		loss: 0.007500
		loss: 0.007500
		loss: 0.007400
		loss: 0.007400
		loss: 0.007400
		loss: 0.007300
		loss: 0.007300
		loss: 0.007300
		loss: 0.007300
		loss: 0.007200
		loss: 0.007200
		loss: 0.007200
		loss: 0.007100
		loss: 0.007100
		loss: 0.007100
		loss: 0.007100
		loss: 0.007000
		loss: 0.007000
		loss: 0.007000
		loss: 0.006900
		loss: 0.006900
		loss: 0.006900
		loss: 0.006900
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
	Overall the loss development was 0.067300 -> 0.005800

Training data for problem d-02.pddl in epoch 4:
model creation time: 30.883620738983154s
problem epoch data for epoch 4, problem epoch 1
	sampling search time: 8.791155338287354s
	during this search the following actions were chosen:
	training time: 162.10768103599548s
	during the training the following losses were computed:
		loss: 1.326300
		loss: 1.240800
		loss: 1.174100
		loss: 1.104100
		loss: 1.042400
		loss: 0.988600
		loss: 0.942700
		loss: 0.905000
		loss: 0.874300
		loss: 0.850300
		loss: 0.832000
		loss: 0.818900
		loss: 0.810100
		loss: 0.804600
		loss: 0.801400
		loss: 0.799700
		loss: 0.798600
		loss: 0.797600
		loss: 0.796000
		loss: 0.793600
		loss: 0.790000
		loss: 0.785100
		loss: 0.779000
		loss: 0.771600
		loss: 0.763000
		loss: 0.753500
		loss: 0.743200
		loss: 0.732400
		loss: 0.721300
		loss: 0.710000
		loss: 0.698800
		loss: 0.687700
		loss: 0.677100
		loss: 0.667100
		loss: 0.658000
		loss: 0.650100
		loss: 0.643700
		loss: 0.638900
		loss: 0.635700
		loss: 0.633200
		loss: 0.630800
		loss: 0.628200
		loss: 0.625500
		loss: 0.623100
		loss: 0.621200
		loss: 0.619900
		loss: 0.618900
		loss: 0.618200
		loss: 0.617300
		loss: 0.616300
		loss: 0.615000
		loss: 0.613400
		loss: 0.611600
		loss: 0.609800
		loss: 0.608100
		loss: 0.606600
		loss: 0.605100
		loss: 0.603400
		loss: 0.601500
		loss: 0.599200
		loss: 0.596700
		loss: 0.594100
		loss: 0.591300
		loss: 0.588400
		loss: 0.585200
		loss: 0.581700
		loss: 0.577800
		loss: 0.573500
		loss: 0.568900
		loss: 0.564200
		loss: 0.559400
		loss: 0.554500
		loss: 0.549600
		loss: 0.545000
		loss: 0.541100
		loss: 0.537200
		loss: 0.533900
		loss: 0.530800
		loss: 0.528100
		loss: 0.525700
		loss: 0.523500
		loss: 0.521700
		loss: 0.520000
		loss: 0.518600
		loss: 0.517200
		loss: 0.516000
		loss: 0.514800
		loss: 0.513700
		loss: 0.512700
		loss: 0.511900
		loss: 0.511100
		loss: 0.510500
		loss: 0.510000
		loss: 0.509500
		loss: 0.509200
		loss: 0.508800
		loss: 0.508300
		loss: 0.507800
		loss: 0.507300
		loss: 0.506800
		loss: 0.506400
		loss: 0.506000
		loss: 0.505700
		loss: 0.505400
		loss: 0.505000
		loss: 0.504600
		loss: 0.504200
		loss: 0.503900
		loss: 0.503500
		loss: 0.503200
		loss: 0.502900
		loss: 0.502500
		loss: 0.502200
		loss: 0.501900
		loss: 0.501600
		loss: 0.501300
		loss: 0.501000
		loss: 0.500700
		loss: 0.500500
		loss: 0.500200
		loss: 0.500000
		loss: 0.499700
		loss: 0.499500
		loss: 0.499200
		loss: 0.499000
		loss: 0.498800
		loss: 0.498500
		loss: 0.498300
		loss: 0.498100
		loss: 0.497900
		loss: 0.497700
		loss: 0.497500
		loss: 0.497300
		loss: 0.497100
		loss: 0.496900
		loss: 0.496700
		loss: 0.496500
		loss: 0.496300
		loss: 0.496100
		loss: 0.495900
		loss: 0.495800
		loss: 0.495600
		loss: 0.495400
		loss: 0.495300
		loss: 0.495100
		loss: 0.494900
		loss: 0.494800
		loss: 0.494600
		loss: 0.494500
		loss: 0.494300
		loss: 0.494100
		loss: 0.494000
		loss: 0.493800
		loss: 0.493700
		loss: 0.493600
		loss: 0.493400
		loss: 0.493300
		loss: 0.493100
		loss: 0.493000
		loss: 0.492800
		loss: 0.492700
		loss: 0.492600
		loss: 0.492400
		loss: 0.492300
		loss: 0.492200
		loss: 0.492100
		loss: 0.491900
		loss: 0.491800
		loss: 0.491700
		loss: 0.491600
		loss: 0.491400
		loss: 0.491300
		loss: 0.491200
		loss: 0.491100
		loss: 0.491000
		loss: 0.490800
		loss: 0.490700
		loss: 0.490600
		loss: 0.490500
		loss: 0.490400
		loss: 0.490300
		loss: 0.490200
		loss: 0.490100
		loss: 0.490000
		loss: 0.489900
		loss: 0.489800
		loss: 0.489700
		loss: 0.489600
		loss: 0.489500
		loss: 0.489400
		loss: 0.489300
		loss: 0.489200
		loss: 0.489100
		loss: 0.489000
		loss: 0.488900
		loss: 0.488800
		loss: 0.488700
		loss: 0.488600
		loss: 0.488500
		loss: 0.488400
		loss: 0.488300
		loss: 0.488300
		loss: 0.488200
		loss: 0.488100
		loss: 0.488000
		loss: 0.487900
		loss: 0.487800
		loss: 0.487800
		loss: 0.487700
		loss: 0.487600
		loss: 0.487500
		loss: 0.487400
		loss: 0.487400
		loss: 0.487300
		loss: 0.487200
		loss: 0.487100
		loss: 0.487100
		loss: 0.487000
		loss: 0.486900
		loss: 0.486800
		loss: 0.486800
		loss: 0.486700
		loss: 0.486600
		loss: 0.486500
		loss: 0.486500
		loss: 0.486400
		loss: 0.486300
		loss: 0.486300
		loss: 0.486200
		loss: 0.486100
		loss: 0.486100
		loss: 0.486000
		loss: 0.485900
		loss: 0.485900
		loss: 0.485800
		loss: 0.485700
		loss: 0.485700
		loss: 0.485600
		loss: 0.485600
		loss: 0.485500
		loss: 0.485400
		loss: 0.485400
		loss: 0.485300
		loss: 0.485200
		loss: 0.485200
		loss: 0.485100
		loss: 0.485100
		loss: 0.485000
		loss: 0.485000
		loss: 0.484900
		loss: 0.484800
		loss: 0.484800
		loss: 0.484700
		loss: 0.484700
		loss: 0.484600
		loss: 0.484600
		loss: 0.484500
		loss: 0.484500
		loss: 0.484400
		loss: 0.484400
		loss: 0.484300
		loss: 0.484300
		loss: 0.484200
		loss: 0.484200
		loss: 0.484100
		loss: 0.484100
		loss: 0.484000
		loss: 0.484000
		loss: 0.483900
		loss: 0.483900
		loss: 0.483800
		loss: 0.483800
		loss: 0.483700
		loss: 0.483700
		loss: 0.483600
		loss: 0.483600
		loss: 0.483600
		loss: 0.483500
		loss: 0.483500
		loss: 0.483400
		loss: 0.483400
		loss: 0.483300
		loss: 0.483300
		loss: 0.483300
		loss: 0.483200
		loss: 0.483200
		loss: 0.483100
		loss: 0.483100
		loss: 0.483100
		loss: 0.483000
		loss: 0.483000
		loss: 0.482900
		loss: 0.482900
		loss: 0.482900
		loss: 0.482800
		loss: 0.482800
		loss: 0.482800
		loss: 0.482700
		loss: 0.482700
		loss: 0.482700
	Overall the loss development was 1.326300 -> 0.482700

Training data for problem d-03.pddl in epoch 4:
model creation time: 79.23651599884033s
problem epoch data for epoch 4, problem epoch 1
	sampling search time: 9.925137042999268s
	during this search the following actions were chosen:
	training time: 104.50129270553589s
	during the training the following losses were computed:
		loss: 2.628200
		loss: 2.446100
		loss: 2.317300
		loss: 2.237900
		loss: 2.200100
		loss: 2.192900
		loss: 2.203400
		loss: 2.219300
		loss: 2.231700
		loss: 2.236300
		loss: 2.232700
		loss: 2.222100
		loss: 2.207200
		loss: 2.190700
		loss: 2.175100
		loss: 2.162600
		loss: 2.154400
		loss: 2.150900
		loss: 2.151300
		loss: 2.153800
		loss: 2.156600
		loss: 2.157800
		loss: 2.156400
		loss: 2.152700
		loss: 2.147200
		loss: 2.141200
		loss: 2.135700
		loss: 2.131500
		loss: 2.129000
		loss: 2.127900
		loss: 2.127900
		loss: 2.128300
		loss: 2.128500
		loss: 2.128300
		loss: 2.127300
		loss: 2.126000
		loss: 2.124300
		loss: 2.122700
		loss: 2.121300
		loss: 2.120100
		loss: 2.119200
		loss: 2.118200
		loss: 2.117300
		loss: 2.116300
		loss: 2.115100
		loss: 2.113900
		loss: 2.112700
		loss: 2.111600
		loss: 2.110500
		loss: 2.109600
		loss: 2.108800
		loss: 2.107900
		loss: 2.106900
		loss: 2.105800
		loss: 2.104500
		loss: 2.103100
		loss: 2.101600
		loss: 2.100100
		loss: 2.098500
		loss: 2.096800
		loss: 2.095000
		loss: 2.093000
		loss: 2.090900
		loss: 2.088600
		loss: 2.086100
		loss: 2.083400
		loss: 2.080600
		loss: 2.077500
		loss: 2.074300
		loss: 2.070800
		loss: 2.067000
		loss: 2.062900
		loss: 2.058600
		loss: 2.054000
		loss: 2.049100
		loss: 2.044100
		loss: 2.038800
		loss: 2.033300
		loss: 2.027700
		loss: 2.022100
		loss: 2.016400
		loss: 2.010900
		loss: 2.005600
		loss: 2.000500
		loss: 1.995700
		loss: 1.991300
		loss: 1.987300
		loss: 1.983700
		loss: 1.980500
		loss: 1.977900
		loss: 1.975700
		loss: 1.973900
		loss: 1.972500
		loss: 1.971300
		loss: 1.970400
		loss: 1.969700
		loss: 1.969200
		loss: 1.968800
		loss: 1.968400
		loss: 1.968200
		loss: 1.968000
		loss: 1.967900
		loss: 1.967700
		loss: 1.967700
		loss: 1.967600
		loss: 1.967500
		loss: 1.967500
		loss: 1.967400
		loss: 1.967400
		loss: 1.967300
		loss: 1.967300
		loss: 1.967200
		loss: 1.967100
		loss: 1.967100
		loss: 1.967000
		loss: 1.967000
		loss: 1.966900
		loss: 1.966900
		loss: 1.966800
		loss: 1.966800
		loss: 1.966700
		loss: 1.966600
		loss: 1.966600
		loss: 1.966500
		loss: 1.966500
		loss: 1.966400
		loss: 1.966400
		loss: 1.966300
		loss: 1.966300
		loss: 1.966200
		loss: 1.966200
		loss: 1.966100
		loss: 1.966100
		loss: 1.966000
		loss: 1.966000
		loss: 1.965900
		loss: 1.965900
		loss: 1.965800
		loss: 1.965800
		loss: 1.965700
		loss: 1.965700
		loss: 1.965700
		loss: 1.965600
		loss: 1.965600
		loss: 1.965500
		loss: 1.965500
		loss: 1.965500
		loss: 1.965400
		loss: 1.965400
		loss: 1.965400
		loss: 1.965300
		loss: 1.965300
		loss: 1.965300
		loss: 1.965200
		loss: 1.965200
		loss: 1.965200
		loss: 1.965100
		loss: 1.965100
		loss: 1.965100
		loss: 1.965100
		loss: 1.965000
		loss: 1.965000
		loss: 1.965000
		loss: 1.965000
		loss: 1.964900
		loss: 1.964900
		loss: 1.964900
		loss: 1.964900
		loss: 1.964800
		loss: 1.964800
		loss: 1.964800
		loss: 1.964800
		loss: 1.964700
		loss: 1.964700
		loss: 1.964700
		loss: 1.964700
		loss: 1.964600
		loss: 1.964600
		loss: 1.964600
		loss: 1.964600
		loss: 1.964600
		loss: 1.964500
		loss: 1.964500
		loss: 1.964500
		loss: 1.964500
		loss: 1.964400
		loss: 1.964400
		loss: 1.964400
		loss: 1.964400
		loss: 1.964400
		loss: 1.964300
		loss: 1.964300
		loss: 1.964300
		loss: 1.964300
		loss: 1.964300
		loss: 1.964200
		loss: 1.964200
		loss: 1.964200
		loss: 1.964200
		loss: 1.964100
		loss: 1.964100
		loss: 1.964100
		loss: 1.964100
		loss: 1.964100
		loss: 1.964000
		loss: 1.964000
		loss: 1.964000
		loss: 1.964000
		loss: 1.964000
		loss: 1.963900
		loss: 1.963900
		loss: 1.963900
		loss: 1.963900
		loss: 1.963900
		loss: 1.963800
		loss: 1.963800
		loss: 1.963800
		loss: 1.963800
		loss: 1.963700
		loss: 1.963700
		loss: 1.963700
		loss: 1.963700
		loss: 1.963700
		loss: 1.963700
		loss: 1.963600
		loss: 1.963600
		loss: 1.963600
		loss: 1.963600
		loss: 1.963600
		loss: 1.963500
		loss: 1.963500
		loss: 1.963500
		loss: 1.963500
		loss: 1.963500
		loss: 1.963400
		loss: 1.963400
		loss: 1.963400
		loss: 1.963400
		loss: 1.963400
		loss: 1.963400
		loss: 1.963300
		loss: 1.963300
		loss: 1.963300
		loss: 1.963300
		loss: 1.963300
		loss: 1.963200
		loss: 1.963200
		loss: 1.963200
		loss: 1.963200
		loss: 1.963200
		loss: 1.963200
		loss: 1.963100
		loss: 1.963100
		loss: 1.963100
		loss: 1.963100
		loss: 1.963100
		loss: 1.963100
		loss: 1.963000
		loss: 1.963000
		loss: 1.963000
		loss: 1.963000
		loss: 1.963000
		loss: 1.963000
		loss: 1.963000
		loss: 1.962900
		loss: 1.962900
		loss: 1.962900
		loss: 1.962900
		loss: 1.962900
		loss: 1.962900
		loss: 1.962800
		loss: 1.962800
		loss: 1.962800
		loss: 1.962800
		loss: 1.962800
		loss: 1.962800
		loss: 1.962800
		loss: 1.962700
		loss: 1.962700
		loss: 1.962700
		loss: 1.962700
		loss: 1.962700
		loss: 1.962700
		loss: 1.962700
		loss: 1.962600
		loss: 1.962600
		loss: 1.962600
		loss: 1.962600
		loss: 1.962600
		loss: 1.962600
		loss: 1.962600
		loss: 1.962500
		loss: 1.962500
		loss: 1.962500
		loss: 1.962500
		loss: 1.962500
		loss: 1.962500
		loss: 1.962500
		loss: 1.962400
		loss: 1.962400
	Overall the loss development was 2.628200 -> 1.962400

Training data for problem d-04.pddl in epoch 4:
model creation time: 65.21811008453369s
problem epoch data for epoch 4, problem epoch 1
	sampling search time: 41.13698101043701s
	during this search the following actions were chosen:
	training time: 320.4916181564331s
	during the training the following losses were computed:
		loss: 7.166700
		loss: 6.666600
		loss: 6.066800
		loss: 5.529600
		loss: 5.044700
		loss: 4.630600
		loss: 4.252500
		loss: 3.931700
		loss: 3.667100
		loss: 3.456300
		loss: 3.297600
		loss: 3.198500
		loss: 3.150700
		loss: 3.119800
		loss: 3.109200
		loss: 3.110300
		loss: 3.111100
		loss: 3.110200
		loss: 3.107100
		loss: 3.102300
		loss: 3.096600
		loss: 3.089400
		loss: 3.080500
		loss: 3.070300
		loss: 3.058900
		loss: 3.046100
		loss: 3.032100
		loss: 3.017200
		loss: 3.001500
		loss: 2.985200
		loss: 2.968300
		loss: 2.951100
		loss: 2.933500
		loss: 2.915800
		loss: 2.898000
		loss: 2.880600
		loss: 2.863600
		loss: 2.846800
		loss: 2.830500
		loss: 2.814700
		loss: 2.799500
		loss: 2.784500
		loss: 2.769700
		loss: 2.755500
		loss: 2.742200
		loss: 2.729000
		loss: 2.716000
		loss: 2.703400
		loss: 2.691600
		loss: 2.680100
		loss: 2.669000
		loss: 2.658100
		loss: 2.647400
		loss: 2.636800
		loss: 2.626400
		loss: 2.616400
		loss: 2.607000
		loss: 2.598300
		loss: 2.590500
		loss: 2.583500
		loss: 2.576500
		loss: 2.569500
		loss: 2.563200
		loss: 2.557500
		loss: 2.551800
		loss: 2.546200
		loss: 2.540800
		loss: 2.535400
		loss: 2.530300
		loss: 2.525500
		loss: 2.520900
		loss: 2.516600
		loss: 2.512500
		loss: 2.508600
		loss: 2.504900
		loss: 2.501400
		loss: 2.498200
		loss: 2.495000
		loss: 2.492000
		loss: 2.489100
		loss: 2.486400
		loss: 2.483800
		loss: 2.481300
		loss: 2.479000
		loss: 2.476700
		loss: 2.474600
		loss: 2.472500
		loss: 2.470500
		loss: 2.468500
		loss: 2.466700
		loss: 2.464900
		loss: 2.463100
		loss: 2.461500
		loss: 2.459800
		loss: 2.458200
		loss: 2.456700
		loss: 2.455100
		loss: 2.453700
		loss: 2.452200
		loss: 2.450800
		loss: 2.449400
		loss: 2.448100
		loss: 2.446700
		loss: 2.445400
		loss: 2.444200
		loss: 2.442900
		loss: 2.441700
		loss: 2.440400
		loss: 2.439200
		loss: 2.438000
		loss: 2.436900
		loss: 2.435700
		loss: 2.434600
		loss: 2.433500
		loss: 2.432400
		loss: 2.431300
		loss: 2.430200
		loss: 2.429100
		loss: 2.428100
		loss: 2.427000
		loss: 2.426000
		loss: 2.425000
		loss: 2.424000
		loss: 2.423000
		loss: 2.422000
		loss: 2.421000
		loss: 2.420000
		loss: 2.419100
		loss: 2.418200
		loss: 2.417200
		loss: 2.416300
		loss: 2.415400
		loss: 2.414500
		loss: 2.413600
		loss: 2.412700
		loss: 2.411800
		loss: 2.410900
		loss: 2.410100
		loss: 2.409200
		loss: 2.408400
		loss: 2.407500
		loss: 2.406700
		loss: 2.405900
		loss: 2.405100
		loss: 2.404300
		loss: 2.403500
		loss: 2.402700
		loss: 2.401900
		loss: 2.401100
		loss: 2.400300
		loss: 2.399600
		loss: 2.398800
		loss: 2.398100
		loss: 2.397300
		loss: 2.396600
		loss: 2.395900
		loss: 2.395200
		loss: 2.394400
		loss: 2.393700
		loss: 2.393000
		loss: 2.392300
		loss: 2.391600
		loss: 2.391000
		loss: 2.390300
		loss: 2.389600
		loss: 2.388900
		loss: 2.388300
		loss: 2.387600
		loss: 2.387000
		loss: 2.386300
		loss: 2.385700
		loss: 2.385100
		loss: 2.384400
		loss: 2.383800
		loss: 2.383200
		loss: 2.382600
		loss: 2.382000
		loss: 2.381400
		loss: 2.380800
		loss: 2.380200
		loss: 2.379600
		loss: 2.379000
		loss: 2.378500
		loss: 2.377900
		loss: 2.377300
		loss: 2.376800
		loss: 2.376200
		loss: 2.375700
		loss: 2.375100
		loss: 2.374600
		loss: 2.374100
		loss: 2.373500
		loss: 2.373000
		loss: 2.372500
		loss: 2.372000
		loss: 2.371400
		loss: 2.370900
		loss: 2.370400
		loss: 2.369900
		loss: 2.369400
		loss: 2.368900
		loss: 2.368400
		loss: 2.368000
		loss: 2.367500
		loss: 2.367000
		loss: 2.366500
		loss: 2.366100
		loss: 2.365600
		loss: 2.365100
		loss: 2.364700
		loss: 2.364200
		loss: 2.363800
		loss: 2.363300
		loss: 2.362900
		loss: 2.362500
		loss: 2.362000
		loss: 2.361600
		loss: 2.361200
		loss: 2.360700
		loss: 2.360300
		loss: 2.359900
		loss: 2.359500
		loss: 2.359100
		loss: 2.358700
		loss: 2.358300
		loss: 2.357900
		loss: 2.357500
		loss: 2.357100
		loss: 2.356700
		loss: 2.356300
		loss: 2.355900
		loss: 2.355500
		loss: 2.355200
		loss: 2.354800
		loss: 2.354400
		loss: 2.354100
		loss: 2.353700
		loss: 2.353300
		loss: 2.353000
		loss: 2.352600
		loss: 2.352300
		loss: 2.351900
		loss: 2.351600
		loss: 2.351200
		loss: 2.350900
		loss: 2.350500
		loss: 2.350200
		loss: 2.349900
		loss: 2.349500
		loss: 2.349200
		loss: 2.348900
		loss: 2.348600
		loss: 2.348200
		loss: 2.347900
		loss: 2.347600
		loss: 2.347300
		loss: 2.347000
		loss: 2.346700
		loss: 2.346400
		loss: 2.346100
		loss: 2.345800
		loss: 2.345500
		loss: 2.345200
		loss: 2.344900
		loss: 2.344600
		loss: 2.344300
		loss: 2.344000
		loss: 2.343700
		loss: 2.343400
		loss: 2.343200
		loss: 2.342900
		loss: 2.342600
		loss: 2.342300
		loss: 2.342100
		loss: 2.341800
		loss: 2.341500
		loss: 2.341300
		loss: 2.341000
		loss: 2.340700
		loss: 2.340500
		loss: 2.340200
		loss: 2.340000
		loss: 2.339700
		loss: 2.339500
		loss: 2.339200
		loss: 2.339000
		loss: 2.338700
		loss: 2.338500
		loss: 2.338200
		loss: 2.338000
		loss: 2.337800
		loss: 2.337500
		loss: 2.337300
		loss: 2.337100
		loss: 2.336800
		loss: 2.336600
		loss: 2.336400
		loss: 2.336200
		loss: 2.335900
		loss: 2.335700
	Overall the loss development was 7.166700 -> 2.335700

Epoch 5:
Training data for problem d-01.pddl in epoch 5:
model creation time: 19.273933172225952s
problem epoch data for epoch 5, problem epoch 1
	sampling search time: 8.12182879447937s
	during this search the following actions were chosen:
	training time: 72.16196775436401s
	during the training the following losses were computed:
		loss: 0.061700
		loss: 0.060100
		loss: 0.058800
		loss: 0.057600
		loss: 0.056400
		loss: 0.055200
		loss: 0.054100
		loss: 0.053000
		loss: 0.051900
		loss: 0.050900
		loss: 0.049900
		loss: 0.048900
		loss: 0.047900
		loss: 0.047000
		loss: 0.046100
		loss: 0.045300
		loss: 0.044400
		loss: 0.043600
		loss: 0.042900
		loss: 0.042100
		loss: 0.041300
		loss: 0.040600
		loss: 0.039900
		loss: 0.039200
		loss: 0.038600
		loss: 0.037900
		loss: 0.037300
		loss: 0.036700
		loss: 0.036100
		loss: 0.035500
		loss: 0.034900
		loss: 0.034400
		loss: 0.033800
		loss: 0.033300
		loss: 0.032800
		loss: 0.032300
		loss: 0.031800
		loss: 0.031300
		loss: 0.030800
		loss: 0.030400
		loss: 0.029900
		loss: 0.029500
		loss: 0.029100
		loss: 0.028700
		loss: 0.028300
		loss: 0.027900
		loss: 0.027500
		loss: 0.027100
		loss: 0.026700
		loss: 0.026300
		loss: 0.026000
		loss: 0.025600
		loss: 0.025300
		loss: 0.024900
		loss: 0.024600
		loss: 0.024300
		loss: 0.023900
		loss: 0.023600
		loss: 0.023300
		loss: 0.023000
		loss: 0.022700
		loss: 0.022400
		loss: 0.022100
		loss: 0.021900
		loss: 0.021600
		loss: 0.021300
		loss: 0.021000
		loss: 0.020800
		loss: 0.020500
		loss: 0.020300
		loss: 0.020000
		loss: 0.019800
		loss: 0.019600
		loss: 0.019300
		loss: 0.019100
		loss: 0.018900
		loss: 0.018700
		loss: 0.018400
		loss: 0.018200
		loss: 0.018000
		loss: 0.017800
		loss: 0.017600
		loss: 0.017400
		loss: 0.017300
		loss: 0.017100
		loss: 0.016900
		loss: 0.016700
		loss: 0.016500
		loss: 0.016400
		loss: 0.016200
		loss: 0.016000
		loss: 0.015900
		loss: 0.015700
		loss: 0.015600
		loss: 0.015400
		loss: 0.015300
		loss: 0.015100
		loss: 0.015000
		loss: 0.014800
		loss: 0.014700
		loss: 0.014600
		loss: 0.014400
		loss: 0.014300
		loss: 0.014200
		loss: 0.014000
		loss: 0.013900
		loss: 0.013800
		loss: 0.013700
		loss: 0.013600
		loss: 0.013400
		loss: 0.013300
		loss: 0.013200
		loss: 0.013100
		loss: 0.013000
		loss: 0.012900
		loss: 0.012800
		loss: 0.012700
		loss: 0.012600
		loss: 0.012500
		loss: 0.012400
		loss: 0.012300
		loss: 0.012200
		loss: 0.012100
		loss: 0.012000
		loss: 0.011900
		loss: 0.011800
		loss: 0.011700
		loss: 0.011600
		loss: 0.011600
		loss: 0.011500
		loss: 0.011400
		loss: 0.011300
		loss: 0.011200
		loss: 0.011100
		loss: 0.011100
		loss: 0.011000
		loss: 0.010900
		loss: 0.010800
		loss: 0.010800
		loss: 0.010700
		loss: 0.010600
		loss: 0.010500
		loss: 0.010500
		loss: 0.010400
		loss: 0.010300
		loss: 0.010300
		loss: 0.010200
		loss: 0.010100
		loss: 0.010100
		loss: 0.010000
		loss: 0.010000
		loss: 0.009900
		loss: 0.009800
		loss: 0.009800
		loss: 0.009700
		loss: 0.009700
		loss: 0.009600
		loss: 0.009500
		loss: 0.009500
		loss: 0.009400
		loss: 0.009400
		loss: 0.009300
		loss: 0.009300
		loss: 0.009200
		loss: 0.009200
		loss: 0.009100
		loss: 0.009100
		loss: 0.009000
		loss: 0.009000
		loss: 0.008900
		loss: 0.008900
		loss: 0.008800
		loss: 0.008800
		loss: 0.008700
		loss: 0.008700
		loss: 0.008600
		loss: 0.008600
		loss: 0.008500
		loss: 0.008500
		loss: 0.008500
		loss: 0.008400
		loss: 0.008400
		loss: 0.008300
		loss: 0.008300
		loss: 0.008300
		loss: 0.008200
		loss: 0.008200
		loss: 0.008100
		loss: 0.008100
		loss: 0.008100
		loss: 0.008000
		loss: 0.008000
		loss: 0.008000
		loss: 0.007900
		loss: 0.007900
		loss: 0.007800
		loss: 0.007800
		loss: 0.007800
		loss: 0.007700
		loss: 0.007700
		loss: 0.007700
		loss: 0.007600
		loss: 0.007600
		loss: 0.007600
		loss: 0.007500
		loss: 0.007500
		loss: 0.007500
		loss: 0.007500
		loss: 0.007400
		loss: 0.007400
		loss: 0.007400
		loss: 0.007300
		loss: 0.007300
		loss: 0.007300
		loss: 0.007300
		loss: 0.007200
		loss: 0.007200
		loss: 0.007200
		loss: 0.007100
		loss: 0.007100
		loss: 0.007100
		loss: 0.007100
		loss: 0.007000
		loss: 0.007000
		loss: 0.007000
		loss: 0.007000
		loss: 0.006900
		loss: 0.006900
		loss: 0.006900
		loss: 0.006900
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
	Overall the loss development was 0.061700 -> 0.005700

Training data for problem d-02.pddl in epoch 5:
model creation time: 33.741456747055054s
problem epoch data for epoch 5, problem epoch 1
	sampling search time: 8.537106990814209s
	during this search the following actions were chosen:
	training time: 121.33907961845398s
	during the training the following losses were computed:
		loss: 1.184300
		loss: 1.125700
		loss: 1.053600
		loss: 0.994500
		loss: 0.936700
		loss: 0.886400
		loss: 0.842800
		loss: 0.805700
		loss: 0.774800
		loss: 0.748800
		loss: 0.727900
		loss: 0.711500
		loss: 0.699100
		loss: 0.690000
		loss: 0.683700
		loss: 0.679400
		loss: 0.676300
		loss: 0.673800
		loss: 0.671600
		loss: 0.669100
		loss: 0.666100
		loss: 0.662300
		loss: 0.657600
		loss: 0.652100
		loss: 0.645800
		loss: 0.638700
		loss: 0.631100
		loss: 0.623100
		loss: 0.614700
		loss: 0.606300
		loss: 0.597800
		loss: 0.589600
		loss: 0.581700
		loss: 0.574300
		loss: 0.567600
		loss: 0.561600
		loss: 0.556300
		loss: 0.551900
		loss: 0.548200
		loss: 0.545000
		loss: 0.542200
		loss: 0.539900
		loss: 0.537900
		loss: 0.536100
		loss: 0.534600
		loss: 0.533300
		loss: 0.532000
		loss: 0.530600
		loss: 0.529100
		loss: 0.527400
		loss: 0.525600
		loss: 0.523600
		loss: 0.521600
		loss: 0.519400
		loss: 0.517000
		loss: 0.514400
		loss: 0.511600
		loss: 0.508600
		loss: 0.505500
		loss: 0.502100
		loss: 0.498500
		loss: 0.494600
		loss: 0.490500
		loss: 0.486400
		loss: 0.482200
		loss: 0.478300
		loss: 0.474400
		loss: 0.470600
		loss: 0.466900
		loss: 0.463600
		loss: 0.461000
		loss: 0.458700
		loss: 0.456500
		loss: 0.454600
		loss: 0.452900
		loss: 0.451400
		loss: 0.450200
		loss: 0.449100
		loss: 0.448100
		loss: 0.447200
		loss: 0.446200
		loss: 0.445300
		loss: 0.444400
		loss: 0.443500
		loss: 0.442800
		loss: 0.442200
		loss: 0.441700
		loss: 0.441400
		loss: 0.441100
		loss: 0.440700
		loss: 0.440300
		loss: 0.439800
		loss: 0.439300
		loss: 0.438900
		loss: 0.438400
		loss: 0.438100
		loss: 0.437800
		loss: 0.437500
		loss: 0.437200
		loss: 0.436800
		loss: 0.436500
		loss: 0.436200
		loss: 0.435800
		loss: 0.435500
		loss: 0.435200
		loss: 0.434900
		loss: 0.434600
		loss: 0.434300
		loss: 0.434000
		loss: 0.433800
		loss: 0.433500
		loss: 0.433200
		loss: 0.433000
		loss: 0.432700
		loss: 0.432500
		loss: 0.432200
		loss: 0.432000
		loss: 0.431800
		loss: 0.431600
		loss: 0.431400
		loss: 0.431200
		loss: 0.431000
		loss: 0.430800
		loss: 0.430600
		loss: 0.430400
		loss: 0.430200
		loss: 0.430000
		loss: 0.429800
		loss: 0.429600
		loss: 0.429500
		loss: 0.429300
		loss: 0.429100
		loss: 0.429000
		loss: 0.428800
		loss: 0.428600
		loss: 0.428500
		loss: 0.428300
		loss: 0.428200
		loss: 0.428000
		loss: 0.427900
		loss: 0.427700
		loss: 0.427600
		loss: 0.427400
		loss: 0.427300
		loss: 0.427100
		loss: 0.427000
		loss: 0.426900
		loss: 0.426700
		loss: 0.426600
		loss: 0.426400
		loss: 0.426300
		loss: 0.426200
		loss: 0.426100
		loss: 0.425900
		loss: 0.425800
		loss: 0.425700
		loss: 0.425600
		loss: 0.425400
		loss: 0.425300
		loss: 0.425200
		loss: 0.425100
		loss: 0.425000
		loss: 0.424900
		loss: 0.424700
		loss: 0.424600
		loss: 0.424500
		loss: 0.424400
		loss: 0.424300
		loss: 0.424200
		loss: 0.424100
		loss: 0.424000
		loss: 0.423900
		loss: 0.423800
		loss: 0.423700
		loss: 0.423600
		loss: 0.423500
		loss: 0.423400
		loss: 0.423300
		loss: 0.423200
		loss: 0.423100
		loss: 0.423000
		loss: 0.422900
		loss: 0.422800
		loss: 0.422700
		loss: 0.422700
		loss: 0.422600
		loss: 0.422500
		loss: 0.422400
		loss: 0.422300
		loss: 0.422200
		loss: 0.422100
		loss: 0.422000
		loss: 0.422000
		loss: 0.421900
		loss: 0.421800
		loss: 0.421700
		loss: 0.421600
		loss: 0.421600
		loss: 0.421500
		loss: 0.421400
		loss: 0.421300
		loss: 0.421300
		loss: 0.421200
		loss: 0.421100
		loss: 0.421000
		loss: 0.420900
		loss: 0.420900
		loss: 0.420800
		loss: 0.420700
		loss: 0.420700
		loss: 0.420600
		loss: 0.420500
		loss: 0.420400
		loss: 0.420400
		loss: 0.420300
		loss: 0.420200
		loss: 0.420200
		loss: 0.420100
		loss: 0.420000
		loss: 0.420000
		loss: 0.419900
		loss: 0.419800
		loss: 0.419800
		loss: 0.419700
		loss: 0.419600
		loss: 0.419600
		loss: 0.419500
		loss: 0.419400
		loss: 0.419400
		loss: 0.419300
		loss: 0.419300
		loss: 0.419200
		loss: 0.419100
		loss: 0.419100
		loss: 0.419000
		loss: 0.419000
		loss: 0.418900
		loss: 0.418900
		loss: 0.418800
		loss: 0.418700
		loss: 0.418700
		loss: 0.418600
		loss: 0.418600
		loss: 0.418500
		loss: 0.418500
		loss: 0.418400
		loss: 0.418400
		loss: 0.418300
		loss: 0.418300
		loss: 0.418200
		loss: 0.418200
		loss: 0.418100
		loss: 0.418100
		loss: 0.418000
		loss: 0.418000
		loss: 0.417900
		loss: 0.417900
		loss: 0.417800
		loss: 0.417800
		loss: 0.417700
		loss: 0.417700
		loss: 0.417600
		loss: 0.417600
		loss: 0.417600
		loss: 0.417500
		loss: 0.417500
		loss: 0.417400
		loss: 0.417400
		loss: 0.417300
		loss: 0.417300
		loss: 0.417200
		loss: 0.417200
		loss: 0.417200
		loss: 0.417100
		loss: 0.417100
		loss: 0.417000
		loss: 0.417000
		loss: 0.417000
		loss: 0.416900
		loss: 0.416900
		loss: 0.416800
		loss: 0.416800
		loss: 0.416800
		loss: 0.416700
		loss: 0.416700
		loss: 0.416700
		loss: 0.416600
		loss: 0.416600
		loss: 0.416500
		loss: 0.416500
		loss: 0.416500
		loss: 0.416400
		loss: 0.416400
		loss: 0.416400
		loss: 0.416300
		loss: 0.416300
		loss: 0.416300
		loss: 0.416200
		loss: 0.416200
		loss: 0.416200
	Overall the loss development was 1.184300 -> 0.416200

Training data for problem d-03.pddl in epoch 5:
model creation time: 114.951664686203s
problem epoch data for epoch 5, problem epoch 1
	sampling search time: 9.783968448638916s
	during this search the following actions were chosen:
	training time: 127.59552693367004s
	during the training the following losses were computed:
		loss: 2.472000
		loss: 2.399700
		loss: 2.342900
		loss: 2.282600
		loss: 2.228300
		loss: 2.181700
		loss: 2.138200
		loss: 2.099700
		loss: 2.066200
		loss: 2.036200
		loss: 2.010000
		loss: 1.987000
		loss: 1.966900
		loss: 1.949200
		loss: 1.933900
		loss: 1.920500
		loss: 1.909000
		loss: 1.899200
		loss: 1.890800
		loss: 1.883800
		loss: 1.878200
		loss: 1.873900
		loss: 1.870600
		loss: 1.868200
		loss: 1.866300
		loss: 1.864600
		loss: 1.862900
		loss: 1.860700
		loss: 1.858000
		loss: 1.854800
		loss: 1.851200
		loss: 1.847100
		loss: 1.842800
		loss: 1.838300
		loss: 1.833900
		loss: 1.829800
		loss: 1.826100
		loss: 1.823000
		loss: 1.820500
		loss: 1.818400
		loss: 1.816500
		loss: 1.814700
		loss: 1.812800
		loss: 1.810800
		loss: 1.808600
		loss: 1.806300
		loss: 1.803900
		loss: 1.801500
		loss: 1.799000
		loss: 1.796500
		loss: 1.793900
		loss: 1.791200
		loss: 1.788500
		loss: 1.785700
		loss: 1.782800
		loss: 1.779800
		loss: 1.776600
		loss: 1.773200
		loss: 1.769600
		loss: 1.765500
		loss: 1.761200
		loss: 1.756300
		loss: 1.751100
		loss: 1.745600
		loss: 1.739700
		loss: 1.733700
		loss: 1.727500
		loss: 1.721400
		loss: 1.715400
		loss: 1.709700
		loss: 1.704200
		loss: 1.699200
		loss: 1.694700
		loss: 1.690900
		loss: 1.687600
		loss: 1.685000
		loss: 1.682800
		loss: 1.681100
		loss: 1.679900
		loss: 1.679100
		loss: 1.678400
		loss: 1.678000
		loss: 1.677700
		loss: 1.677500
		loss: 1.677300
		loss: 1.677100
		loss: 1.677100
		loss: 1.677000
		loss: 1.676900
		loss: 1.676800
		loss: 1.676600
		loss: 1.676600
		loss: 1.676500
		loss: 1.676300
		loss: 1.676200
		loss: 1.676100
		loss: 1.675900
		loss: 1.675800
		loss: 1.675600
		loss: 1.675500
		loss: 1.675300
		loss: 1.675200
		loss: 1.675100
		loss: 1.674900
		loss: 1.674800
		loss: 1.674600
		loss: 1.674500
		loss: 1.674400
		loss: 1.674200
		loss: 1.674100
		loss: 1.674000
		loss: 1.673900
		loss: 1.673700
		loss: 1.673600
		loss: 1.673500
		loss: 1.673400
		loss: 1.673300
		loss: 1.673200
		loss: 1.673100
		loss: 1.673000
		loss: 1.673000
		loss: 1.672900
		loss: 1.672800
		loss: 1.672700
		loss: 1.672700
		loss: 1.672600
		loss: 1.672500
		loss: 1.672500
		loss: 1.672400
		loss: 1.672300
		loss: 1.672300
		loss: 1.672200
		loss: 1.672100
		loss: 1.672100
		loss: 1.672000
		loss: 1.672000
		loss: 1.671900
		loss: 1.671900
		loss: 1.671800
		loss: 1.671800
		loss: 1.671700
		loss: 1.671700
		loss: 1.671600
		loss: 1.671600
		loss: 1.671500
		loss: 1.671500
		loss: 1.671400
		loss: 1.671400
		loss: 1.671300
		loss: 1.671300
		loss: 1.671300
		loss: 1.671200
		loss: 1.671200
		loss: 1.671100
		loss: 1.671100
		loss: 1.671000
		loss: 1.671000
		loss: 1.671000
		loss: 1.670900
		loss: 1.670900
		loss: 1.670800
		loss: 1.670800
		loss: 1.670800
		loss: 1.670700
		loss: 1.670700
		loss: 1.670700
		loss: 1.670600
		loss: 1.670600
		loss: 1.670500
		loss: 1.670500
		loss: 1.670500
		loss: 1.670400
		loss: 1.670400
		loss: 1.670400
		loss: 1.670300
		loss: 1.670300
		loss: 1.670300
		loss: 1.670200
		loss: 1.670200
		loss: 1.670200
		loss: 1.670100
		loss: 1.670100
		loss: 1.670100
		loss: 1.670000
		loss: 1.670000
		loss: 1.670000
		loss: 1.670000
		loss: 1.669900
		loss: 1.669900
		loss: 1.669900
		loss: 1.669800
		loss: 1.669800
		loss: 1.669800
		loss: 1.669800
		loss: 1.669700
		loss: 1.669700
		loss: 1.669700
		loss: 1.669600
		loss: 1.669600
		loss: 1.669600
		loss: 1.669600
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669400
		loss: 1.669400
		loss: 1.669400
		loss: 1.669400
		loss: 1.669300
		loss: 1.669300
		loss: 1.669300
		loss: 1.669300
		loss: 1.669200
		loss: 1.669200
		loss: 1.669200
		loss: 1.669200
		loss: 1.669200
		loss: 1.669100
		loss: 1.669100
		loss: 1.669100
		loss: 1.669100
		loss: 1.669000
		loss: 1.669000
		loss: 1.669000
		loss: 1.669000
		loss: 1.669000
		loss: 1.668900
		loss: 1.668900
		loss: 1.668900
		loss: 1.668900
		loss: 1.668900
		loss: 1.668800
		loss: 1.668800
		loss: 1.668800
		loss: 1.668800
		loss: 1.668800
		loss: 1.668700
		loss: 1.668700
		loss: 1.668700
		loss: 1.668700
		loss: 1.668700
		loss: 1.668600
		loss: 1.668600
		loss: 1.668600
		loss: 1.668600
		loss: 1.668600
		loss: 1.668600
		loss: 1.668500
		loss: 1.668500
		loss: 1.668500
		loss: 1.668500
		loss: 1.668500
		loss: 1.668400
		loss: 1.668400
		loss: 1.668400
		loss: 1.668400
		loss: 1.668400
		loss: 1.668400
		loss: 1.668300
		loss: 1.668300
		loss: 1.668300
		loss: 1.668300
		loss: 1.668300
		loss: 1.668300
		loss: 1.668200
		loss: 1.668200
		loss: 1.668200
		loss: 1.668200
		loss: 1.668200
		loss: 1.668200
		loss: 1.668100
		loss: 1.668100
		loss: 1.668100
		loss: 1.668100
		loss: 1.668100
		loss: 1.668100
		loss: 1.668100
		loss: 1.668000
		loss: 1.668000
		loss: 1.668000
		loss: 1.668000
		loss: 1.668000
		loss: 1.668000
		loss: 1.667900
		loss: 1.667900
		loss: 1.667900
		loss: 1.667900
		loss: 1.667900
		loss: 1.667900
		loss: 1.667900
		loss: 1.667800
		loss: 1.667800
		loss: 1.667800
		loss: 1.667800
		loss: 1.667800
		loss: 1.667800
		loss: 1.667800
		loss: 1.667700
		loss: 1.667700
	Overall the loss development was 2.472000 -> 1.667700

Training data for problem d-04.pddl in epoch 5:
model creation time: 66.27163505554199s
problem epoch data for epoch 5, problem epoch 1
	sampling search time: 57.757800817489624s
	during this search the following actions were chosen:
	training time: 335.45385122299194s
	during the training the following losses were computed:
		loss: 8.466200
		loss: 7.754200
		loss: 7.229900
		loss: 6.695500
		loss: 6.250100
		loss: 5.819000
		loss: 5.436100
		loss: 5.083000
		loss: 4.757500
		loss: 4.471100
		loss: 4.221400
		loss: 4.008600
		loss: 3.829200
		loss: 3.680000
		loss: 3.557900
		loss: 3.459000
		loss: 3.381500
		loss: 3.324100
		loss: 3.282400
		loss: 3.255100
		loss: 3.239300
		loss: 3.231100
		loss: 3.224100
		loss: 3.217400
		loss: 3.210900
		loss: 3.204400
		loss: 3.196700
		loss: 3.188500
		loss: 3.180200
		loss: 3.171800
		loss: 3.163500
		loss: 3.155300
		loss: 3.147300
		loss: 3.139400
		loss: 3.131500
		loss: 3.123600
		loss: 3.115700
		loss: 3.107800
		loss: 3.099700
		loss: 3.091500
		loss: 3.083200
		loss: 3.074700
		loss: 3.066200
		loss: 3.057700
		loss: 3.049100
		loss: 3.040500
		loss: 3.032000
		loss: 3.023700
		loss: 3.015500
		loss: 3.007400
		loss: 2.999500
		loss: 2.991500
		loss: 2.983800
		loss: 2.976900
		loss: 2.970600
		loss: 2.964400
		loss: 2.958300
		loss: 2.952300
		loss: 2.946200
		loss: 2.940000
		loss: 2.933400
		loss: 2.926300
		loss: 2.918800
		loss: 2.911100
		loss: 2.903000
		loss: 2.894600
		loss: 2.885900
		loss: 2.876800
		loss: 2.867400
		loss: 2.857800
		loss: 2.847800
		loss: 2.837400
		loss: 2.827100
		loss: 2.816800
		loss: 2.806900
		loss: 2.797100
		loss: 2.787400
		loss: 2.778000
		loss: 2.769600
		loss: 2.761100
		loss: 2.752500
		loss: 2.743800
		loss: 2.735400
		loss: 2.727800
		loss: 2.719900
		loss: 2.712000
		loss: 2.704300
		loss: 2.697200
		loss: 2.690100
		loss: 2.683500
		loss: 2.677200
		loss: 2.671500
		loss: 2.666100
		loss: 2.661200
		loss: 2.656600
		loss: 2.652400
		loss: 2.648500
		loss: 2.644900
		loss: 2.641500
		loss: 2.638200
		loss: 2.635100
		loss: 2.632300
		loss: 2.629600
		loss: 2.627000
		loss: 2.624700
		loss: 2.622400
		loss: 2.620300
		loss: 2.618300
		loss: 2.616300
		loss: 2.614500
		loss: 2.612700
		loss: 2.611000
		loss: 2.609400
		loss: 2.607900
		loss: 2.606300
		loss: 2.604900
		loss: 2.603400
		loss: 2.602100
		loss: 2.600700
		loss: 2.599400
		loss: 2.598200
		loss: 2.596900
		loss: 2.595700
		loss: 2.594500
		loss: 2.593400
		loss: 2.592300
		loss: 2.591100
		loss: 2.590000
		loss: 2.588900
		loss: 2.587800
		loss: 2.586800
		loss: 2.585800
		loss: 2.584700
		loss: 2.583700
		loss: 2.582700
		loss: 2.581700
		loss: 2.580800
		loss: 2.579800
		loss: 2.578900
		loss: 2.577900
		loss: 2.577000
		loss: 2.576100
		loss: 2.575200
		loss: 2.574300
		loss: 2.573500
		loss: 2.572600
		loss: 2.571700
		loss: 2.570900
		loss: 2.570100
		loss: 2.569300
		loss: 2.568500
		loss: 2.567600
		loss: 2.566800
		loss: 2.566100
		loss: 2.565300
		loss: 2.564500
		loss: 2.563700
		loss: 2.563000
		loss: 2.562200
		loss: 2.561500
		loss: 2.560800
		loss: 2.560000
		loss: 2.559300
		loss: 2.558600
		loss: 2.557900
		loss: 2.557200
		loss: 2.556500
		loss: 2.555800
		loss: 2.555200
		loss: 2.554500
		loss: 2.553800
		loss: 2.553200
		loss: 2.552500
		loss: 2.551900
		loss: 2.551300
		loss: 2.550600
		loss: 2.550000
		loss: 2.549400
		loss: 2.548800
		loss: 2.548200
		loss: 2.547500
		loss: 2.546900
		loss: 2.546300
		loss: 2.545800
		loss: 2.545200
		loss: 2.544600
		loss: 2.544100
		loss: 2.543500
		loss: 2.542900
		loss: 2.542300
		loss: 2.541800
		loss: 2.541300
		loss: 2.540700
		loss: 2.540200
		loss: 2.539600
		loss: 2.539100
		loss: 2.538600
		loss: 2.538100
		loss: 2.537600
		loss: 2.537100
		loss: 2.536600
		loss: 2.536000
		loss: 2.535500
		loss: 2.535000
		loss: 2.534600
		loss: 2.534100
		loss: 2.533600
		loss: 2.533100
		loss: 2.532600
		loss: 2.532200
		loss: 2.531700
		loss: 2.531200
		loss: 2.530800
		loss: 2.530300
		loss: 2.529900
		loss: 2.529500
		loss: 2.529000
		loss: 2.528600
		loss: 2.528100
		loss: 2.527700
		loss: 2.527300
		loss: 2.526800
		loss: 2.526400
		loss: 2.526000
		loss: 2.525600
		loss: 2.525200
		loss: 2.524800
		loss: 2.524400
		loss: 2.524000
		loss: 2.523600
		loss: 2.523200
		loss: 2.522800
		loss: 2.522400
		loss: 2.522000
		loss: 2.521600
		loss: 2.521300
		loss: 2.520900
		loss: 2.520500
		loss: 2.520100
		loss: 2.519800
		loss: 2.519400
		loss: 2.519100
		loss: 2.518700
		loss: 2.518300
		loss: 2.518000
		loss: 2.517600
		loss: 2.517300
		loss: 2.517000
		loss: 2.516600
		loss: 2.516300
		loss: 2.515900
		loss: 2.515600
		loss: 2.515300
		loss: 2.514900
		loss: 2.514600
		loss: 2.514300
		loss: 2.514000
		loss: 2.513700
		loss: 2.513300
		loss: 2.513000
		loss: 2.512700
		loss: 2.512400
		loss: 2.512100
		loss: 2.511800
		loss: 2.511500
		loss: 2.511200
		loss: 2.510900
		loss: 2.510600
		loss: 2.510300
		loss: 2.510000
		loss: 2.509800
		loss: 2.509500
		loss: 2.509200
		loss: 2.508900
		loss: 2.508600
		loss: 2.508400
		loss: 2.508100
		loss: 2.507800
		loss: 2.507600
		loss: 2.507300
		loss: 2.507000
		loss: 2.506800
		loss: 2.506500
		loss: 2.506200
		loss: 2.506000
		loss: 2.505700
		loss: 2.505500
		loss: 2.505200
		loss: 2.505000
		loss: 2.504700
		loss: 2.504500
		loss: 2.504200
		loss: 2.504000
		loss: 2.503700
		loss: 2.503500
		loss: 2.503300
		loss: 2.503000
		loss: 2.502800
		loss: 2.502600
		loss: 2.502400
	Overall the loss development was 8.466200 -> 2.502400

Epoch 6:
Training data for problem d-01.pddl in epoch 6:
model creation time: 21.971381187438965s
problem epoch data for epoch 6, problem epoch 1
	sampling search time: 4.394946098327637s
	during this search the following actions were chosen:
	training time: 75.06740355491638s
	during the training the following losses were computed:
		loss: 0.060300
		loss: 0.059000
		loss: 0.057900
		loss: 0.056600
		loss: 0.055400
		loss: 0.054300
		loss: 0.053200
		loss: 0.052100
		loss: 0.051100
		loss: 0.050000
		loss: 0.049100
		loss: 0.048100
		loss: 0.047200
		loss: 0.046300
		loss: 0.045500
		loss: 0.044700
		loss: 0.043900
		loss: 0.043100
		loss: 0.042300
		loss: 0.041600
		loss: 0.040900
		loss: 0.040200
		loss: 0.039500
		loss: 0.038800
		loss: 0.038200
		loss: 0.037500
		loss: 0.036900
		loss: 0.036300
		loss: 0.035700
		loss: 0.035100
		loss: 0.034600
		loss: 0.034000
		loss: 0.033500
		loss: 0.033000
		loss: 0.032400
		loss: 0.031900
		loss: 0.031400
		loss: 0.031000
		loss: 0.030500
		loss: 0.030000
		loss: 0.029600
		loss: 0.029100
		loss: 0.028700
		loss: 0.028300
		loss: 0.027800
		loss: 0.027400
		loss: 0.027000
		loss: 0.026600
		loss: 0.026300
		loss: 0.025900
		loss: 0.025500
		loss: 0.025100
		loss: 0.024800
		loss: 0.024400
		loss: 0.024100
		loss: 0.023800
		loss: 0.023400
		loss: 0.023100
		loss: 0.022800
		loss: 0.022500
		loss: 0.022200
		loss: 0.021900
		loss: 0.021600
		loss: 0.021300
		loss: 0.021100
		loss: 0.020800
		loss: 0.020500
		loss: 0.020300
		loss: 0.020000
		loss: 0.019700
		loss: 0.019500
		loss: 0.019300
		loss: 0.019000
		loss: 0.018800
		loss: 0.018600
		loss: 0.018300
		loss: 0.018100
		loss: 0.017900
		loss: 0.017700
		loss: 0.017500
		loss: 0.017300
		loss: 0.017100
		loss: 0.016900
		loss: 0.016700
		loss: 0.016500
		loss: 0.016300
		loss: 0.016200
		loss: 0.016000
		loss: 0.015800
		loss: 0.015600
		loss: 0.015500
		loss: 0.015300
		loss: 0.015200
		loss: 0.015000
		loss: 0.014800
		loss: 0.014700
		loss: 0.014500
		loss: 0.014400
		loss: 0.014200
		loss: 0.014100
		loss: 0.014000
		loss: 0.013800
		loss: 0.013700
		loss: 0.013600
		loss: 0.013400
		loss: 0.013300
		loss: 0.013200
		loss: 0.013100
		loss: 0.012900
		loss: 0.012800
		loss: 0.012700
		loss: 0.012600
		loss: 0.012500
		loss: 0.012400
		loss: 0.012300
		loss: 0.012200
		loss: 0.012000
		loss: 0.011900
		loss: 0.011800
		loss: 0.011700
		loss: 0.011600
		loss: 0.011500
		loss: 0.011500
		loss: 0.011400
		loss: 0.011300
		loss: 0.011200
		loss: 0.011100
		loss: 0.011000
		loss: 0.010900
		loss: 0.010800
		loss: 0.010700
		loss: 0.010700
		loss: 0.010600
		loss: 0.010500
		loss: 0.010400
		loss: 0.010300
		loss: 0.010300
		loss: 0.010200
		loss: 0.010100
		loss: 0.010000
		loss: 0.010000
		loss: 0.009900
		loss: 0.009800
		loss: 0.009800
		loss: 0.009700
		loss: 0.009600
		loss: 0.009600
		loss: 0.009500
		loss: 0.009400
		loss: 0.009400
		loss: 0.009300
		loss: 0.009200
		loss: 0.009200
		loss: 0.009100
		loss: 0.009100
		loss: 0.009000
		loss: 0.009000
		loss: 0.008900
		loss: 0.008800
		loss: 0.008800
		loss: 0.008700
		loss: 0.008700
		loss: 0.008600
		loss: 0.008600
		loss: 0.008500
		loss: 0.008500
		loss: 0.008400
		loss: 0.008400
		loss: 0.008300
		loss: 0.008300
		loss: 0.008200
		loss: 0.008200
		loss: 0.008200
		loss: 0.008100
		loss: 0.008100
		loss: 0.008000
		loss: 0.008000
		loss: 0.007900
		loss: 0.007900
		loss: 0.007900
		loss: 0.007800
		loss: 0.007800
		loss: 0.007700
		loss: 0.007700
		loss: 0.007700
		loss: 0.007600
		loss: 0.007600
		loss: 0.007500
		loss: 0.007500
		loss: 0.007500
		loss: 0.007400
		loss: 0.007400
		loss: 0.007400
		loss: 0.007300
		loss: 0.007300
		loss: 0.007300
		loss: 0.007200
		loss: 0.007200
		loss: 0.007200
		loss: 0.007100
		loss: 0.007100
		loss: 0.007100
		loss: 0.007100
		loss: 0.007000
		loss: 0.007000
		loss: 0.007000
		loss: 0.006900
		loss: 0.006900
		loss: 0.006900
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005600
		loss: 0.005600
		loss: 0.005600
		loss: 0.005600
		loss: 0.005600
		loss: 0.005600
		loss: 0.005600
		loss: 0.005600
		loss: 0.005600
		loss: 0.005500
		loss: 0.005500
		loss: 0.005500
		loss: 0.005500
		loss: 0.005500
		loss: 0.005500
		loss: 0.005500
		loss: 0.005500
		loss: 0.005500
		loss: 0.005400
		loss: 0.005400
		loss: 0.005400
		loss: 0.005400
		loss: 0.005400
		loss: 0.005400
		loss: 0.005400
		loss: 0.005400
		loss: 0.005400
	Overall the loss development was 0.060300 -> 0.005400

Training data for problem d-02.pddl in epoch 6:
model creation time: 30.248298406600952s
problem epoch data for epoch 6, problem epoch 1
	sampling search time: 7.712933540344238s
	during this search the following actions were chosen:
	training time: 101.28250503540039s
	during the training the following losses were computed:
		loss: 1.360300
		loss: 1.261100
		loss: 1.172600
		loss: 1.094600
		loss: 1.027000
		loss: 0.971800
		loss: 0.923700
		loss: 0.884900
		loss: 0.854700
		loss: 0.832400
		loss: 0.816600
		loss: 0.806400
		loss: 0.800500
		loss: 0.797400
		loss: 0.795800
		loss: 0.794600
		loss: 0.793300
		loss: 0.791200
		loss: 0.788000
		loss: 0.783400
		loss: 0.777500
		loss: 0.770300
		loss: 0.761900
		loss: 0.752800
		loss: 0.742800
		loss: 0.732400
		loss: 0.721900
		loss: 0.711200
		loss: 0.700600
		loss: 0.690200
		loss: 0.680100
		loss: 0.670600
		loss: 0.661700
		loss: 0.653600
		loss: 0.646400
		loss: 0.640100
		loss: 0.634600
		loss: 0.629900
		loss: 0.625700
		loss: 0.622200
		loss: 0.619200
		loss: 0.616600
		loss: 0.614300
		loss: 0.612300
		loss: 0.610400
		loss: 0.608300
		loss: 0.606200
		loss: 0.603900
		loss: 0.601300
		loss: 0.598500
		loss: 0.595500
		loss: 0.592200
		loss: 0.588800
		loss: 0.585200
		loss: 0.581600
		loss: 0.577800
		loss: 0.573900
		loss: 0.569700
		loss: 0.565400
		loss: 0.560900
		loss: 0.556200
		loss: 0.551400
		loss: 0.546600
		loss: 0.541900
		loss: 0.537400
		loss: 0.533500
		loss: 0.530000
		loss: 0.527000
		loss: 0.524400
		loss: 0.522300
		loss: 0.520700
		loss: 0.519200
		loss: 0.517800
		loss: 0.516400
		loss: 0.515100
		loss: 0.513900
		loss: 0.512700
		loss: 0.511500
		loss: 0.510400
		loss: 0.509400
		loss: 0.508600
		loss: 0.507900
		loss: 0.507400
		loss: 0.506900
		loss: 0.506400
		loss: 0.506000
		loss: 0.505500
		loss: 0.505100
		loss: 0.504600
		loss: 0.504100
		loss: 0.503700
		loss: 0.503300
		loss: 0.502900
		loss: 0.502500
		loss: 0.502200
		loss: 0.501900
		loss: 0.501600
		loss: 0.501300
		loss: 0.500900
		loss: 0.500600
		loss: 0.500300
		loss: 0.499900
		loss: 0.499700
		loss: 0.499400
		loss: 0.499100
		loss: 0.498900
		loss: 0.498600
		loss: 0.498300
		loss: 0.498000
		loss: 0.497800
		loss: 0.497500
		loss: 0.497300
		loss: 0.497100
		loss: 0.496900
		loss: 0.496600
		loss: 0.496400
		loss: 0.496200
		loss: 0.496000
		loss: 0.495800
		loss: 0.495600
		loss: 0.495400
		loss: 0.495200
		loss: 0.495000
		loss: 0.494800
		loss: 0.494600
		loss: 0.494400
		loss: 0.494300
		loss: 0.494100
		loss: 0.493900
		loss: 0.493700
		loss: 0.493600
		loss: 0.493400
		loss: 0.493300
		loss: 0.493100
		loss: 0.492900
		loss: 0.492800
		loss: 0.492600
		loss: 0.492500
		loss: 0.492300
		loss: 0.492200
		loss: 0.492000
		loss: 0.491900
		loss: 0.491800
		loss: 0.491600
		loss: 0.491500
		loss: 0.491400
		loss: 0.491200
		loss: 0.491100
		loss: 0.491000
		loss: 0.490800
		loss: 0.490700
		loss: 0.490600
		loss: 0.490500
		loss: 0.490300
		loss: 0.490200
		loss: 0.490100
		loss: 0.490000
		loss: 0.489900
		loss: 0.489800
		loss: 0.489600
		loss: 0.489500
		loss: 0.489400
		loss: 0.489300
		loss: 0.489200
		loss: 0.489100
		loss: 0.489000
		loss: 0.488900
		loss: 0.488800
		loss: 0.488700
		loss: 0.488600
		loss: 0.488500
		loss: 0.488400
		loss: 0.488300
		loss: 0.488200
		loss: 0.488100
		loss: 0.488000
		loss: 0.487900
		loss: 0.487800
		loss: 0.487700
		loss: 0.487600
		loss: 0.487600
		loss: 0.487500
		loss: 0.487400
		loss: 0.487300
		loss: 0.487200
		loss: 0.487100
		loss: 0.487000
		loss: 0.487000
		loss: 0.486900
		loss: 0.486800
		loss: 0.486700
		loss: 0.486700
		loss: 0.486600
		loss: 0.486500
		loss: 0.486400
		loss: 0.486300
		loss: 0.486300
		loss: 0.486200
		loss: 0.486100
		loss: 0.486100
		loss: 0.486000
		loss: 0.485900
		loss: 0.485800
		loss: 0.485800
		loss: 0.485700
		loss: 0.485600
		loss: 0.485600
		loss: 0.485500
		loss: 0.485400
		loss: 0.485400
		loss: 0.485300
		loss: 0.485300
		loss: 0.485200
		loss: 0.485100
		loss: 0.485100
		loss: 0.485000
		loss: 0.485000
		loss: 0.484900
		loss: 0.484800
		loss: 0.484800
		loss: 0.484700
		loss: 0.484700
		loss: 0.484600
		loss: 0.484600
		loss: 0.484500
		loss: 0.484500
		loss: 0.484400
		loss: 0.484400
		loss: 0.484300
		loss: 0.484300
		loss: 0.484200
		loss: 0.484100
		loss: 0.484100
		loss: 0.484000
		loss: 0.484000
		loss: 0.484000
		loss: 0.483900
		loss: 0.483900
		loss: 0.483800
		loss: 0.483800
		loss: 0.483700
		loss: 0.483700
		loss: 0.483600
		loss: 0.483600
		loss: 0.483500
		loss: 0.483500
		loss: 0.483500
		loss: 0.483400
		loss: 0.483400
		loss: 0.483300
		loss: 0.483300
		loss: 0.483200
		loss: 0.483200
		loss: 0.483200
		loss: 0.483100
		loss: 0.483100
		loss: 0.483000
		loss: 0.483000
		loss: 0.483000
		loss: 0.482900
		loss: 0.482900
		loss: 0.482800
		loss: 0.482800
		loss: 0.482800
		loss: 0.482700
		loss: 0.482700
		loss: 0.482700
		loss: 0.482600
		loss: 0.482600
		loss: 0.482600
		loss: 0.482500
		loss: 0.482500
		loss: 0.482400
		loss: 0.482400
		loss: 0.482400
		loss: 0.482300
		loss: 0.482300
		loss: 0.482300
		loss: 0.482200
		loss: 0.482200
		loss: 0.482200
		loss: 0.482200
		loss: 0.482100
		loss: 0.482100
		loss: 0.482100
		loss: 0.482000
		loss: 0.482000
		loss: 0.482000
		loss: 0.481900
		loss: 0.481900
		loss: 0.481900
		loss: 0.481900
		loss: 0.481800
		loss: 0.481800
		loss: 0.481800
		loss: 0.481700
		loss: 0.481700
		loss: 0.481700
		loss: 0.481700
		loss: 0.481600
	Overall the loss development was 1.360300 -> 0.481600

Training data for problem d-03.pddl in epoch 6:
model creation time: 25.27046227455139s
problem epoch data for epoch 6, problem epoch 1
	sampling search time: 7.116190671920776s
	during this search the following actions were chosen:
	training time: 86.09319710731506s
	during the training the following losses were computed:
		loss: 2.453000
		loss: 2.278300
		loss: 2.170600
		loss: 2.118800
		loss: 2.107000
		loss: 2.117400
		loss: 2.133600
		loss: 2.144800
		loss: 2.146900
		loss: 2.140100
		loss: 2.126600
		loss: 2.109600
		loss: 2.092300
		loss: 2.077500
		loss: 2.067300
		loss: 2.062700
		loss: 2.063000
		loss: 2.066100
		loss: 2.069300
		loss: 2.070100
		loss: 2.067600
		loss: 2.062400
		loss: 2.055600
		loss: 2.048800
		loss: 2.043100
		loss: 2.039200
		loss: 2.037200
		loss: 2.036500
		loss: 2.036400
		loss: 2.036100
		loss: 2.035100
		loss: 2.033400
		loss: 2.031100
		loss: 2.028700
		loss: 2.026700
		loss: 2.025300
		loss: 2.024400
		loss: 2.023700
		loss: 2.022800
		loss: 2.021300
		loss: 2.019400
		loss: 2.017300
		loss: 2.015000
		loss: 2.013000
		loss: 2.011200
		loss: 2.009600
		loss: 2.008000
		loss: 2.006300
		loss: 2.004300
		loss: 2.002000
		loss: 1.999400
		loss: 1.996600
		loss: 1.993800
		loss: 1.990800
		loss: 1.987700
		loss: 1.984300
		loss: 1.980600
		loss: 1.976600
		loss: 1.972300
		loss: 1.967900
		loss: 1.963300
		loss: 1.958600
		loss: 1.953700
		loss: 1.948600
		loss: 1.943300
		loss: 1.938000
		loss: 1.932600
		loss: 1.927400
		loss: 1.922400
		loss: 1.917500
		loss: 1.912900
		loss: 1.908700
		loss: 1.904900
		loss: 1.901700
		loss: 1.898800
		loss: 1.896300
		loss: 1.894200
		loss: 1.892600
		loss: 1.891400
		loss: 1.890400
		loss: 1.889600
		loss: 1.889000
		loss: 1.888600
		loss: 1.888300
		loss: 1.888000
		loss: 1.887900
		loss: 1.887800
		loss: 1.887700
		loss: 1.887600
		loss: 1.887500
		loss: 1.887500
		loss: 1.887500
		loss: 1.887400
		loss: 1.887300
		loss: 1.887300
		loss: 1.887200
		loss: 1.887200
		loss: 1.887100
		loss: 1.887000
		loss: 1.887000
		loss: 1.886900
		loss: 1.886800
		loss: 1.886700
		loss: 1.886700
		loss: 1.886600
		loss: 1.886500
		loss: 1.886500
		loss: 1.886400
		loss: 1.886300
		loss: 1.886200
		loss: 1.886200
		loss: 1.886100
		loss: 1.886000
		loss: 1.886000
		loss: 1.885900
		loss: 1.885800
		loss: 1.885800
		loss: 1.885700
		loss: 1.885700
		loss: 1.885600
		loss: 1.885500
		loss: 1.885500
		loss: 1.885400
		loss: 1.885400
		loss: 1.885300
		loss: 1.885300
		loss: 1.885200
		loss: 1.885200
		loss: 1.885100
		loss: 1.885100
		loss: 1.885000
		loss: 1.885000
		loss: 1.885000
		loss: 1.884900
		loss: 1.884900
		loss: 1.884800
		loss: 1.884800
		loss: 1.884800
		loss: 1.884700
		loss: 1.884700
		loss: 1.884700
		loss: 1.884600
		loss: 1.884600
		loss: 1.884600
		loss: 1.884500
		loss: 1.884500
		loss: 1.884500
		loss: 1.884400
		loss: 1.884400
		loss: 1.884400
		loss: 1.884300
		loss: 1.884300
		loss: 1.884300
		loss: 1.884200
		loss: 1.884200
		loss: 1.884200
		loss: 1.884200
		loss: 1.884100
		loss: 1.884100
		loss: 1.884100
		loss: 1.884000
		loss: 1.884000
		loss: 1.884000
		loss: 1.884000
		loss: 1.883900
		loss: 1.883900
		loss: 1.883900
		loss: 1.883900
		loss: 1.883800
		loss: 1.883800
		loss: 1.883800
		loss: 1.883800
		loss: 1.883700
		loss: 1.883700
		loss: 1.883700
		loss: 1.883700
		loss: 1.883600
		loss: 1.883600
		loss: 1.883600
		loss: 1.883600
		loss: 1.883500
		loss: 1.883500
		loss: 1.883500
		loss: 1.883500
		loss: 1.883400
		loss: 1.883400
		loss: 1.883400
		loss: 1.883400
		loss: 1.883300
		loss: 1.883300
		loss: 1.883300
		loss: 1.883300
		loss: 1.883300
		loss: 1.883200
		loss: 1.883200
		loss: 1.883200
		loss: 1.883200
		loss: 1.883100
		loss: 1.883100
		loss: 1.883100
		loss: 1.883100
		loss: 1.883100
		loss: 1.883000
		loss: 1.883000
		loss: 1.883000
		loss: 1.883000
		loss: 1.883000
		loss: 1.882900
		loss: 1.882900
		loss: 1.882900
		loss: 1.882900
		loss: 1.882900
		loss: 1.882800
		loss: 1.882800
		loss: 1.882800
		loss: 1.882800
		loss: 1.882800
		loss: 1.882700
		loss: 1.882700
		loss: 1.882700
		loss: 1.882700
		loss: 1.882700
		loss: 1.882600
		loss: 1.882600
		loss: 1.882600
		loss: 1.882600
		loss: 1.882600
		loss: 1.882500
		loss: 1.882500
		loss: 1.882500
		loss: 1.882500
		loss: 1.882500
		loss: 1.882500
		loss: 1.882400
		loss: 1.882400
		loss: 1.882400
		loss: 1.882400
		loss: 1.882400
		loss: 1.882300
		loss: 1.882300
		loss: 1.882300
		loss: 1.882300
		loss: 1.882300
		loss: 1.882300
		loss: 1.882200
		loss: 1.882200
		loss: 1.882200
		loss: 1.882200
		loss: 1.882200
		loss: 1.882200
		loss: 1.882100
		loss: 1.882100
		loss: 1.882100
		loss: 1.882100
		loss: 1.882100
		loss: 1.882100
		loss: 1.882000
		loss: 1.882000
		loss: 1.882000
		loss: 1.882000
		loss: 1.882000
		loss: 1.882000
		loss: 1.881900
		loss: 1.881900
		loss: 1.881900
		loss: 1.881900
		loss: 1.881900
		loss: 1.881900
		loss: 1.881800
		loss: 1.881800
		loss: 1.881800
		loss: 1.881800
		loss: 1.881800
		loss: 1.881800
		loss: 1.881800
		loss: 1.881700
		loss: 1.881700
		loss: 1.881700
		loss: 1.881700
		loss: 1.881700
		loss: 1.881700
		loss: 1.881700
		loss: 1.881600
		loss: 1.881600
		loss: 1.881600
		loss: 1.881600
		loss: 1.881600
		loss: 1.881600
		loss: 1.881600
		loss: 1.881500
		loss: 1.881500
		loss: 1.881500
		loss: 1.881500
		loss: 1.881500
		loss: 1.881500
		loss: 1.881500
		loss: 1.881400
		loss: 1.881400
		loss: 1.881400
		loss: 1.881400
	Overall the loss development was 2.453000 -> 1.881400

Training data for problem d-04.pddl in epoch 6:
model creation time: 62.54282069206238s
problem epoch data for epoch 6, problem epoch 1
	sampling search time: 37.393887996673584s
	during this search the following actions were chosen:
	training time: 411.84503722190857s
	during the training the following losses were computed:
		loss: 6.966500
		loss: 6.265200
		loss: 5.664300
		loss: 5.144900
		loss: 4.695800
		loss: 4.313900
		loss: 3.990600
		loss: 3.721700
		loss: 3.505500
		loss: 3.336500
		loss: 3.208300
		loss: 3.114900
		loss: 3.060200
		loss: 3.030100
		loss: 3.015700
		loss: 3.007800
		loss: 3.005100
		loss: 3.004400
		loss: 3.003800
		loss: 3.002400
		loss: 2.999900
		loss: 2.996100
		loss: 2.990700
		loss: 2.984000
		loss: 2.976200
		loss: 2.967300
		loss: 2.957800
		loss: 2.947700
		loss: 2.937500
		loss: 2.927000
		loss: 2.916600
		loss: 2.905900
		loss: 2.895000
		loss: 2.883700
		loss: 2.872100
		loss: 2.860100
		loss: 2.847600
		loss: 2.834500
		loss: 2.821200
		loss: 2.807900
		loss: 2.795000
		loss: 2.782500
		loss: 2.770700
		loss: 2.759700
		loss: 2.749200
		loss: 2.738800
		loss: 2.728400
		loss: 2.717700
		loss: 2.706900
		loss: 2.696000
		loss: 2.685000
		loss: 2.673900
		loss: 2.663000
		loss: 2.652200
		loss: 2.641700
		loss: 2.631800
		loss: 2.622400
		loss: 2.613700
		loss: 2.605200
		loss: 2.596600
		loss: 2.588100
		loss: 2.580700
		loss: 2.574300
		loss: 2.568500
		loss: 2.563300
		loss: 2.558100
		loss: 2.552500
		loss: 2.546900
		loss: 2.541500
		loss: 2.536400
		loss: 2.531800
		loss: 2.527500
		loss: 2.523300
		loss: 2.519500
		loss: 2.515900
		loss: 2.512500
		loss: 2.509300
		loss: 2.506500
		loss: 2.503800
		loss: 2.501000
		loss: 2.498600
		loss: 2.496300
		loss: 2.494100
		loss: 2.492100
		loss: 2.490100
		loss: 2.488200
		loss: 2.486400
		loss: 2.484600
		loss: 2.482800
		loss: 2.481100
		loss: 2.479500
		loss: 2.477900
		loss: 2.476300
		loss: 2.474800
		loss: 2.473300
		loss: 2.471900
		loss: 2.470500
		loss: 2.469100
		loss: 2.467700
		loss: 2.466400
		loss: 2.465100
		loss: 2.463800
		loss: 2.462500
		loss: 2.461300
		loss: 2.460000
		loss: 2.458800
		loss: 2.457600
		loss: 2.456500
		loss: 2.455300
		loss: 2.454200
		loss: 2.453000
		loss: 2.451900
		loss: 2.450800
		loss: 2.449700
		loss: 2.448600
		loss: 2.447600
		loss: 2.446500
		loss: 2.445500
		loss: 2.444500
		loss: 2.443400
		loss: 2.442400
		loss: 2.441400
		loss: 2.440500
		loss: 2.439500
		loss: 2.438500
		loss: 2.437600
		loss: 2.436600
		loss: 2.435700
		loss: 2.434800
		loss: 2.433900
		loss: 2.433000
		loss: 2.432100
		loss: 2.431200
		loss: 2.430300
		loss: 2.429400
		loss: 2.428600
		loss: 2.427700
		loss: 2.426900
		loss: 2.426100
		loss: 2.425200
		loss: 2.424400
		loss: 2.423600
		loss: 2.422800
		loss: 2.422000
		loss: 2.421200
		loss: 2.420500
		loss: 2.419700
		loss: 2.418900
		loss: 2.418200
		loss: 2.417400
		loss: 2.416700
		loss: 2.415900
		loss: 2.415200
		loss: 2.414500
		loss: 2.413800
		loss: 2.413100
		loss: 2.412400
		loss: 2.411700
		loss: 2.411000
		loss: 2.410300
		loss: 2.409600
		loss: 2.409000
		loss: 2.408300
		loss: 2.407600
		loss: 2.407000
		loss: 2.406400
		loss: 2.405700
		loss: 2.405100
		loss: 2.404400
		loss: 2.403800
		loss: 2.403200
		loss: 2.402600
		loss: 2.402000
		loss: 2.401400
		loss: 2.400800
		loss: 2.400200
		loss: 2.399600
		loss: 2.399000
		loss: 2.398500
		loss: 2.397900
		loss: 2.397300
		loss: 2.396800
		loss: 2.396200
		loss: 2.395700
		loss: 2.395100
		loss: 2.394600
		loss: 2.394100
		loss: 2.393500
		loss: 2.393000
		loss: 2.392500
		loss: 2.392000
		loss: 2.391400
		loss: 2.390900
		loss: 2.390400
		loss: 2.389900
		loss: 2.389400
		loss: 2.388900
		loss: 2.388500
		loss: 2.388000
		loss: 2.387500
		loss: 2.387000
		loss: 2.386500
		loss: 2.386100
		loss: 2.385600
		loss: 2.385200
		loss: 2.384700
		loss: 2.384300
		loss: 2.383800
		loss: 2.383400
		loss: 2.382900
		loss: 2.382500
		loss: 2.382000
		loss: 2.381600
		loss: 2.381200
		loss: 2.380800
		loss: 2.380300
		loss: 2.379900
		loss: 2.379500
		loss: 2.379100
		loss: 2.378700
		loss: 2.378300
		loss: 2.377900
		loss: 2.377500
		loss: 2.377100
		loss: 2.376700
		loss: 2.376400
		loss: 2.376000
		loss: 2.375600
		loss: 2.375200
		loss: 2.374800
		loss: 2.374500
		loss: 2.374100
		loss: 2.373700
		loss: 2.373400
		loss: 2.373000
		loss: 2.372700
		loss: 2.372300
		loss: 2.372000
		loss: 2.371600
		loss: 2.371300
		loss: 2.371000
		loss: 2.370600
		loss: 2.370300
		loss: 2.370000
		loss: 2.369600
		loss: 2.369300
		loss: 2.369000
		loss: 2.368700
		loss: 2.368400
		loss: 2.368000
		loss: 2.367700
		loss: 2.367400
		loss: 2.367100
		loss: 2.366800
		loss: 2.366500
		loss: 2.366200
		loss: 2.365900
		loss: 2.365600
		loss: 2.365300
		loss: 2.365000
		loss: 2.364800
		loss: 2.364500
		loss: 2.364200
		loss: 2.363900
		loss: 2.363600
		loss: 2.363400
		loss: 2.363100
		loss: 2.362800
		loss: 2.362600
		loss: 2.362300
		loss: 2.362000
		loss: 2.361800
		loss: 2.361500
		loss: 2.361200
		loss: 2.361000
		loss: 2.360700
		loss: 2.360500
		loss: 2.360200
		loss: 2.360000
		loss: 2.359800
		loss: 2.359500
		loss: 2.359300
		loss: 2.359000
		loss: 2.358800
		loss: 2.358600
		loss: 2.358300
		loss: 2.358100
		loss: 2.357900
		loss: 2.357700
		loss: 2.357400
		loss: 2.357200
		loss: 2.357000
		loss: 2.356800
		loss: 2.356500
		loss: 2.356300
		loss: 2.356100
		loss: 2.355900
		loss: 2.355700
		loss: 2.355500
		loss: 2.355300
	Overall the loss development was 6.966500 -> 2.355300

Epoch 7:
Training data for problem d-01.pddl in epoch 7:
model creation time: 19.791563272476196s
problem epoch data for epoch 7, problem epoch 1
	sampling search time: 8.255841493606567s
	during this search the following actions were chosen:
	training time: 82.9642686843872s
	during the training the following losses were computed:
		loss: 0.058900
		loss: 0.057600
		loss: 0.056500
		loss: 0.055200
		loss: 0.054100
		loss: 0.052900
		loss: 0.051800
		loss: 0.050700
		loss: 0.049700
		loss: 0.048700
		loss: 0.047800
		loss: 0.046800
		loss: 0.045900
		loss: 0.045100
		loss: 0.044200
		loss: 0.043400
		loss: 0.042600
		loss: 0.041800
		loss: 0.041100
		loss: 0.040400
		loss: 0.039600
		loss: 0.039000
		loss: 0.038300
		loss: 0.037600
		loss: 0.037000
		loss: 0.036400
		loss: 0.035800
		loss: 0.035200
		loss: 0.034600
		loss: 0.034100
		loss: 0.033500
		loss: 0.033000
		loss: 0.032400
		loss: 0.031900
		loss: 0.031400
		loss: 0.031000
		loss: 0.030500
		loss: 0.030000
		loss: 0.029600
		loss: 0.029200
		loss: 0.028700
		loss: 0.028300
		loss: 0.027900
		loss: 0.027500
		loss: 0.027100
		loss: 0.026700
		loss: 0.026400
		loss: 0.026000
		loss: 0.025700
		loss: 0.025300
		loss: 0.025000
		loss: 0.024600
		loss: 0.024300
		loss: 0.024000
		loss: 0.023700
		loss: 0.023400
		loss: 0.023100
		loss: 0.022800
		loss: 0.022500
		loss: 0.022200
		loss: 0.022000
		loss: 0.021700
		loss: 0.021400
		loss: 0.021200
		loss: 0.020900
		loss: 0.020700
		loss: 0.020500
		loss: 0.020200
		loss: 0.020000
		loss: 0.019800
		loss: 0.019500
		loss: 0.019300
		loss: 0.019100
		loss: 0.018900
		loss: 0.018700
		loss: 0.018500
		loss: 0.018300
		loss: 0.018100
		loss: 0.017900
		loss: 0.017700
		loss: 0.017600
		loss: 0.017400
		loss: 0.017200
		loss: 0.017000
		loss: 0.016900
		loss: 0.016700
		loss: 0.016500
		loss: 0.016400
		loss: 0.016200
		loss: 0.016100
		loss: 0.015900
		loss: 0.015800
		loss: 0.015600
		loss: 0.015500
		loss: 0.015300
		loss: 0.015200
		loss: 0.015100
		loss: 0.014900
		loss: 0.014800
		loss: 0.014700
		loss: 0.014600
		loss: 0.014400
		loss: 0.014300
		loss: 0.014200
		loss: 0.014100
		loss: 0.013900
		loss: 0.013800
		loss: 0.013700
		loss: 0.013600
		loss: 0.013500
		loss: 0.013400
		loss: 0.013300
		loss: 0.013200
		loss: 0.013100
		loss: 0.013000
		loss: 0.012900
		loss: 0.012800
		loss: 0.012700
		loss: 0.012600
		loss: 0.012500
		loss: 0.012400
		loss: 0.012300
		loss: 0.012200
		loss: 0.012100
		loss: 0.012000
		loss: 0.011900
		loss: 0.011900
		loss: 0.011800
		loss: 0.011700
		loss: 0.011600
		loss: 0.011500
		loss: 0.011400
		loss: 0.011400
		loss: 0.011300
		loss: 0.011200
		loss: 0.011100
		loss: 0.011100
		loss: 0.011000
		loss: 0.010900
		loss: 0.010900
		loss: 0.010800
		loss: 0.010700
		loss: 0.010600
		loss: 0.010600
		loss: 0.010500
		loss: 0.010400
		loss: 0.010400
		loss: 0.010300
		loss: 0.010200
		loss: 0.010200
		loss: 0.010100
		loss: 0.010100
		loss: 0.010000
		loss: 0.009900
		loss: 0.009900
		loss: 0.009800
		loss: 0.009800
		loss: 0.009700
		loss: 0.009700
		loss: 0.009600
		loss: 0.009500
		loss: 0.009500
		loss: 0.009400
		loss: 0.009400
		loss: 0.009300
		loss: 0.009300
		loss: 0.009200
		loss: 0.009200
		loss: 0.009100
		loss: 0.009100
		loss: 0.009000
		loss: 0.009000
		loss: 0.008900
		loss: 0.008900
		loss: 0.008900
		loss: 0.008800
		loss: 0.008800
		loss: 0.008700
		loss: 0.008700
		loss: 0.008600
		loss: 0.008600
		loss: 0.008500
		loss: 0.008500
		loss: 0.008500
		loss: 0.008400
		loss: 0.008400
		loss: 0.008300
		loss: 0.008300
		loss: 0.008300
		loss: 0.008200
		loss: 0.008200
		loss: 0.008100
		loss: 0.008100
		loss: 0.008100
		loss: 0.008000
		loss: 0.008000
		loss: 0.008000
		loss: 0.007900
		loss: 0.007900
		loss: 0.007900
		loss: 0.007800
		loss: 0.007800
		loss: 0.007800
		loss: 0.007700
		loss: 0.007700
		loss: 0.007700
		loss: 0.007600
		loss: 0.007600
		loss: 0.007600
		loss: 0.007500
		loss: 0.007500
		loss: 0.007500
		loss: 0.007400
		loss: 0.007400
		loss: 0.007400
		loss: 0.007400
		loss: 0.007300
		loss: 0.007300
		loss: 0.007300
		loss: 0.007200
		loss: 0.007200
		loss: 0.007200
		loss: 0.007200
		loss: 0.007100
		loss: 0.007100
		loss: 0.007100
		loss: 0.007100
		loss: 0.007000
		loss: 0.007000
		loss: 0.007000
		loss: 0.007000
		loss: 0.006900
		loss: 0.006900
		loss: 0.006900
		loss: 0.006900
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
	Overall the loss development was 0.058900 -> 0.005800

Training data for problem d-02.pddl in epoch 7:
model creation time: 30.887205839157104s
problem epoch data for epoch 7, problem epoch 1
	sampling search time: 7.294339418411255s
	during this search the following actions were chosen:
	training time: 103.20426201820374s
	during the training the following losses were computed:
		loss: 1.273400
		loss: 1.181200
		loss: 1.099300
		loss: 1.027300
		loss: 0.965100
		loss: 0.912500
		loss: 0.869200
		loss: 0.834900
		loss: 0.808800
		loss: 0.790100
		loss: 0.777800
		loss: 0.770700
		loss: 0.767300
		loss: 0.766000
		loss: 0.765900
		loss: 0.765900
		loss: 0.765400
		loss: 0.764000
		loss: 0.761500
		loss: 0.757700
		loss: 0.752700
		loss: 0.746600
		loss: 0.739600
		loss: 0.731800
		loss: 0.723500
		loss: 0.714900
		loss: 0.706100
		loss: 0.697300
		loss: 0.688700
		loss: 0.680100
		loss: 0.671800
		loss: 0.663500
		loss: 0.655300
		loss: 0.647100
		loss: 0.638900
		loss: 0.630900
		loss: 0.623300
		loss: 0.616300
		loss: 0.610600
		loss: 0.606500
		loss: 0.604100
		loss: 0.602700
		loss: 0.601100
		loss: 0.598900
		loss: 0.596200
		loss: 0.593500
		loss: 0.591300
		loss: 0.589700
		loss: 0.588400
		loss: 0.587200
		loss: 0.585800
		loss: 0.583900
		loss: 0.581600
		loss: 0.579200
		loss: 0.576700
		loss: 0.574500
		loss: 0.572400
		loss: 0.570400
		loss: 0.568100
		loss: 0.565400
		loss: 0.562400
		loss: 0.559300
		loss: 0.556100
		loss: 0.553000
		loss: 0.549700
		loss: 0.546200
		loss: 0.542300
		loss: 0.538300
		loss: 0.534200
		loss: 0.530300
		loss: 0.526500
		loss: 0.522600
		loss: 0.518800
		loss: 0.515300
		loss: 0.512400
		loss: 0.509900
		loss: 0.507700
		loss: 0.506000
		loss: 0.504900
		loss: 0.503900
		loss: 0.502900
		loss: 0.501800
		loss: 0.500800
		loss: 0.499800
		loss: 0.498700
		loss: 0.497600
		loss: 0.496600
		loss: 0.495700
		loss: 0.494900
		loss: 0.494200
		loss: 0.493500
		loss: 0.493000
		loss: 0.492500
		loss: 0.492000
		loss: 0.491600
		loss: 0.491100
		loss: 0.490700
		loss: 0.490200
		loss: 0.489700
		loss: 0.489300
		loss: 0.488800
		loss: 0.488500
		loss: 0.488100
		loss: 0.487800
		loss: 0.487500
		loss: 0.487100
		loss: 0.486800
		loss: 0.486400
		loss: 0.486100
		loss: 0.485700
		loss: 0.485400
		loss: 0.485100
		loss: 0.484800
		loss: 0.484500
		loss: 0.484200
		loss: 0.484000
		loss: 0.483700
		loss: 0.483400
		loss: 0.483100
		loss: 0.482900
		loss: 0.482600
		loss: 0.482400
		loss: 0.482100
		loss: 0.481900
		loss: 0.481700
		loss: 0.481400
		loss: 0.481200
		loss: 0.481000
		loss: 0.480800
		loss: 0.480500
		loss: 0.480300
		loss: 0.480100
		loss: 0.479900
		loss: 0.479700
		loss: 0.479500
		loss: 0.479300
		loss: 0.479200
		loss: 0.479000
		loss: 0.478800
		loss: 0.478600
		loss: 0.478400
		loss: 0.478300
		loss: 0.478100
		loss: 0.477900
		loss: 0.477700
		loss: 0.477600
		loss: 0.477400
		loss: 0.477300
		loss: 0.477100
		loss: 0.476900
		loss: 0.476800
		loss: 0.476600
		loss: 0.476500
		loss: 0.476300
		loss: 0.476200
		loss: 0.476000
		loss: 0.475900
		loss: 0.475800
		loss: 0.475600
		loss: 0.475500
		loss: 0.475400
		loss: 0.475200
		loss: 0.475100
		loss: 0.475000
		loss: 0.474800
		loss: 0.474700
		loss: 0.474600
		loss: 0.474400
		loss: 0.474300
		loss: 0.474200
		loss: 0.474100
		loss: 0.474000
		loss: 0.473800
		loss: 0.473700
		loss: 0.473600
		loss: 0.473500
		loss: 0.473400
		loss: 0.473300
		loss: 0.473200
		loss: 0.473100
		loss: 0.472900
		loss: 0.472800
		loss: 0.472700
		loss: 0.472600
		loss: 0.472500
		loss: 0.472400
		loss: 0.472300
		loss: 0.472200
		loss: 0.472100
		loss: 0.472000
		loss: 0.471900
		loss: 0.471800
		loss: 0.471700
		loss: 0.471700
		loss: 0.471600
		loss: 0.471500
		loss: 0.471400
		loss: 0.471300
		loss: 0.471200
		loss: 0.471100
		loss: 0.471000
		loss: 0.470900
		loss: 0.470800
		loss: 0.470800
		loss: 0.470700
		loss: 0.470600
		loss: 0.470500
		loss: 0.470400
		loss: 0.470400
		loss: 0.470300
		loss: 0.470200
		loss: 0.470100
		loss: 0.470000
		loss: 0.470000
		loss: 0.469900
		loss: 0.469800
		loss: 0.469700
		loss: 0.469700
		loss: 0.469600
		loss: 0.469500
		loss: 0.469400
		loss: 0.469400
		loss: 0.469300
		loss: 0.469200
		loss: 0.469200
		loss: 0.469100
		loss: 0.469000
		loss: 0.469000
		loss: 0.468900
		loss: 0.468800
		loss: 0.468800
		loss: 0.468700
		loss: 0.468600
		loss: 0.468600
		loss: 0.468500
		loss: 0.468400
		loss: 0.468400
		loss: 0.468300
		loss: 0.468200
		loss: 0.468200
		loss: 0.468100
		loss: 0.468100
		loss: 0.468000
		loss: 0.467900
		loss: 0.467900
		loss: 0.467800
		loss: 0.467800
		loss: 0.467700
		loss: 0.467700
		loss: 0.467600
		loss: 0.467500
		loss: 0.467500
		loss: 0.467400
		loss: 0.467400
		loss: 0.467300
		loss: 0.467300
		loss: 0.467200
		loss: 0.467200
		loss: 0.467100
		loss: 0.467100
		loss: 0.467000
		loss: 0.467000
		loss: 0.466900
		loss: 0.466900
		loss: 0.466800
		loss: 0.466800
		loss: 0.466700
		loss: 0.466700
		loss: 0.466600
		loss: 0.466600
		loss: 0.466500
		loss: 0.466500
		loss: 0.466400
		loss: 0.466400
		loss: 0.466300
		loss: 0.466300
		loss: 0.466200
		loss: 0.466200
		loss: 0.466200
		loss: 0.466100
		loss: 0.466100
		loss: 0.466000
		loss: 0.466000
		loss: 0.465900
		loss: 0.465900
		loss: 0.465900
		loss: 0.465800
		loss: 0.465800
		loss: 0.465700
		loss: 0.465700
		loss: 0.465600
		loss: 0.465600
		loss: 0.465600
		loss: 0.465500
		loss: 0.465500
		loss: 0.465400
		loss: 0.465400
		loss: 0.465400
		loss: 0.465300
		loss: 0.465300
	Overall the loss development was 1.273400 -> 0.465300

Training data for problem d-03.pddl in epoch 7:
model creation time: 26.20295023918152s
problem epoch data for epoch 7, problem epoch 1
	sampling search time: 6.580456733703613s
	during this search the following actions were chosen:
	training time: 86.26618027687073s
	during the training the following losses were computed:
		loss: 2.406500
		loss: 2.310700
		loss: 2.226300
		loss: 2.153200
		loss: 2.090900
		loss: 2.039000
		loss: 1.997700
		loss: 1.963500
		loss: 1.936400
		loss: 1.915900
		loss: 1.900800
		loss: 1.890100
		loss: 1.882900
		loss: 1.878600
		loss: 1.876400
		loss: 1.875600
		loss: 1.875700
		loss: 1.875900
		loss: 1.875800
		loss: 1.875000
		loss: 1.873200
		loss: 1.870300
		loss: 1.866600
		loss: 1.862200
		loss: 1.857400
		loss: 1.852400
		loss: 1.847600
		loss: 1.842900
		loss: 1.838800
		loss: 1.835200
		loss: 1.832200
		loss: 1.829600
		loss: 1.827300
		loss: 1.825400
		loss: 1.823500
		loss: 1.821700
		loss: 1.819900
		loss: 1.818000
		loss: 1.816000
		loss: 1.813800
		loss: 1.811400
		loss: 1.808800
		loss: 1.806100
		loss: 1.803200
		loss: 1.800300
		loss: 1.797300
		loss: 1.794300
		loss: 1.791300
		loss: 1.788300
		loss: 1.785200
		loss: 1.782100
		loss: 1.778900
		loss: 1.775500
		loss: 1.771900
		loss: 1.768200
		loss: 1.764100
		loss: 1.759800
		loss: 1.755200
		loss: 1.750400
		loss: 1.745300
		loss: 1.740100
		loss: 1.734900
		loss: 1.729700
		loss: 1.724500
		loss: 1.719300
		loss: 1.714400
		loss: 1.709600
		loss: 1.705100
		loss: 1.700900
		loss: 1.697000
		loss: 1.693600
		loss: 1.690600
		loss: 1.688000
		loss: 1.685700
		loss: 1.683800
		loss: 1.682300
		loss: 1.681100
		loss: 1.680200
		loss: 1.679400
		loss: 1.678700
		loss: 1.678200
		loss: 1.677900
		loss: 1.677600
		loss: 1.677400
		loss: 1.677200
		loss: 1.677000
		loss: 1.676900
		loss: 1.676700
		loss: 1.676600
		loss: 1.676500
		loss: 1.676400
		loss: 1.676300
		loss: 1.676200
		loss: 1.676100
		loss: 1.676000
		loss: 1.675900
		loss: 1.675800
		loss: 1.675700
		loss: 1.675600
		loss: 1.675500
		loss: 1.675400
		loss: 1.675300
		loss: 1.675200
		loss: 1.675100
		loss: 1.675000
		loss: 1.674900
		loss: 1.674800
		loss: 1.674700
		loss: 1.674600
		loss: 1.674500
		loss: 1.674500
		loss: 1.674400
		loss: 1.674300
		loss: 1.674200
		loss: 1.674200
		loss: 1.674100
		loss: 1.674000
		loss: 1.673900
		loss: 1.673900
		loss: 1.673800
		loss: 1.673700
		loss: 1.673700
		loss: 1.673600
		loss: 1.673600
		loss: 1.673500
		loss: 1.673400
		loss: 1.673400
		loss: 1.673300
		loss: 1.673300
		loss: 1.673200
		loss: 1.673200
		loss: 1.673100
		loss: 1.673100
		loss: 1.673000
		loss: 1.673000
		loss: 1.672900
		loss: 1.672900
		loss: 1.672900
		loss: 1.672800
		loss: 1.672800
		loss: 1.672700
		loss: 1.672700
		loss: 1.672600
		loss: 1.672600
		loss: 1.672600
		loss: 1.672500
		loss: 1.672500
		loss: 1.672400
		loss: 1.672400
		loss: 1.672400
		loss: 1.672300
		loss: 1.672300
		loss: 1.672300
		loss: 1.672200
		loss: 1.672200
		loss: 1.672100
		loss: 1.672100
		loss: 1.672100
		loss: 1.672000
		loss: 1.672000
		loss: 1.672000
		loss: 1.671900
		loss: 1.671900
		loss: 1.671900
		loss: 1.671800
		loss: 1.671800
		loss: 1.671700
		loss: 1.671700
		loss: 1.671700
		loss: 1.671600
		loss: 1.671600
		loss: 1.671600
		loss: 1.671500
		loss: 1.671500
		loss: 1.671500
		loss: 1.671400
		loss: 1.671400
		loss: 1.671400
		loss: 1.671400
		loss: 1.671300
		loss: 1.671300
		loss: 1.671300
		loss: 1.671200
		loss: 1.671200
		loss: 1.671200
		loss: 1.671100
		loss: 1.671100
		loss: 1.671100
		loss: 1.671000
		loss: 1.671000
		loss: 1.671000
		loss: 1.671000
		loss: 1.670900
		loss: 1.670900
		loss: 1.670900
		loss: 1.670800
		loss: 1.670800
		loss: 1.670800
		loss: 1.670800
		loss: 1.670700
		loss: 1.670700
		loss: 1.670700
		loss: 1.670700
		loss: 1.670600
		loss: 1.670600
		loss: 1.670600
		loss: 1.670600
		loss: 1.670500
		loss: 1.670500
		loss: 1.670500
		loss: 1.670500
		loss: 1.670400
		loss: 1.670400
		loss: 1.670400
		loss: 1.670400
		loss: 1.670300
		loss: 1.670300
		loss: 1.670300
		loss: 1.670300
		loss: 1.670200
		loss: 1.670200
		loss: 1.670200
		loss: 1.670200
		loss: 1.670200
		loss: 1.670100
		loss: 1.670100
		loss: 1.670100
		loss: 1.670100
		loss: 1.670000
		loss: 1.670000
		loss: 1.670000
		loss: 1.670000
		loss: 1.670000
		loss: 1.669900
		loss: 1.669900
		loss: 1.669900
		loss: 1.669900
		loss: 1.669900
		loss: 1.669800
		loss: 1.669800
		loss: 1.669800
		loss: 1.669800
		loss: 1.669800
		loss: 1.669700
		loss: 1.669700
		loss: 1.669700
		loss: 1.669700
		loss: 1.669700
		loss: 1.669700
		loss: 1.669600
		loss: 1.669600
		loss: 1.669600
		loss: 1.669600
		loss: 1.669600
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669400
		loss: 1.669400
		loss: 1.669400
		loss: 1.669400
		loss: 1.669400
		loss: 1.669400
		loss: 1.669300
		loss: 1.669300
		loss: 1.669300
		loss: 1.669300
		loss: 1.669300
		loss: 1.669300
		loss: 1.669200
		loss: 1.669200
		loss: 1.669200
		loss: 1.669200
		loss: 1.669200
		loss: 1.669200
		loss: 1.669100
		loss: 1.669100
		loss: 1.669100
		loss: 1.669100
		loss: 1.669100
		loss: 1.669100
		loss: 1.669100
		loss: 1.669000
		loss: 1.669000
		loss: 1.669000
		loss: 1.669000
		loss: 1.669000
		loss: 1.669000
		loss: 1.668900
		loss: 1.668900
		loss: 1.668900
		loss: 1.668900
		loss: 1.668900
		loss: 1.668900
		loss: 1.668900
		loss: 1.668800
		loss: 1.668800
	Overall the loss development was 2.406500 -> 1.668800

Training data for problem d-04.pddl in epoch 7:
model creation time: 62.412477254867554s
problem epoch data for epoch 7, problem epoch 1
	sampling search time: 34.60304498672485s
	during this search the following actions were chosen:
	training time: 284.069030046463s
	during the training the following losses were computed:
		loss: 6.893900
		loss: 6.334400
		loss: 5.914200
		loss: 5.481500
		loss: 5.122000
		loss: 4.794500
		loss: 4.498600
		loss: 4.229100
		loss: 3.997300
		loss: 3.795200
		loss: 3.625400
		loss: 3.483900
		loss: 3.370000
		loss: 3.279900
		loss: 3.209100
		loss: 3.157100
		loss: 3.120500
		loss: 3.097700
		loss: 3.082700
		loss: 3.076300
		loss: 3.072600
		loss: 3.069300
		loss: 3.064700
		loss: 3.059100
		loss: 3.052600
		loss: 3.044900
		loss: 3.036200
		loss: 3.026800
		loss: 3.016700
		loss: 3.006000
		loss: 2.994800
		loss: 2.983200
		loss: 2.971400
		loss: 2.959300
		loss: 2.947000
		loss: 2.934500
		loss: 2.921900
		loss: 2.909200
		loss: 2.896400
		loss: 2.883700
		loss: 2.871100
		loss: 2.858700
		loss: 2.846600
		loss: 2.835700
		loss: 2.825700
		loss: 2.816200
		loss: 2.807300
		loss: 2.798300
		loss: 2.789200
		loss: 2.779700
		loss: 2.769900
		loss: 2.760000
		loss: 2.750300
		loss: 2.741000
		loss: 2.732200
		loss: 2.723800
		loss: 2.715700
		loss: 2.707300
		loss: 2.698900
		loss: 2.690600
		loss: 2.682500
		loss: 2.675000
		loss: 2.668000
		loss: 2.661400
		loss: 2.654800
		loss: 2.648500
		loss: 2.642600
		loss: 2.637000
		loss: 2.631400
		loss: 2.626000
		loss: 2.620800
		loss: 2.616000
		loss: 2.611500
		loss: 2.607100
		loss: 2.602900
		loss: 2.599000
		loss: 2.595500
		loss: 2.592100
		loss: 2.588800
		loss: 2.585700
		loss: 2.582800
		loss: 2.580100
		loss: 2.577500
		loss: 2.575000
		loss: 2.572700
		loss: 2.570500
		loss: 2.568400
		loss: 2.566400
		loss: 2.564500
		loss: 2.562600
		loss: 2.560900
		loss: 2.559100
		loss: 2.557500
		loss: 2.555900
		loss: 2.554300
		loss: 2.552800
		loss: 2.551300
		loss: 2.549900
		loss: 2.548500
		loss: 2.547100
		loss: 2.545800
		loss: 2.544500
		loss: 2.543200
		loss: 2.541900
		loss: 2.540700
		loss: 2.539400
		loss: 2.538200
		loss: 2.537000
		loss: 2.535800
		loss: 2.534700
		loss: 2.533500
		loss: 2.532400
		loss: 2.531300
		loss: 2.530200
		loss: 2.529100
		loss: 2.528000
		loss: 2.526900
		loss: 2.525900
		loss: 2.524800
		loss: 2.523800
		loss: 2.522700
		loss: 2.521700
		loss: 2.520700
		loss: 2.519700
		loss: 2.518700
		loss: 2.517800
		loss: 2.516800
		loss: 2.515800
		loss: 2.514900
		loss: 2.513900
		loss: 2.513000
		loss: 2.512100
		loss: 2.511200
		loss: 2.510300
		loss: 2.509400
		loss: 2.508500
		loss: 2.507600
		loss: 2.506700
		loss: 2.505900
		loss: 2.505000
		loss: 2.504200
		loss: 2.503300
		loss: 2.502500
		loss: 2.501700
		loss: 2.500900
		loss: 2.500100
		loss: 2.499300
		loss: 2.498500
		loss: 2.497700
		loss: 2.496900
		loss: 2.496100
		loss: 2.495400
		loss: 2.494600
		loss: 2.493800
		loss: 2.493100
		loss: 2.492400
		loss: 2.491600
		loss: 2.490900
		loss: 2.490200
		loss: 2.489500
		loss: 2.488800
		loss: 2.488100
		loss: 2.487400
		loss: 2.486700
		loss: 2.486000
		loss: 2.485300
		loss: 2.484700
		loss: 2.484000
		loss: 2.483400
		loss: 2.482700
		loss: 2.482100
		loss: 2.481400
		loss: 2.480700
		loss: 2.480100
		loss: 2.479500
		loss: 2.478900
		loss: 2.478300
		loss: 2.477600
		loss: 2.477000
		loss: 2.476500
		loss: 2.475900
		loss: 2.475300
		loss: 2.474700
		loss: 2.474100
		loss: 2.473500
		loss: 2.472900
		loss: 2.472400
		loss: 2.471800
		loss: 2.471200
		loss: 2.470700
		loss: 2.470100
		loss: 2.469600
		loss: 2.469100
		loss: 2.468500
		loss: 2.468000
		loss: 2.467500
		loss: 2.466900
		loss: 2.466400
		loss: 2.465900
		loss: 2.465400
		loss: 2.464900
		loss: 2.464400
		loss: 2.463900
		loss: 2.463400
		loss: 2.462900
		loss: 2.462400
		loss: 2.461900
		loss: 2.461400
		loss: 2.461000
		loss: 2.460500
		loss: 2.460000
		loss: 2.459500
		loss: 2.459100
		loss: 2.458600
		loss: 2.458200
		loss: 2.457700
		loss: 2.457300
		loss: 2.456800
		loss: 2.456400
		loss: 2.455900
		loss: 2.455500
		loss: 2.455100
		loss: 2.454700
		loss: 2.454200
		loss: 2.453800
		loss: 2.453400
		loss: 2.453000
		loss: 2.452600
		loss: 2.452200
		loss: 2.451800
		loss: 2.451400
		loss: 2.451000
		loss: 2.450600
		loss: 2.450200
		loss: 2.449800
		loss: 2.449400
		loss: 2.449000
		loss: 2.448700
		loss: 2.448300
		loss: 2.447900
		loss: 2.447500
		loss: 2.447200
		loss: 2.446800
		loss: 2.446400
		loss: 2.446100
		loss: 2.445700
		loss: 2.445400
		loss: 2.445000
		loss: 2.444700
		loss: 2.444300
		loss: 2.444000
		loss: 2.443700
		loss: 2.443300
		loss: 2.443000
		loss: 2.442700
		loss: 2.442300
		loss: 2.442000
		loss: 2.441700
		loss: 2.441400
		loss: 2.441000
		loss: 2.440700
		loss: 2.440400
		loss: 2.440100
		loss: 2.439800
		loss: 2.439500
		loss: 2.439200
		loss: 2.438900
		loss: 2.438600
		loss: 2.438300
		loss: 2.438000
		loss: 2.437700
		loss: 2.437400
		loss: 2.437100
		loss: 2.436900
		loss: 2.436600
		loss: 2.436300
		loss: 2.436000
		loss: 2.435700
		loss: 2.435500
		loss: 2.435200
		loss: 2.434900
		loss: 2.434700
		loss: 2.434400
		loss: 2.434100
		loss: 2.433900
		loss: 2.433600
		loss: 2.433300
		loss: 2.433100
		loss: 2.432800
		loss: 2.432600
		loss: 2.432300
		loss: 2.432100
		loss: 2.431900
		loss: 2.431600
		loss: 2.431400
		loss: 2.431100
		loss: 2.430900
		loss: 2.430600
		loss: 2.430400
		loss: 2.430200
	Overall the loss development was 6.893900 -> 2.430200

Epoch 8:
Training data for problem d-01.pddl in epoch 8:
model creation time: 18.679933547973633s
problem epoch data for epoch 8, problem epoch 1
	sampling search time: 4.04928183555603s
	during this search the following actions were chosen:
	training time: 74.63641381263733s
	during the training the following losses were computed:
		loss: 0.060400
		loss: 0.058800
		loss: 0.057400
		loss: 0.056100
		loss: 0.054900
		loss: 0.053800
		loss: 0.052700
		loss: 0.051600
		loss: 0.050500
		loss: 0.049500
		loss: 0.048500
		loss: 0.047600
		loss: 0.046600
		loss: 0.045700
		loss: 0.044900
		loss: 0.044100
		loss: 0.043300
		loss: 0.042500
		loss: 0.041700
		loss: 0.041000
		loss: 0.040300
		loss: 0.039600
		loss: 0.038900
		loss: 0.038200
		loss: 0.037600
		loss: 0.036900
		loss: 0.036300
		loss: 0.035700
		loss: 0.035200
		loss: 0.034600
		loss: 0.034000
		loss: 0.033500
		loss: 0.033000
		loss: 0.032500
		loss: 0.032000
		loss: 0.031500
		loss: 0.031000
		loss: 0.030500
		loss: 0.030100
		loss: 0.029600
		loss: 0.029200
		loss: 0.028800
		loss: 0.028400
		loss: 0.028000
		loss: 0.027600
		loss: 0.027200
		loss: 0.026800
		loss: 0.026400
		loss: 0.026100
		loss: 0.025700
		loss: 0.025400
		loss: 0.025000
		loss: 0.024700
		loss: 0.024400
		loss: 0.024100
		loss: 0.023800
		loss: 0.023500
		loss: 0.023200
		loss: 0.022900
		loss: 0.022600
		loss: 0.022300
		loss: 0.022000
		loss: 0.021800
		loss: 0.021500
		loss: 0.021200
		loss: 0.021000
		loss: 0.020700
		loss: 0.020500
		loss: 0.020300
		loss: 0.020000
		loss: 0.019800
		loss: 0.019600
		loss: 0.019400
		loss: 0.019200
		loss: 0.018900
		loss: 0.018700
		loss: 0.018500
		loss: 0.018300
		loss: 0.018100
		loss: 0.017900
		loss: 0.017800
		loss: 0.017600
		loss: 0.017400
		loss: 0.017200
		loss: 0.017000
		loss: 0.016900
		loss: 0.016700
		loss: 0.016500
		loss: 0.016400
		loss: 0.016200
		loss: 0.016100
		loss: 0.015900
		loss: 0.015700
		loss: 0.015600
		loss: 0.015500
		loss: 0.015300
		loss: 0.015200
		loss: 0.015000
		loss: 0.014900
		loss: 0.014800
		loss: 0.014600
		loss: 0.014500
		loss: 0.014400
		loss: 0.014200
		loss: 0.014100
		loss: 0.014000
		loss: 0.013900
		loss: 0.013800
		loss: 0.013600
		loss: 0.013500
		loss: 0.013400
		loss: 0.013300
		loss: 0.013200
		loss: 0.013100
		loss: 0.013000
		loss: 0.012900
		loss: 0.012800
		loss: 0.012700
		loss: 0.012600
		loss: 0.012500
		loss: 0.012400
		loss: 0.012300
		loss: 0.012200
		loss: 0.012100
		loss: 0.012000
		loss: 0.011900
		loss: 0.011800
		loss: 0.011800
		loss: 0.011700
		loss: 0.011600
		loss: 0.011500
		loss: 0.011400
		loss: 0.011300
		loss: 0.011300
		loss: 0.011200
		loss: 0.011100
		loss: 0.011000
		loss: 0.010900
		loss: 0.010900
		loss: 0.010800
		loss: 0.010700
		loss: 0.010700
		loss: 0.010600
		loss: 0.010500
		loss: 0.010400
		loss: 0.010400
		loss: 0.010300
		loss: 0.010200
		loss: 0.010200
		loss: 0.010100
		loss: 0.010100
		loss: 0.010000
		loss: 0.009900
		loss: 0.009900
		loss: 0.009800
		loss: 0.009800
		loss: 0.009700
		loss: 0.009600
		loss: 0.009600
		loss: 0.009500
		loss: 0.009500
		loss: 0.009400
		loss: 0.009400
		loss: 0.009300
		loss: 0.009300
		loss: 0.009200
		loss: 0.009200
		loss: 0.009100
		loss: 0.009000
		loss: 0.009000
		loss: 0.009000
		loss: 0.008900
		loss: 0.008900
		loss: 0.008800
		loss: 0.008800
		loss: 0.008700
		loss: 0.008700
		loss: 0.008600
		loss: 0.008600
		loss: 0.008500
		loss: 0.008500
		loss: 0.008500
		loss: 0.008400
		loss: 0.008400
		loss: 0.008300
		loss: 0.008300
		loss: 0.008200
		loss: 0.008200
		loss: 0.008200
		loss: 0.008100
		loss: 0.008100
		loss: 0.008100
		loss: 0.008000
		loss: 0.008000
		loss: 0.007900
		loss: 0.007900
		loss: 0.007900
		loss: 0.007800
		loss: 0.007800
		loss: 0.007800
		loss: 0.007700
		loss: 0.007700
		loss: 0.007700
		loss: 0.007600
		loss: 0.007600
		loss: 0.007600
		loss: 0.007500
		loss: 0.007500
		loss: 0.007500
		loss: 0.007400
		loss: 0.007400
		loss: 0.007400
		loss: 0.007300
		loss: 0.007300
		loss: 0.007300
		loss: 0.007300
		loss: 0.007200
		loss: 0.007200
		loss: 0.007200
		loss: 0.007100
		loss: 0.007100
		loss: 0.007100
		loss: 0.007100
		loss: 0.007000
		loss: 0.007000
		loss: 0.007000
		loss: 0.007000
		loss: 0.006900
		loss: 0.006900
		loss: 0.006900
		loss: 0.006900
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006800
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006700
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006600
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006500
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006400
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006300
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006200
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006100
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.006000
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005900
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005800
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
		loss: 0.005700
	Overall the loss development was 0.060400 -> 0.005700

Training data for problem d-02.pddl in epoch 8:
model creation time: 30.34970784187317s
problem epoch data for epoch 8, problem epoch 1
	sampling search time: 7.174267292022705s
	during this search the following actions were chosen:
	training time: 100.02804446220398s
	during the training the following losses were computed:
		loss: 1.058900
		loss: 1.004400
		loss: 0.937900
		loss: 0.878900
		loss: 0.827100
		loss: 0.782400
		loss: 0.744400
		loss: 0.712800
		loss: 0.687000
		loss: 0.666600
		loss: 0.651200
		loss: 0.639800
		loss: 0.632100
		loss: 0.627200
		loss: 0.624400
		loss: 0.622900
		loss: 0.622100
		loss: 0.621400
		loss: 0.620300
		loss: 0.618600
		loss: 0.616000
		loss: 0.612600
		loss: 0.608300
		loss: 0.603200
		loss: 0.597500
		loss: 0.591200
		loss: 0.584400
		loss: 0.577400
		loss: 0.570100
		loss: 0.562700
		loss: 0.555400
		loss: 0.548100
		loss: 0.540900
		loss: 0.533800
		loss: 0.527100
		loss: 0.520600
		loss: 0.514600
		loss: 0.509100
		loss: 0.504100
		loss: 0.499600
		loss: 0.495600
		loss: 0.492100
		loss: 0.489100
		loss: 0.486500
		loss: 0.484200
		loss: 0.481900
		loss: 0.479800
		loss: 0.477600
		loss: 0.475500
		loss: 0.473400
		loss: 0.471200
		loss: 0.468900
		loss: 0.466500
		loss: 0.463800
		loss: 0.461000
		loss: 0.458000
		loss: 0.454900
		loss: 0.451600
		loss: 0.448200
		loss: 0.444700
		loss: 0.441000
		loss: 0.437300
		loss: 0.433600
		loss: 0.429900
		loss: 0.426300
		loss: 0.423200
		loss: 0.420600
		loss: 0.418500
		loss: 0.416700
		loss: 0.415100
		loss: 0.414000
		loss: 0.413100
		loss: 0.412200
		loss: 0.411400
		loss: 0.410500
		loss: 0.409500
		loss: 0.408500
		loss: 0.407500
		loss: 0.406600
		loss: 0.405700
		loss: 0.404900
		loss: 0.404200
		loss: 0.403700
		loss: 0.403200
		loss: 0.402800
		loss: 0.402400
		loss: 0.401900
		loss: 0.401400
		loss: 0.400900
		loss: 0.400400
		loss: 0.399900
		loss: 0.399500
		loss: 0.399100
		loss: 0.398700
		loss: 0.398400
		loss: 0.398100
		loss: 0.397800
		loss: 0.397400
		loss: 0.397100
		loss: 0.396800
		loss: 0.396500
		loss: 0.396100
		loss: 0.395800
		loss: 0.395500
		loss: 0.395200
		loss: 0.395000
		loss: 0.394700
		loss: 0.394400
		loss: 0.394100
		loss: 0.393900
		loss: 0.393600
		loss: 0.393400
		loss: 0.393100
		loss: 0.392900
		loss: 0.392700
		loss: 0.392400
		loss: 0.392200
		loss: 0.392000
		loss: 0.391800
		loss: 0.391600
		loss: 0.391400
		loss: 0.391200
		loss: 0.391000
		loss: 0.390800
		loss: 0.390600
		loss: 0.390400
		loss: 0.390200
		loss: 0.390000
		loss: 0.389900
		loss: 0.389700
		loss: 0.389500
		loss: 0.389300
		loss: 0.389200
		loss: 0.389000
		loss: 0.388800
		loss: 0.388700
		loss: 0.388500
		loss: 0.388300
		loss: 0.388200
		loss: 0.388000
		loss: 0.387900
		loss: 0.387700
		loss: 0.387600
		loss: 0.387400
		loss: 0.387300
		loss: 0.387200
		loss: 0.387000
		loss: 0.386900
		loss: 0.386700
		loss: 0.386600
		loss: 0.386500
		loss: 0.386400
		loss: 0.386200
		loss: 0.386100
		loss: 0.386000
		loss: 0.385800
		loss: 0.385700
		loss: 0.385600
		loss: 0.385500
		loss: 0.385400
		loss: 0.385200
		loss: 0.385100
		loss: 0.385000
		loss: 0.384900
		loss: 0.384800
		loss: 0.384700
		loss: 0.384600
		loss: 0.384500
		loss: 0.384400
		loss: 0.384300
		loss: 0.384100
		loss: 0.384000
		loss: 0.383900
		loss: 0.383800
		loss: 0.383700
		loss: 0.383600
		loss: 0.383500
		loss: 0.383400
		loss: 0.383400
		loss: 0.383300
		loss: 0.383200
		loss: 0.383100
		loss: 0.383000
		loss: 0.382900
		loss: 0.382800
		loss: 0.382700
		loss: 0.382600
		loss: 0.382500
		loss: 0.382500
		loss: 0.382400
		loss: 0.382300
		loss: 0.382200
		loss: 0.382100
		loss: 0.382000
		loss: 0.382000
		loss: 0.381900
		loss: 0.381800
		loss: 0.381700
		loss: 0.381700
		loss: 0.381600
		loss: 0.381500
		loss: 0.381400
		loss: 0.381400
		loss: 0.381300
		loss: 0.381200
		loss: 0.381100
		loss: 0.381100
		loss: 0.381000
		loss: 0.380900
		loss: 0.380900
		loss: 0.380800
		loss: 0.380700
		loss: 0.380700
		loss: 0.380600
		loss: 0.380500
		loss: 0.380500
		loss: 0.380400
		loss: 0.380300
		loss: 0.380300
		loss: 0.380200
		loss: 0.380200
		loss: 0.380100
		loss: 0.380000
		loss: 0.380000
		loss: 0.379900
		loss: 0.379900
		loss: 0.379800
		loss: 0.379800
		loss: 0.379700
		loss: 0.379600
		loss: 0.379600
		loss: 0.379500
		loss: 0.379500
		loss: 0.379400
		loss: 0.379400
		loss: 0.379300
		loss: 0.379300
		loss: 0.379200
		loss: 0.379200
		loss: 0.379100
		loss: 0.379100
		loss: 0.379000
		loss: 0.379000
		loss: 0.378900
		loss: 0.378900
		loss: 0.378800
		loss: 0.378800
		loss: 0.378700
		loss: 0.378700
		loss: 0.378700
		loss: 0.378600
		loss: 0.378600
		loss: 0.378500
		loss: 0.378500
		loss: 0.378400
		loss: 0.378400
		loss: 0.378400
		loss: 0.378300
		loss: 0.378300
		loss: 0.378200
		loss: 0.378200
		loss: 0.378200
		loss: 0.378100
		loss: 0.378100
		loss: 0.378000
		loss: 0.378000
		loss: 0.378000
		loss: 0.377900
		loss: 0.377900
		loss: 0.377900
		loss: 0.377800
		loss: 0.377800
		loss: 0.377700
		loss: 0.377700
		loss: 0.377700
		loss: 0.377600
		loss: 0.377600
		loss: 0.377600
		loss: 0.377500
		loss: 0.377500
		loss: 0.377500
		loss: 0.377400
		loss: 0.377400
		loss: 0.377400
		loss: 0.377300
		loss: 0.377300
		loss: 0.377300
		loss: 0.377300
		loss: 0.377200
		loss: 0.377200
		loss: 0.377200
		loss: 0.377100
		loss: 0.377100
		loss: 0.377100
		loss: 0.377100
		loss: 0.377000
		loss: 0.377000
		loss: 0.377000
		loss: 0.376900
		loss: 0.376900
	Overall the loss development was 1.058900 -> 0.376900

Training data for problem d-03.pddl in epoch 8:
model creation time: 26.017040014266968s
problem epoch data for epoch 8, problem epoch 1
	sampling search time: 6.428325891494751s
	during this search the following actions were chosen:
	training time: 86.37133026123047s
	during the training the following losses were computed:
		loss: 2.080200
		loss: 2.019600
		loss: 1.956700
		loss: 1.913000
		loss: 1.886200
		loss: 1.869700
		loss: 1.860900
		loss: 1.857600
		loss: 1.857500
		loss: 1.858700
		loss: 1.859700
		loss: 1.859600
		loss: 1.858100
		loss: 1.855200
		loss: 1.851200
		loss: 1.846300
		loss: 1.841100
		loss: 1.836000
		loss: 1.831500
		loss: 1.827800
		loss: 1.825100
		loss: 1.823600
		loss: 1.822900
		loss: 1.822600
		loss: 1.822400
		loss: 1.821800
		loss: 1.820700
		loss: 1.819100
		loss: 1.817200
		loss: 1.815200
		loss: 1.813400
		loss: 1.811800
		loss: 1.810600
		loss: 1.809500
		loss: 1.808600
		loss: 1.807700
		loss: 1.806700
		loss: 1.805500
		loss: 1.804200
		loss: 1.802600
		loss: 1.800900
		loss: 1.799100
		loss: 1.797200
		loss: 1.795200
		loss: 1.793200
		loss: 1.791200
		loss: 1.789100
		loss: 1.786900
		loss: 1.784500
		loss: 1.781900
		loss: 1.779100
		loss: 1.775900
		loss: 1.772500
		loss: 1.768800
		loss: 1.764900
		loss: 1.760800
		loss: 1.756400
		loss: 1.751800
		loss: 1.746900
		loss: 1.741700
		loss: 1.736200
		loss: 1.730500
		loss: 1.724600
		loss: 1.718800
		loss: 1.713100
		loss: 1.707700
		loss: 1.702600
		loss: 1.697800
		loss: 1.693600
		loss: 1.689800
		loss: 1.686600
		loss: 1.683900
		loss: 1.681700
		loss: 1.680000
		loss: 1.678800
		loss: 1.677900
		loss: 1.677200
		loss: 1.676600
		loss: 1.676300
		loss: 1.676000
		loss: 1.675900
		loss: 1.675800
		loss: 1.675800
		loss: 1.675700
		loss: 1.675600
		loss: 1.675600
		loss: 1.675600
		loss: 1.675500
		loss: 1.675500
		loss: 1.675400
		loss: 1.675400
		loss: 1.675400
		loss: 1.675300
		loss: 1.675300
		loss: 1.675200
		loss: 1.675200
		loss: 1.675100
		loss: 1.675000
		loss: 1.675000
		loss: 1.674900
		loss: 1.674800
		loss: 1.674800
		loss: 1.674700
		loss: 1.674600
		loss: 1.674500
		loss: 1.674500
		loss: 1.674400
		loss: 1.674300
		loss: 1.674300
		loss: 1.674200
		loss: 1.674100
		loss: 1.674000
		loss: 1.674000
		loss: 1.673900
		loss: 1.673800
		loss: 1.673800
		loss: 1.673700
		loss: 1.673700
		loss: 1.673600
		loss: 1.673500
		loss: 1.673500
		loss: 1.673400
		loss: 1.673400
		loss: 1.673300
		loss: 1.673300
		loss: 1.673200
		loss: 1.673200
		loss: 1.673200
		loss: 1.673100
		loss: 1.673100
		loss: 1.673000
		loss: 1.673000
		loss: 1.673000
		loss: 1.672900
		loss: 1.672900
		loss: 1.672800
		loss: 1.672800
		loss: 1.672800
		loss: 1.672700
		loss: 1.672700
		loss: 1.672700
		loss: 1.672600
		loss: 1.672600
		loss: 1.672600
		loss: 1.672500
		loss: 1.672500
		loss: 1.672500
		loss: 1.672500
		loss: 1.672400
		loss: 1.672400
		loss: 1.672400
		loss: 1.672300
		loss: 1.672300
		loss: 1.672300
		loss: 1.672300
		loss: 1.672200
		loss: 1.672200
		loss: 1.672200
		loss: 1.672200
		loss: 1.672100
		loss: 1.672100
		loss: 1.672100
		loss: 1.672000
		loss: 1.672000
		loss: 1.672000
		loss: 1.672000
		loss: 1.671900
		loss: 1.671900
		loss: 1.671900
		loss: 1.671900
		loss: 1.671800
		loss: 1.671800
		loss: 1.671800
		loss: 1.671800
		loss: 1.671700
		loss: 1.671700
		loss: 1.671700
		loss: 1.671700
		loss: 1.671600
		loss: 1.671600
		loss: 1.671600
		loss: 1.671600
		loss: 1.671600
		loss: 1.671500
		loss: 1.671500
		loss: 1.671500
		loss: 1.671500
		loss: 1.671400
		loss: 1.671400
		loss: 1.671400
		loss: 1.671400
		loss: 1.671300
		loss: 1.671300
		loss: 1.671300
		loss: 1.671300
		loss: 1.671300
		loss: 1.671200
		loss: 1.671200
		loss: 1.671200
		loss: 1.671200
		loss: 1.671100
		loss: 1.671100
		loss: 1.671100
		loss: 1.671100
		loss: 1.671100
		loss: 1.671000
		loss: 1.671000
		loss: 1.671000
		loss: 1.671000
		loss: 1.671000
		loss: 1.670900
		loss: 1.670900
		loss: 1.670900
		loss: 1.670900
		loss: 1.670900
		loss: 1.670800
		loss: 1.670800
		loss: 1.670800
		loss: 1.670800
		loss: 1.670800
		loss: 1.670700
		loss: 1.670700
		loss: 1.670700
		loss: 1.670700
		loss: 1.670700
		loss: 1.670600
		loss: 1.670600
		loss: 1.670600
		loss: 1.670600
		loss: 1.670600
		loss: 1.670500
		loss: 1.670500
		loss: 1.670500
		loss: 1.670500
		loss: 1.670500
		loss: 1.670500
		loss: 1.670400
		loss: 1.670400
		loss: 1.670400
		loss: 1.670400
		loss: 1.670400
		loss: 1.670300
		loss: 1.670300
		loss: 1.670300
		loss: 1.670300
		loss: 1.670300
		loss: 1.670300
		loss: 1.670200
		loss: 1.670200
		loss: 1.670200
		loss: 1.670200
		loss: 1.670200
		loss: 1.670200
		loss: 1.670100
		loss: 1.670100
		loss: 1.670100
		loss: 1.670100
		loss: 1.670100
		loss: 1.670000
		loss: 1.670000
		loss: 1.670000
		loss: 1.670000
		loss: 1.670000
		loss: 1.670000
		loss: 1.669900
		loss: 1.669900
		loss: 1.669900
		loss: 1.669900
		loss: 1.669900
		loss: 1.669900
		loss: 1.669900
		loss: 1.669800
		loss: 1.669800
		loss: 1.669800
		loss: 1.669800
		loss: 1.669800
		loss: 1.669800
		loss: 1.669700
		loss: 1.669700
		loss: 1.669700
		loss: 1.669700
		loss: 1.669700
		loss: 1.669700
		loss: 1.669600
		loss: 1.669600
		loss: 1.669600
		loss: 1.669600
		loss: 1.669600
		loss: 1.669600
		loss: 1.669600
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669500
		loss: 1.669400
		loss: 1.669400
		loss: 1.669400
	Overall the loss development was 2.080200 -> 1.669400

Training data for problem d-04.pddl in epoch 8:
model creation time: 62.37139534950256s
problem epoch data for epoch 8, problem epoch 1
	sampling search time: 30.85763120651245s
	during this search the following actions were chosen:
	training time: 292.17295360565186s
	during the training the following losses were computed:
		loss: 6.937100
		loss: 6.293300
		loss: 5.826900
		loss: 5.360800
		loss: 4.958500
		loss: 4.619900
		loss: 4.307700
		loss: 4.041300
		loss: 3.810800
		loss: 3.614500
		loss: 3.454100
		loss: 3.328300
		loss: 3.231200
		loss: 3.161000
		loss: 3.111100
		loss: 3.074700
		loss: 3.048600
		loss: 3.035500
		loss: 3.030600
		loss: 3.027900
		loss: 3.026000
		loss: 3.024400
		loss: 3.022200
		loss: 3.018600
		loss: 3.013600
		loss: 3.007200
		loss: 2.999500
		loss: 2.990900
		loss: 2.981200
		loss: 2.971000
		loss: 2.960200
		loss: 2.948900
		loss: 2.937200
		loss: 2.925000
		loss: 2.912500
		loss: 2.899600
		loss: 2.886300
		loss: 2.872700
		loss: 2.858900
		loss: 2.844900
		loss: 2.830900
		loss: 2.817000
		loss: 2.803200
		loss: 2.789800
		loss: 2.777000
		loss: 2.764600
		loss: 2.752900
		loss: 2.741600
		loss: 2.731100
		loss: 2.721600
		loss: 2.712600
		loss: 2.703900
		loss: 2.696400
		loss: 2.689300
		loss: 2.682100
		loss: 2.674800
		loss: 2.667500
		loss: 2.660400
		loss: 2.653700
		loss: 2.647400
		loss: 2.641700
		loss: 2.636100
		loss: 2.630400
		loss: 2.624700
		loss: 2.618900
		loss: 2.612900
		loss: 2.607000
		loss: 2.601500
		loss: 2.596200
		loss: 2.591200
		loss: 2.586200
		loss: 2.581200
		loss: 2.576300
		loss: 2.571600
		loss: 2.567100
		loss: 2.562800
		loss: 2.558400
		loss: 2.554000
		loss: 2.549800
		loss: 2.545900
		loss: 2.542200
		loss: 2.538500
		loss: 2.534800
		loss: 2.531300
		loss: 2.528000
		loss: 2.524900
		loss: 2.521700
		loss: 2.518800
		loss: 2.516000
		loss: 2.513300
		loss: 2.510800
		loss: 2.508400
		loss: 2.506100
		loss: 2.503800
		loss: 2.501700
		loss: 2.499700
		loss: 2.497700
		loss: 2.495800
		loss: 2.494000
		loss: 2.492200
		loss: 2.490500
		loss: 2.488800
		loss: 2.487200
		loss: 2.485600
		loss: 2.484100
		loss: 2.482600
		loss: 2.481100
		loss: 2.479700
		loss: 2.478300
		loss: 2.476900
		loss: 2.475600
		loss: 2.474300
		loss: 2.473000
		loss: 2.471700
		loss: 2.470500
		loss: 2.469300
		loss: 2.468100
		loss: 2.466900
		loss: 2.465700
		loss: 2.464600
		loss: 2.463400
		loss: 2.462300
		loss: 2.461200
		loss: 2.460100
		loss: 2.459100
		loss: 2.458000
		loss: 2.457000
		loss: 2.455900
		loss: 2.454900
		loss: 2.453900
		loss: 2.452900
		loss: 2.451900
		loss: 2.451000
		loss: 2.450000
		loss: 2.449000
		loss: 2.448100
		loss: 2.447200
		loss: 2.446300
		loss: 2.445400
		loss: 2.444500
		loss: 2.443600
		loss: 2.442700
		loss: 2.441800
		loss: 2.441000
		loss: 2.440100
		loss: 2.439300
		loss: 2.438500
		loss: 2.437600
		loss: 2.436800
		loss: 2.436000
		loss: 2.435200
		loss: 2.434400
		loss: 2.433700
		loss: 2.432900
		loss: 2.432100
		loss: 2.431400
		loss: 2.430600
		loss: 2.429900
		loss: 2.429100
		loss: 2.428400
		loss: 2.427700
		loss: 2.427000
		loss: 2.426300
		loss: 2.425600
		loss: 2.424900
		loss: 2.424200
		loss: 2.423500
		loss: 2.422800
		loss: 2.422200
		loss: 2.421500
		loss: 2.420900
		loss: 2.420200
		loss: 2.419600
		loss: 2.419000
		loss: 2.418300
		loss: 2.417700
		loss: 2.417100
		loss: 2.416500
		loss: 2.415900
		loss: 2.415300
		loss: 2.414700
		loss: 2.414100
		loss: 2.413500
		loss: 2.412900
		loss: 2.412400
		loss: 2.411800
		loss: 2.411200
		loss: 2.410700
		loss: 2.410100
		loss: 2.409600
		loss: 2.409000
		loss: 2.408500
		loss: 2.408000
		loss: 2.407400
		loss: 2.406900
		loss: 2.406400
		loss: 2.405900
		loss: 2.405400
		loss: 2.404900
		loss: 2.404400
		loss: 2.403900
		loss: 2.403400
		loss: 2.402900
		loss: 2.402400
		loss: 2.401900
		loss: 2.401400
		loss: 2.401000
		loss: 2.400500
		loss: 2.400000
		loss: 2.399600
		loss: 2.399100
		loss: 2.398700
		loss: 2.398200
		loss: 2.397800
		loss: 2.397300
		loss: 2.396900
		loss: 2.396500
		loss: 2.396100
		loss: 2.395600
		loss: 2.395200
		loss: 2.394800
		loss: 2.394400
		loss: 2.394000
		loss: 2.393600
		loss: 2.393200
		loss: 2.392800
		loss: 2.392400
		loss: 2.392000
		loss: 2.391600
		loss: 2.391200
		loss: 2.390800
		loss: 2.390400
		loss: 2.390100
		loss: 2.389700
		loss: 2.389300
		loss: 2.389000
		loss: 2.388600
		loss: 2.388200
		loss: 2.387900
		loss: 2.387500
		loss: 2.387200
		loss: 2.386800
		loss: 2.386500
		loss: 2.386200
		loss: 2.385800
		loss: 2.385500
		loss: 2.385100
		loss: 2.384800
		loss: 2.384500
		loss: 2.384200
		loss: 2.383800
		loss: 2.383500
		loss: 2.383200
		loss: 2.382900
		loss: 2.382600
		loss: 2.382300
		loss: 2.382000
		loss: 2.381700
		loss: 2.381400
		loss: 2.381100
		loss: 2.380800
		loss: 2.380500
		loss: 2.380200
		loss: 2.379900
		loss: 2.379600
		loss: 2.379300
		loss: 2.379000
		loss: 2.378800
		loss: 2.378500
		loss: 2.378200
		loss: 2.377900
		loss: 2.377700
		loss: 2.377400
		loss: 2.377100
		loss: 2.376900
		loss: 2.376600
		loss: 2.376300
		loss: 2.376100
		loss: 2.375800
		loss: 2.375600
		loss: 2.375300
		loss: 2.375100
		loss: 2.374800
		loss: 2.374600
		loss: 2.374300
		loss: 2.374100
		loss: 2.373900
		loss: 2.373600
		loss: 2.373400
		loss: 2.373100
		loss: 2.372900
		loss: 2.372700
		loss: 2.372500
		loss: 2.372200
		loss: 2.372000
		loss: 2.371800
		loss: 2.371600
		loss: 2.371300
		loss: 2.371100
		loss: 2.370900
	Overall the loss development was 6.937100 -> 2.370900

